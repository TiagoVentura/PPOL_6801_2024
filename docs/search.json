[
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Instructions for PPOL 6801 - Final Project",
    "section": "",
    "text": "Slides\n\nWeek 1 Slides\n\nRequired Readings\n\nGrimmer, Justin, Margaret E. Roberts, and Brandon M. Stewart. Text as data: A new framework for machine learning and the social sciences. Princeton University Press, 2022 - Chapter 2.\n\nOr one of the three below:\n\nPolitical Science Perspective: Wilkerson, J. and Casas, A. (2017). Large-Scale Computerized Text Analysis in Political Science: Opportunities and Challenges. Annual Review of Political Science, 20, 529:544.\nEconomics Perspective: Gentzkow, Matthew, Bryan Kelly, and Matt Taddy. “Text as data.” Journal of Economic Literature 57, no. 3 (2019): 535-574.\nSociology Perspective: https://www.sciencedirect.com/science/article/pii/S0049089X22000904"
  },
  {
    "objectID": "schedule.html#weekly-schedule",
    "href": "schedule.html#weekly-schedule",
    "title": "Schedule",
    "section": "",
    "text": "Week\n\n\nTopic\n\n\nDate\n\n\n\n\n\n\nWeek 01\n\n\nIntroduction, Overview of the course\n\n\nJanuary 17, 2024\n\n\n\n\nWeek 02\n\n\nFrom Text to Matrices: Representing Text as Data\n\n\nJanuary 24, 2024\n\n\n\n\nWeek 03\n\n\nText Similarity, Text re-use and Complexity\n\n\nJanuary 31, 2024\n\n\n\n\nWeek 04\n\n\nSupervised Learning I: Dictionary Methods and Off-the-Shelf Classifiers\n\n\nFebruary 07, 2024\n\n\n\n\nWeek 05\n\n\nSupervised Learning II: Training your own classifiers\n\n\nFebruary 14, 2024\n\n\n\n\nWeek 06\n\n\nReplication Class I: Students’ Presentation\n\n\nFebruary 21, 2023\n\n\n\n\nWeek 07\n\n\nUnsupervised Learning: Topic Models\n\n\nFebruary 28, 2024\n\n\n\n\nWeek 08\n\n\nWord Embeddings: What are they and how to estimate?\n\n\nMarch 13, 2024\n\n\n\n\nWeek 09\n\n\nWord Embeddings: Social Science Applications\n\n\nMarch 20, 2024\n\n\n\n\nWeek 10\n\n\nUsing Text to Measure Ideology - Scaling\n\n\nMarch 27, 2024\n\n\n\n\nWeek 11\n\n\nReplication Class II\n\n\nApril 03, 2024\n\n\n\n\nWeek 12\n\n\nLarge Language Models: Theory and Fine-tuning a Transformers-based model (Invited Speaker Dr. Sebastian Vallejo)\n\n\nApril 10, 2024\n\n\n\n\nWeek 13\n\n\nLarge Language Models: Outsourcing Applications\n\n\nApril 17, 2023\n\n\n\n\nWeek 14\n\n\nPresentations of Final Projects\n\n\nApril 24, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "slides/week_01_intro.html#outline",
    "href": "slides/week_01_intro.html#outline",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Outline",
    "text": "Outline\n\n\nIntroduction (me)\nMotivation for Computational Linguistics\n\nDigital information age\nPrinciples of Computational Linguistics.\nWhat this course is not.\nExamples of models and applications for this course\n\nIntroductions (you)\nClass Logistics ( + 10 min for you to read through the syllabus)\nQ&A\nAcquiring text in the web (Jupyter notebooks for scrapping)"
  },
  {
    "objectID": "slides/week_01_intro.html#introduction",
    "href": "slides/week_01_intro.html#introduction",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Introduction",
    "text": "Introduction\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProfessor Tiago Ventura (he/him)\n\nAssistant Professor at McCourt School.\nPolitical Science Ph.D.\nPostdoc at Center for Social Media and Politics - NYU.\nResearcher at Twitter.\n\nSome Projects I am involved\n\nGlobal Social Media Deactivation.\nEffects of WhatsApp on Elections in the Global South.\nAI and Misinformation in 2024 elections.\nPanels of voter files and twitter users.\n\nOutside of work, I enjoy watching soccer, reading sci-fi and running"
  },
  {
    "objectID": "slides/week_01_intro.html#rise-of-the-digital-information-age",
    "href": "slides/week_01_intro.html#rise-of-the-digital-information-age",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Rise of the digital information age",
    "text": "Rise of the digital information age"
  },
  {
    "objectID": "slides/week_01_intro.html#official-documents-congressional-speeches-bills-press-releases-transcripts-from-all-over-the-world",
    "href": "slides/week_01_intro.html#official-documents-congressional-speeches-bills-press-releases-transcripts-from-all-over-the-world",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Official Documents: Congressional Speeches, Bills, Press Releases, Transcripts, from all over the world!!",
    "text": "Official Documents: Congressional Speeches, Bills, Press Releases, Transcripts, from all over the world!!"
  },
  {
    "objectID": "slides/week_01_intro.html#social-media",
    "href": "slides/week_01_intro.html#social-media",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Social Media",
    "text": "Social Media"
  },
  {
    "objectID": "slides/week_01_intro.html#the-internet-news-comments-blogs-etc",
    "href": "slides/week_01_intro.html#the-internet-news-comments-blogs-etc",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "The internet: News, Comments, Blogs, etc…",
    "text": "The internet: News, Comments, Blogs, etc…"
  },
  {
    "objectID": "slides/week_01_intro.html#what-is-this-class-about",
    "href": "slides/week_01_intro.html#what-is-this-class-about",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "What is this class about?",
    "text": "What is this class about?\n\nFor many years, social scientists use text in their analysis\nMostly through in-depth reading of documents.\nClose Reading. Humans are great at this!\nDigital Revolution:\n\nProduction of text increased\nThe capacity to analyze them at scale as well.\n\nThis class covers methods (and many applications) of using text as data to answer social science problems and test social science theories\nComputational Linguistics ~ Distant Reading. Computers are better at understanding patterns, classify and describe content across millions of documents."
  },
  {
    "objectID": "slides/week_01_intro.html#principles-of-text-analysis-gmb-textbook",
    "href": "slides/week_01_intro.html#principles-of-text-analysis-gmb-textbook",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Principles of Text Analysis (GMB Textbook)",
    "text": "Principles of Text Analysis (GMB Textbook)\n\nPrinciple 1: Social science theories and substantive knowledge are essential for research design\nPrinciple 2: Text analysis does not replace humans—it augments them.\nPrinciple 3: Building, refining, and testing social science theories requires iteration and cumulation.\nPrinciple 4: Text analysis methods distill generalizations from language. (all models are wrong!)\nPrinciple 5: The best method depends on the task. (Qualitative knowledge)\nPrinciple 6: Validations are essential and depend on the theory and the task"
  },
  {
    "objectID": "slides/week_01_intro.html#challenges-i-text-is-an-unstructure-data-source",
    "href": "slides/week_01_intro.html#challenges-i-text-is-an-unstructure-data-source",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Challenges I: Text is an unstructure data source",
    "text": "Challenges I: Text is an unstructure data source"
  },
  {
    "objectID": "slides/week_01_intro.html#challenge-ii-text-is-high-dimensionality",
    "href": "slides/week_01_intro.html#challenge-ii-text-is-high-dimensionality",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Challenge II: Text is High dimensionality",
    "text": "Challenge II: Text is High dimensionality\nFrom Gentzkow et al 2017:\n\nsample of documents, each \\(n_L\\) words long, drawn from vocabulary of \\(n_V\\) words.\nThe unique representation of each document has dimension \\(n_{V}^{n_L}\\) .\n\ne.g., a sample of 30-word (\\(n_L\\)) Twitter messages using only the one thousand most common words in the English language\nDimensionality = \\(1000^{30}\\)\nAs a matrix: \\(M^{1000}_{n_tweets}\\)\n\nMost of what you learned in statistics so far does not equip you to deal with this curse of dimensionality."
  },
  {
    "objectID": "slides/week_01_intro.html#text-as-data-workflow",
    "href": "slides/week_01_intro.html#text-as-data-workflow",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Text-as-Data Workflow",
    "text": "Text-as-Data Workflow\n\nAcquire textual data:\n\nExisting corpora; scraped data; digitized text"
  },
  {
    "objectID": "slides/week_01_intro.html#text-as-data-workflow-1",
    "href": "slides/week_01_intro.html#text-as-data-workflow-1",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Text-as-Data Workflow",
    "text": "Text-as-Data Workflow\n\nMap Documents to a numerical representation M\n\nBag-of-words (sparse vectors)\nEmbeddings (dense vectors)\nReduce noise, capture signal"
  },
  {
    "objectID": "slides/week_01_intro.html#text-as-data-workflow-2",
    "href": "slides/week_01_intro.html#text-as-data-workflow-2",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Text-as-Data Workflow",
    "text": "Text-as-Data Workflow\n\nMap M to predicted values \\(V^{*}\\) of unknown outcomes V\n\nDescriptive Analysis\nClassify documents into unknown categories\n\nTopic models\n\nClassify documents into known categories\n\nDictionary methods\nSupervised machine learning\nTransfer-Learning - use models trained in text for other purposes\n\nScale documents on latent dimension:"
  },
  {
    "objectID": "slides/week_01_intro.html#text-as-data-workflow-3",
    "href": "slides/week_01_intro.html#text-as-data-workflow-3",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Text-as-Data Workflow",
    "text": "Text-as-Data Workflow\n\nUse \\(V^{*}\\) in subsequent analysis with other data sources\n\nThis is where social science happens!"
  },
  {
    "objectID": "slides/week_01_intro.html#assume-you-already-did-it",
    "href": "slides/week_01_intro.html#assume-you-already-did-it",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Assume you already did it!",
    "text": "Assume you already did it!\n\nAcquire textual data: Existing corpora; scraped data; digitized text"
  },
  {
    "objectID": "slides/week_01_intro.html#overview-of-tad-methods",
    "href": "slides/week_01_intro.html#overview-of-tad-methods",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Overview of TAD Methods",
    "text": "Overview of TAD Methods\n\nDescriptive inference: how to convert text to matrices, vector space model, bag-of-words, dissimilarity measures, diversity, complexity, style.\nSupervised techniques: dictionaries, classication, scaling, machine learning approaches.\nUnsupervised techniques: clustering, topic models, embeddings.\nSpecial topics: Word embeddings and Large Language Models."
  },
  {
    "objectID": "slides/week_01_intro.html#some-cool-applications",
    "href": "slides/week_01_intro.html#some-cool-applications",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Some cool applications",
    "text": "Some cool applications\n\nMeasure text-reuse across thousands of bills from U.S. state legislatures\nEstimate levels of toxicity of comments from stremming chats platforms during political debates\nMeasure how out-group negative makes things go viral on social media\nMeasure the policy target of bills proposals in Mexico\nEstimate ideological positions using who a user follows on Twitter, what a user share on social media, political manifestos, or just asking ChatGPT to pair-wise compare politicians"
  },
  {
    "objectID": "slides/week_01_intro.html#what-this-class-in-not-about-it",
    "href": "slides/week_01_intro.html#what-this-class-in-not-about-it",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "What this class in not about it…",
    "text": "What this class in not about it…\n\nData acquisition: no scrapping in class. Assume you have learned already.\nRegular expressions and basic text manipulation.\nCS Stuff: machine translation, OCR, POS, entity recognition.\n\nMost NLP/CS will focus on developing new algorithms, information retrievel and purely better measurements.\nin a productive dialogue with NLP, we will focus on using text for social science research\n\ntheoretically driven discovery and measurement\nintegration with social science problems + tabular data."
  },
  {
    "objectID": "slides/week_01_intro.html#your-turn",
    "href": "slides/week_01_intro.html#your-turn",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Your turn!",
    "text": "Your turn!\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\nName & pronouns\nWhy are you taking this course?\nYour experience (if any) working with text\nThe most interesting thing you learned in the DSPP so far"
  },
  {
    "objectID": "slides/week_01_intro.html#read-the-syllabus",
    "href": "slides/week_01_intro.html#read-the-syllabus",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Read the syllabus!",
    "text": "Read the syllabus!\n\n\n\n−+\n10:00"
  },
  {
    "objectID": "slides/week_01_intro.html#class-requirements",
    "href": "slides/week_01_intro.html#class-requirements",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Class Requirements",
    "text": "Class Requirements\n\nAssume you all have a intro course in statistics and probability (which I know you do)\nMath: Basic knowledge of calculus, probability, densities, distributions, statistical tests, hypothesis testing, the linear model, maximum likelihood and generalized linear models is assumed.\nProgramming: Functional knowledge of R - main programming language of the course. Some Python at the end.\n\nR is excellent for text analysis, and for some social science applications, better than Python\nFree, and massive online community writing packages and extending modeling capabilities.\nWe will divide our learning between using tidytext and quanteda for text analysis.\nDownload RStudio IDE!"
  },
  {
    "objectID": "slides/week_01_intro.html#how-to-do-well-in-the-class",
    "href": "slides/week_01_intro.html#how-to-do-well-in-the-class",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "How to do well in the class?",
    "text": "How to do well in the class?\nI designed this course as PhD style seminar:\n\nSo far, you learned a lot of DS techniques (DS I, DS II, DS III)\nYou haven’t dig deep enough in a particular field. That’s what electives are for!\nHeavy on readings - Lot’s of applied and technical readings.\nDo the readings before class\nSubstantive readings are especially important, because they’ll help you understand what an interesting question looks like – in social science/public policy.\nPlan ahead – particularly for the replication exercise\nIf you have a corpus you want work with, please bring it to class!"
  },
  {
    "objectID": "slides/week_01_intro.html#what-our-classes-will-look-like.",
    "href": "slides/week_01_intro.html#what-our-classes-will-look-like.",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "What our classes will look like.",
    "text": "What our classes will look like.\nThis is a one meeting per week class. You should expect:\n\nBetween 1h-1.5h of lecture based on this week topics + readings\nYour participation in the lecture is expected I will ask your insights about the readings.\nBreak (10min)\nCoding.\n\nMix of you working through some code I prepared.\nAnd I live-coding for you."
  },
  {
    "objectID": "slides/week_01_intro.html#textbook",
    "href": "slides/week_01_intro.html#textbook",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Textbook",
    "text": "Textbook"
  },
  {
    "objectID": "slides/week_01_intro.html#logistics",
    "href": "slides/week_01_intro.html#logistics",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Logistics",
    "text": "Logistics\n\nCommunication: via slack. Join the channel!\nAll materials: hosted on the class website: https://tiagoventura.github.io/PPOL_6801_2024//\nSyllabus: also on the website.\nMy Office Hours: Every Tuesday from 4 to 6pm. Just stop by!\nCanvas: Only for communication! Materials will be hosted in the website!"
  },
  {
    "objectID": "slides/week_01_intro.html#evalutation",
    "href": "slides/week_01_intro.html#evalutation",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Evalutation",
    "text": "Evalutation\n\n\n\nAssignment\nPercentage of Grade\n\n\n\n\nParticipation/Attendance\n10%\n\n\nProblem Sets\n20%\n\n\nReplication Exercises\n30%\n\n\nFinal Project\n40%"
  },
  {
    "objectID": "slides/week_01_intro.html#participation",
    "href": "slides/week_01_intro.html#participation",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Participation",
    "text": "Participation\n\nActive involvement during class sessions, fostering a dynamic learning environment.\nContributions made to your group’s ultimate project.\nAssisting classmates with slack questions, sharing interesting materials on slack, asking question, and anything that provides healthy contributions to the course."
  },
  {
    "objectID": "slides/week_01_intro.html#problem-sets",
    "href": "slides/week_01_intro.html#problem-sets",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Problem Sets",
    "text": "Problem Sets\n\n\n\nAssignment\nDate Assigned\nDate Due\n\n\n\n\nNo. 1\nWeek 4\nBefore EOD of Week 5’s class\n\n\nNo. 2\nWeek 7\nBefore EOD of Week 8’s class\n\n\nNo. 3\nWeek 9\nBefore EOD of Week 10’s class\n\n\n\n\nYou will have a week to complete your assignments\nindividual assignment\ndistributed through github"
  },
  {
    "objectID": "slides/week_01_intro.html#replication-exercises",
    "href": "slides/week_01_intro.html#replication-exercises",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Replication Exercises",
    "text": "Replication Exercises\nOpportunity to learn how science is made!\nWork in randomly assigned pairs I will post on Slack.\n\nStep 1: finding a paper to replicate (from the syllabus)\n\nBy the end of the week 2 and week 7, you should select an article from the syllabus to be replicated by your team.\nInform the class on slack\n“first come, first served”\n\nStep 2: Acquiring the Data\n\nif you fail to get the data, pick another article.\n\nStep 3: Presentation (weeks 6 and 11)\nStep 4: Replication Repository on Github"
  },
  {
    "objectID": "slides/week_01_intro.html#final-project",
    "href": "slides/week_01_intro.html#final-project",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Final Project",
    "text": "Final Project\nThe project is composed of three parts:\n\na 2 page project proposal: (which should be discussed and approved by me)\nan in-class presentation,\nA 10-page project report.\n\n\n\n\n\n\n\n\n\n\nRequirement\nDue\nLength\nPercentage\n\n\n\n\nProject Proposal\nEOD Friday Week 9\n2 pages\n5%\n\n\nPresentation\nWeek 14\n10-15 minutes\n10%\n\n\nProject Report\nWednesday Week 15\n10 pages\n25%"
  },
  {
    "objectID": "slides/week_01_intro.html#chatgpt",
    "href": "slides/week_01_intro.html#chatgpt",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "ChatGPT",
    "text": "ChatGPT\n\nYou are allowed to use ChatGPT as you would use google in this class. This means:\n\nDo not copy the responses from chatgpt – a lot of them are wrong or will just not run on your computer\nUse chatgpt as a auxiliary source.\nIf your entire homework comes straight from chatgpt, I will consider it plagiarism.\nIf you use chatgpt, I ask you to mention on your code how chatgpt worked for you."
  },
  {
    "objectID": "slides/week_01_intro.html#acquiring-text",
    "href": "slides/week_01_intro.html#acquiring-text",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Acquiring text:",
    "text": "Acquiring text:\n\nAs a review, here are some notebooks I developed for Data Science I introducing a full toolkit for acquiring data in the web:\n\nStatic Websites\nAPIs\nSelenium for Dynamics Websites\n\n\n\n\nText-as-Data"
  },
  {
    "objectID": "slides/week_04.html#housekeeping",
    "href": "slides/week_04.html#housekeeping",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Housekeeping",
    "text": "Housekeeping\nYour first problem set will be assigned today! Some important information:\n\n\nYou will receive and submit your assignment using Github!\n\nGithub Clasroom: Creates automatically a repo for the assignment. You and I are owner of the repo.\n\nDeadline: EOD next Wednesday, February 14th.\nPlease use an .RMD/.QMD file to submit your assignment. If you prefer to solve using jupyter, let me know!\nAny questions?"
  },
  {
    "objectID": "slides/week_04.html#where-are-we",
    "href": "slides/week_04.html#where-are-we",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Where are we?",
    "text": "Where are we?\nAfter learning how to process and represent text as numbers, we started digging in on how to use text on a research pipeline.\n\n\nDescriptive inference:\n\n\nCounting words (Ban’s Paper)\nComparing document similarity using vector space model (text re-use)\nMeasures of lexical diversity and readability"
  },
  {
    "objectID": "slides/week_04.html#plans-for-today",
    "href": "slides/week_04.html#plans-for-today",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Plans for today",
    "text": "Plans for today\nFor the next two weeks, we will talk about Measurement\n\nMeasurement: Map/Measure concepts from theory to data.\n\n\n\nDocuments pertaining to certain classes and how we can use statistical assumptions to measure these classes\n\n\n\n\n\nDictionary Methods\n\nDiscuss some well-known dictionaries\n\nOff-the-Shelf Classifiers\n\nPerspective API\nHugging Face (only see as a off-the-shelf machines, LMMs later in this course)\n\nNext week: training our own machine learning models"
  },
  {
    "objectID": "slides/week_04.html#connecting-machine-learning-with-tad",
    "href": "slides/week_04.html#connecting-machine-learning-with-tad",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Connecting Machine Learning with TAD",
    "text": "Connecting Machine Learning with TAD\nIn the Machine Learning tradition, we are introduced to two core family of models:\n\n\nUnsupervised Models: learning (hidden or latent) structure in unlabeled data.\n\nTopic Models to cluster documents and words\n\n\n\n\n\nSupervised Models: learning relationship between inputs and a labeled set of outputs.\n\nSentiment Analysis, classify if a tweet contains misinformation, etc..\n\n\n\n\n\nIn TAD, we mostly use unsupervised techniques for discovery and supervised for measurement of concepts."
  },
  {
    "objectID": "slides/week_04.html#supervised-learning-pipeline-for-tad",
    "href": "slides/week_04.html#supervised-learning-pipeline-for-tad",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Supervised Learning Pipeline for TAD",
    "text": "Supervised Learning Pipeline for TAD\n\n\nStep 1: label some examples of the concept of we want to measure\n\nsome tweets are positive, some are neutral and some are negative\n\nStep 2: train a statistical model on these set of label data using the document-feature matrix as input\n\nchoose a model (transformation function) that gives higher out-of-sample accuracy\n\nStep 3: use the classifier - some f(x) - to predict unseen documents.\nStep 4: use the measure + metadata|exogenous shocks to learn something new about the world.\n\nThis is where social science happens!"
  },
  {
    "objectID": "slides/week_04.html#back-to-the-future-exercise",
    "href": "slides/week_04.html#back-to-the-future-exercise",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Back to the Future Exercise",
    "text": "Back to the Future Exercise\n\n\n\n\n\nvia GIPHY\n\n\nAssume you got the delorean to travel back twenty years ago, you want to run a simple sentiment analysis in a corpus of news articles.\n\nWhich challenges would you face?\nHow could you solve it?\nPlease consider all four steps described before"
  },
  {
    "objectID": "slides/week_04.html#overview-of-dictionaries",
    "href": "slides/week_04.html#overview-of-dictionaries",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Overview of Dictionaries",
    "text": "Overview of Dictionaries\n\n\nUse a set of pre-defined words that allow us to classify documents automatically, quickly and accurately.\nInstead of optimizing a transformation function using statistical assumption and seen data, in dictionaries we have a pre-assumed recipe for the transformation function.\nA dictionary contains:\n\na list of words that corresponds to each category\n\npositive and negative for sentiment\nSexism, homophobia, xenophobia, racism for hate speech\n\n\nWeights given to each word ~ same for all words or some continuous variation."
  },
  {
    "objectID": "slides/week_04.html#more-specifically",
    "href": "slides/week_04.html#more-specifically",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "More specifically…",
    "text": "More specifically…\nWe have a set of key words with weights,\n\ne.g. for sentiment analysis: horrible is scored as \\(-1\\) and beautiful as \\(+1\\)\nthe relative rate of occurrence of these terms tells us about the overall tone or category that the document should be placed in.\n\n\nFor document \\(i\\) and words \\(m=1,\\ldots, M\\) in the dictionary,\n\\[\\text{tone of document $i$}= \\sum^M_{m=1} \\frac{s_m w_{im}}{N_i}\\]\nWhere:\n\n\\(s_m\\) is the score of word \\(m\\)\n\\(w_{im}\\) is the number of occurrences of the \\(m_{th}\\) dictionary word in the document \\(i\\)\n\\(N_i\\) is the total number of all dictionary words in the document"
  },
  {
    "objectID": "slides/week_04.html#why-dictionaries",
    "href": "slides/week_04.html#why-dictionaries",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Why Dictionaries?",
    "text": "Why Dictionaries?\n\n\nLow cost and computationally efficient ~ if using a dictionary developed and validated by others\nA hybrid procedure between qualitative and quantitative classification at the fully automated end of the text analysis spectrum\nDictionary construction involves a lot of contextual interpretation and qualitative judgment\nTransparency: no black-box model behind the classification task"
  },
  {
    "objectID": "slides/week_04.html#general-inquirer-stone-et-al-1966",
    "href": "slides/week_04.html#general-inquirer-stone-et-al-1966",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "General Inquirer (Stone et al 1966)",
    "text": "General Inquirer (Stone et al 1966)\n\n\nIt combines several dictionaries to make total of 182 categories:\n\nthe “Harvard IV-4” dictionary: psychology, themes, topics\nthe “Lasswell” dictionary, five categories based on the social cognition work of Semin and Fiedler\n\n“self references”, containing mostly pronouns;\n“negatives”, the largest category with 2291 entries"
  },
  {
    "objectID": "slides/week_04.html#linquistic-inquiry-and-word-count",
    "href": "slides/week_04.html#linquistic-inquiry-and-word-count",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Linquistic Inquiry and Word Count",
    "text": "Linquistic Inquiry and Word Count\nCreated by Pennebaker et al — see http://www.liwc.net\n\nLarge dictionary with around 4,500 words and words steams\n90 categories\nCategories are organized hierarchically\n\nAll anger words, by definition, will be categorized as negative emotion and overall emotion words.\n\nWords are in one or more categories\n\nthe word cried is part of five word categories: sadness, negative emotion, overall affect, verb, and past tense verb.\n\nYou can buy it here: http://www.liwc.net/descriptiontable1.php"
  },
  {
    "objectID": "slides/week_04.html#heavily-used-in-academia",
    "href": "slides/week_04.html#heavily-used-in-academia",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Heavily used in academia!",
    "text": "Heavily used in academia!\n\n\n\n\n\n\n\nPennebaker et al, 2009"
  },
  {
    "objectID": "slides/week_04.html#vader-an-open-source-alternative-to-liwc",
    "href": "slides/week_04.html#vader-an-open-source-alternative-to-liwc",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "VADER: an open-source alternative to LIWC",
    "text": "VADER: an open-source alternative to LIWC\nValence Aware Dictionary and sEntiment Reasoner:\n\nTuned for social media text\nCapture polarity and intensity\n\nSentiment Lexicon: This is a list of known words and their associated sentiment scores.\nSentiment Intensity Scores: Each word in the lexicon is assigned a score that ranges from -4 (extremely negative) to +4 (extremely positive).\nFive Heuristic-based rules: exclamation points, caps lock, intensifiers, negation, tri-grams\n\nPython and R libraries: https://github.com/cjhutto/vaderSentiment\nArticle: https://ojs.aaai.org/index.php/ICWSM/article/view/14550/14399"
  },
  {
    "objectID": "slides/week_04.html#young-sarokas-lexicoder-sentiment-dictionary",
    "href": "slides/week_04.html#young-sarokas-lexicoder-sentiment-dictionary",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Young & Saroka’s Lexicoder Sentiment Dictionary",
    "text": "Young & Saroka’s Lexicoder Sentiment Dictionary\n\nCreate dictionary specifically for political communication\nCombines:\n\nGeneral Inquirer;\nRoget’s Thesaurus and\nRegressive Imagery Dictionary\n\nEach words pertains to a single class\nPlus\n\nHand coding\nKeyword in context dos disambiguation"
  },
  {
    "objectID": "slides/week_04.html#performance",
    "href": "slides/week_04.html#performance",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Performance",
    "text": "Performance\n\n\n\n\n\n\n\nLSD results assign 74% to the positive category and just 12% to the negative category. Of the 495 articles that are categorized as negative by at least two coders, LSD results assign 53% to the negative category and 32% to the positive category ~ 69% of accuracy"
  },
  {
    "objectID": "slides/week_04.html#laver-and-garry-2000",
    "href": "slides/week_04.html#laver-and-garry-2000",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Laver and Garry, 2000",
    "text": "Laver and Garry, 2000\nA hierarchical set of categories to distinguish policy domains and policy positions on party manifestos\n\nFive Domains:\n\neconomy\npolitical system\nsocial system\nexternal relations\n\nLookes for word occurrences within “word strings with an average length of ten words”\nArticle: Estimating Policy Positions from Political Texts"
  },
  {
    "objectID": "slides/week_04.html#laver-and-garry-2000-1",
    "href": "slides/week_04.html#laver-and-garry-2000-1",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Laver and Garry, 2000",
    "text": "Laver and Garry, 2000"
  },
  {
    "objectID": "slides/week_04.html#rathje-et.-al-2020-pnas-out-group-animosity",
    "href": "slides/week_04.html#rathje-et.-al-2020-pnas-out-group-animosity",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Rathje et. al 2020, PNAS, Out-group animosity",
    "text": "Rathje et. al 2020, PNAS, Out-group animosity"
  },
  {
    "objectID": "slides/week_04.html#rathje-et.-al-2020-pnas-out-group-animosity-1",
    "href": "slides/week_04.html#rathje-et.-al-2020-pnas-out-group-animosity-1",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Rathje et. al 2020, PNAS, Out-group animosity",
    "text": "Rathje et. al 2020, PNAS, Out-group animosity\n\nWe used the R package quanteda to analyze Twitter and Facebook text. During text preprocessing, we removed punctuation, URLs, and numbers. To classify whether a specific post was referring to a liberal or a conservative, we adapted previously used dictionaries that referred to words associated with liberals or conservatives. Specifically, these dictionaries included 1) a list of the top 100 most famous Democratic and Republican politicians according to YouGov, along with their Twitter handles (or Facebook page names for the Facebook datasets) (e.g., “Trump,” “Pete Buttigieg,” “@realDonaldTrump”); 2) a list of the current Democratic and Republican (but not independent) US Congressional members (532 total) along with their Twitter and Facebook names (e.g., “Amy Klobuchar,” “Tom Cotton”); and 3) a list of about 10 terms associated with Democratic (e.g., “liberal,” “democrat,” or “leftist”) or Republican identity (e.g., “conservative,” “republican,” or “ring-wing”)."
  },
  {
    "objectID": "slides/week_04.html#rathje-et.-al-2020-pnas-out-group-animosity-2",
    "href": "slides/week_04.html#rathje-et.-al-2020-pnas-out-group-animosity-2",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Rathje et. al 2020, PNAS, Out-group animosity",
    "text": "Rathje et. al 2020, PNAS, Out-group animosity\n\nWe then assigned each tweet a count for words that matched our Republican and Democrat dictionaries (for instance, if a tweet mentioned two words in the “Republican” dictionary, it would receive a score of “2” in that category). We also used previously validated dictionaries that counted the number of positive and negative affect words per post and the number of moral-emotional words per post (LIWC)."
  },
  {
    "objectID": "slides/week_04.html#advantages",
    "href": "slides/week_04.html#advantages",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Advantages",
    "text": "Advantages\nWe already discussed some of the advantages:\n\nlow-cost when working with open sourced dictionaries\n\nrelatively easy to build/expand on new dictionaries\n\nbridge qualitative and quantitative\neasy to validate\n\ndictionaries are transparent and reliable.\n\ntransfer well across languages."
  },
  {
    "objectID": "slides/week_04.html#disadvantage-context-specific",
    "href": "slides/week_04.html#disadvantage-context-specific",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Disadvantage: Context specific",
    "text": "Disadvantage: Context specific\n\n\n\n\n\n\n\nSource: Gonzalez-Bailon et al"
  },
  {
    "objectID": "slides/week_04.html#disadvantage-performance",
    "href": "slides/week_04.html#disadvantage-performance",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Disadvantage: Performance",
    "text": "Disadvantage: Performance\n\n\n\n\n\n\n\nMuddiman et al, 2019 - Reclaiming our Expertise"
  },
  {
    "objectID": "slides/week_04.html#off-the-shelf-models-ventura-et.-al.-2021.",
    "href": "slides/week_04.html#off-the-shelf-models-ventura-et.-al.-2021.",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Off-the-shelf models: Ventura et. al. 2021.",
    "text": "Off-the-shelf models: Ventura et. al. 2021.\n\n\n\n\n\n\n\nSee article: Ventura et al. 2021, Connective Effervescence and Streaming Chat"
  },
  {
    "objectID": "slides/week_04.html#off-the-shelf-deep-learning-models",
    "href": "slides/week_04.html#off-the-shelf-deep-learning-models",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Off-the-shelf Deep Learning Models",
    "text": "Off-the-shelf Deep Learning Models\n\nDefinition: Pre-trained models designed for general-purpose classification tasks\n\nIn general those are models built on TONS of data and optimized for a particular task\n\nKey Features:\n\nReady to use\nLow to zero cost\nDeep ML architectures ~ High accuracy\nCan be re-trained for your specific task"
  },
  {
    "objectID": "slides/week_04.html#transformers",
    "href": "slides/week_04.html#transformers",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Transformers",
    "text": "Transformers"
  },
  {
    "objectID": "slides/week_06.html#housekeeping",
    "href": "slides/week_06.html#housekeeping",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Housekeeping",
    "text": "Housekeeping\nLet’s quickly review your future assignments:\n\n\nProblem Set 3\n\nNo problem set 2\nAssigned: week 9, March 20\nDue: Following week.\n\nReplication Class II\n\nIn group\nSelect you paper this week\nMore rigorous ~&gt; Contact the author soon to get the raw data!\nClass: April 03\n\nFinal Project\n\nProposal: EOD Friday, Week 9, March 20\n\nyou have to meet with me before submitting your proposal\nSend me a draft of the proposal before the meeting\n\nPresentation: Week 14."
  },
  {
    "objectID": "slides/week_06.html#where-are-we",
    "href": "slides/week_06.html#where-are-we",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Where are we?",
    "text": "Where are we?\nWe started from pre-processing text as data, representing text as numbers, describing features of the tex, and learned how to measure concepts in text:\n\n\nLast Week: Supervised learning ~ Training your models\n\nCrowdsourcing label classification\nFull pipeline for model training\nRegularized regressions\nEvaluating Performance\n\nThis week: unsupervised learning\n\nBegin to take a purely inductive approach\nDiscovery\nLook for things we don’t know about in the text."
  },
  {
    "objectID": "slides/week_06.html#overview-unsupervised-learning",
    "href": "slides/week_06.html#overview-unsupervised-learning",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Overview: Unsupervised Learning",
    "text": "Overview: Unsupervised Learning\n\nData: humans, documents, votes, etc. are not pre-labelled in terms of some underlying concept.\n\nThink about congressional speeches, we know the author, their party, other metadata, but:\n\nwe don’t yet know what that speech ‘represents’ in terms of its latent properties, what ‘kind’ of speech it is, what ‘topics’ it covers, what speeches it is similar to conceptually, etc.\n\n\nGoal is to take the observations and find hidden structure and meaning in them.\n\nsimilarity\ngroups\ntopics\nassociation between word, etc…"
  },
  {
    "objectID": "slides/week_06.html#main-challenges-of-topic-models",
    "href": "slides/week_06.html#main-challenges-of-topic-models",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Main challenges of Topic Models",
    "text": "Main challenges of Topic Models\n\n\nHard to get it right\n\n\nunsupervised learning requires several ad-hoc decisions and these decisions matter for quality of your results\n\nnumber of clusters\nnumber of topics\npre-processing steps\n\nDomain knowledge (and honestly a bit of randomness) guides a lot of these decisions\n\n\n\n\n\nHard to know if you are doing right!\n\n\nin contrast to supervised approaches, we won’t know ‘how correct’ the output is in a simple statistical sense\nuse statistical measures of fit/unfit of different modeling decisions\n\nbut in general, it will involve a hugely amount of qualitative assessment.\n\nNo easy measure of acccuracy, recall and precision."
  },
  {
    "objectID": "slides/week_06.html#k-means-clustering",
    "href": "slides/week_06.html#k-means-clustering",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "K-means Clustering",
    "text": "K-means Clustering\nPurpose: look for ‘groups’ in data explicitly.\n\n\nInput: text + number of clusters\nOutput: documents ~&gt; clusters"
  },
  {
    "objectID": "slides/week_06.html#visually",
    "href": "slides/week_06.html#visually",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Visually",
    "text": "Visually"
  },
  {
    "objectID": "slides/week_06.html#visually-1",
    "href": "slides/week_06.html#visually-1",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Visually",
    "text": "Visually"
  },
  {
    "objectID": "slides/week_06.html#visually-2",
    "href": "slides/week_06.html#visually-2",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Visually",
    "text": "Visually"
  },
  {
    "objectID": "slides/week_06.html#visually-3",
    "href": "slides/week_06.html#visually-3",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Visually",
    "text": "Visually"
  },
  {
    "objectID": "slides/week_06.html#cluster-methods-vs-topic-models",
    "href": "slides/week_06.html#cluster-methods-vs-topic-models",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Cluster Methods vs Topic Models",
    "text": "Cluster Methods vs Topic Models\nTopics models can be thought as a probabilistic generalization of of clustering methods\n\nClustering:\n\nEvery document is assigned to a cluster\n\nTopic Models:\n\nevery document has a probability distribution of topic.\nevery topic has a probability distribution of words."
  },
  {
    "objectID": "slides/week_06.html#topid-models-intuition",
    "href": "slides/week_06.html#topid-models-intuition",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Topid Models: Intuition",
    "text": "Topid Models: Intuition\n\nCapture words that are more likely to occur together across a set of documents.\nAssign these words a probability of being part of a cluster (topic).\nAssign documents a probability of being associated of these clusters.\n\nDocuments: formed from probability distribution of topics\n\na speech can be 40% about trade, 30% about sports, 10% about health, and 20% spread across topics you don’t think make much sense\n\nTopics: fromed from probability distribution over words\n\nthe topic health will have words like hospital, clinic, dr., sick, cancer"
  },
  {
    "objectID": "slides/week_06.html#blei-2012",
    "href": "slides/week_06.html#blei-2012",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Blei, 2012,",
    "text": "Blei, 2012,"
  },
  {
    "objectID": "slides/week_06.html#intuition-language-model",
    "href": "slides/week_06.html#intuition-language-model",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Intuition: Language Model",
    "text": "Intuition: Language Model\n\nStep 1: For each document:\n\nRandomly choose a distribution over topics. That is, choose one of many multinomial distributions, each which mixes the topics in different proportions.\n\nStep 2: Then, for every word in the document\n\nRandomly choose a topic from the distribution over topics from step 1.\nRandomly choose a word from the distribution over the vocabulary that the topic implies."
  },
  {
    "objectID": "slides/week_06.html#step-1-or-what-a-multinomial-distribution-looks-like",
    "href": "slides/week_06.html#step-1-or-what-a-multinomial-distribution-looks-like",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Step 1: or what a multinomial distribution looks like",
    "text": "Step 1: or what a multinomial distribution looks like\nFor each document ~ Randomly choose a distribution over topics from a multinomal distribution"
  },
  {
    "objectID": "slides/week_06.html#step-2-sampling-words",
    "href": "slides/week_06.html#step-2-sampling-words",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Step 2: sampling words",
    "text": "Step 2: sampling words\nFor every word:\n\n\nRandomly choose a topic from the distribution over topics from step 1.\nRandomly choose a word from the distribution over the vocabulary that the topic implies."
  },
  {
    "objectID": "slides/week_06.html#latent-dirichlet-allocation",
    "href": "slides/week_06.html#latent-dirichlet-allocation",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Latent Dirichlet Allocation",
    "text": "Latent Dirichlet Allocation\nTo estimate the model, we need to assume some known mathematical distributions for this data generating process:\n\nFor every topic:\n\n\\(\\beta_k \\sim \\text{Dirichlet}(\\tau)\\)\n\nFor every document:\n\n\\(\\theta_d \\sim \\text{Dirichlet}(\\alpha)\\),\n\nFor every word:\n\na topic \\(z_{dn} \\sim \\text{Multinomial}(\\theta_d)\\).\na word \\(w_{dn} \\sim \\text{Multinomial}(\\beta_{z_{dn}})\\)\n\nwhere:\n\n\\(\\alpha\\) and \\(\\tau\\) are hyperparameter of the Dirichlet priors\n\\(\\beta_k\\) is drawn from a Dirichlet for per-topic word distribution\n\\(\\theta_d\\) is drawn from a Dirichlet for the topic distribution for documents - \\(K\\) topics\n\\(D\\) documents in the corpus\n\\(N_d\\) words in document \\(d\\)"
  },
  {
    "objectID": "slides/week_06.html#aside-dirichlet-distribution",
    "href": "slides/week_06.html#aside-dirichlet-distribution",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Aside: Dirichlet distribution",
    "text": "Aside: Dirichlet distribution\n\nThe Dirichlet distribution is a conjugate prior for the multinomial distribution.\n\nIt makes joint distributions easier to calculate because we know their families.\n\nIt is parameterized by a vector of positive real numbers (\\(alpha\\))\n\nLarger values of \\(\\alpha\\) (assuming we are in symmetric case) mean we think (a priori) that documents are generally an even mix of the topics.\nIf \\(\\alpha\\) is small (less than 1) we think a given document is generally from one or a few topics."
  },
  {
    "objectID": "slides/week_06.html#visually-4",
    "href": "slides/week_06.html#visually-4",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Visually",
    "text": "Visually\n\n\n\n\n\n\n\nSource: Andrew Heiss"
  },
  {
    "objectID": "slides/week_06.html#exercise-plate-notation",
    "href": "slides/week_06.html#exercise-plate-notation",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Exercise: Plate Notation",
    "text": "Exercise: Plate Notation"
  },
  {
    "objectID": "slides/week_06.html#inference-how-to-estimate-all-these-parameters",
    "href": "slides/week_06.html#inference-how-to-estimate-all-these-parameters",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Inference: How to estimate all these parameters?",
    "text": "Inference: How to estimate all these parameters?\nUse the observed data, the words, to make an inference about the latent parameters: the \\(\\beta\\)s, the \\(z\\)s, the \\(\\theta\\)s.\nWe start with the joint distribution implied by our language model (Blei, 2012):\n\\[\np(\\beta_{1:K}, \\theta_{1:D}, z_{1:D}, w_{1:D})= \\prod_{K}^{i=1}p(\\beta_i)\\prod_{D}^{d=1}p(\\theta_d)(\\prod_{N}^{n=1}p(z_{d,n}|\\theta_d)p(w_{d,n}|\\beta_{1:K},z_{d,n})\n\\]\nTo get to the conditional:\n\\[\np(\\beta_{1:K}, \\theta_{1:D}, z_{1:D}|w_{1:D})=\\frac{p(\\beta_{1:K}, \\theta_{1:D}, z_{1:D}, w_{1:D})}{p(w_{1:D})}\n\\]\nThe denominator is hard complicate to be estimate (requires integration for every word for every topic):\n\nSimulate with Gibbs Sampling or Variational Inference.\nTake a Bayesian statistic course to learn more about this type of inference!"
  },
  {
    "objectID": "slides/week_06.html#show-me-results",
    "href": "slides/week_06.html#show-me-results",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Show me results!",
    "text": "Show me results!"
  },
  {
    "objectID": "slides/week_06.html#choosing-the-number-of-topics",
    "href": "slides/week_06.html#choosing-the-number-of-topics",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Choosing the number of topics",
    "text": "Choosing the number of topics\n\n\nChoosing K is “one of the most difficult questions in unsupervised learning” (Grimmer and Stewart, 2013, p.19)\nCommon approach: decide based on cross-validated statistical measures model fit or other measures of topic quality."
  },
  {
    "objectID": "slides/week_06.html#validation-of-topics",
    "href": "slides/week_06.html#validation-of-topics",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Validation of topics",
    "text": "Validation of topics\n\nWorking with topic models require a lot of back-and-forth and humans in the loop.\nHow to measure the quality of the topics?\n\n\n\n\nCrowdsourcing for:\n\nwhether a topic has (human-identifiable) semantic coherence: word intrusion, asking subjects to identify a spurious word inserted into a topic\nwhether the association between a document and a topic makes sense: topic intrusion, asking subjects to identify a topic that was not associated with the document by the model\nSee Ying et al, 2022, Political Analysis."
  },
  {
    "objectID": "slides/week_06.html#barbera-et-al-american-political-science-review-2020.",
    "href": "slides/week_06.html#barbera-et-al-american-political-science-review-2020.",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Barbera et al, American Political Science Review, 2020.",
    "text": "Barbera et al, American Political Science Review, 2020.\n\nData: tweets sent by US legislators, samples of the public, and media outlets.\nLDA with K = 100 topics\nTopic predictions are used to understand agenda-setting dynamics (who leads? who follows?)\nConclusion: Legislators are more likely to follow, than to lead, discussion of public issues,\nDecisions:\nk=100"
  },
  {
    "objectID": "slides/week_06.html#motolinia-american-political-science-review-2021",
    "href": "slides/week_06.html#motolinia-american-political-science-review-2021",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Motolinia, American Political Science Review, 2021",
    "text": "Motolinia, American Political Science Review, 2021\n\nData: transcripts of legislative sessions in Mexican states\nCorrelated Topic model to identify “particularistic” legislation; i.e. laws with clear benefits to voters\nEach topic is then classified into particularistic or not\nValidation: correlation with spending\nUse exogenous electoral reform that allowed legislators to be re-elected"
  },
  {
    "objectID": "slides/week_06.html#exercise",
    "href": "slides/week_06.html#exercise",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Exercise",
    "text": "Exercise\n\nTake 5-min to read the methods sections of this paper\nList to me some of the decisions the authors need to make to get the topic models to work.\nDo you think these make sense? What would you do different"
  },
  {
    "objectID": "slides/week_06.html#extensions-many-more-beyond-lda",
    "href": "slides/week_06.html#extensions-many-more-beyond-lda",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Extensions: Many more beyond LDA",
    "text": "Extensions: Many more beyond LDA\n\nStructural topic model: allow (1) topic prevalence, (2) topic content to vary as a function of document-level covariates (e.g., how do topics vary over time or documents produced in 1990 talk about something differently than documents produced in 2020?); implemented in stm in R (Roberts, Stewart, Tingley, Benoit)\nCorrelated topic model: way to explore between-topic relationships (Blei and Lafferty, 2017); implemented in topicmodels in R; possibly somewhere in Python as well!\nKeyword-assisted topic model: seed topic model with keywords to try to increase the face validity of topics to what you’re trying to measure; implemented in keyATM in R (Eshima, Imai, Sasaki, 2019)\nBertTopic: BERTopic is a topic modeling technique that leverages transformers and TF-IDF to create dense clusters of words."
  },
  {
    "objectID": "slides/week_06.html#stm-adding-structure-to-the-lda",
    "href": "slides/week_06.html#stm-adding-structure-to-the-lda",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "STM: Adding Structure to the LDA",
    "text": "STM: Adding Structure to the LDA\n\n\n\n\n\n\n\n\n  \n\n\nPrevalence: Prior on the mixture over topics is now document-specific, and can be a function of covariates.\nContent: distribution over words is now document-specific and can be a function of covariates.\n\nSee Roberts et al 2014"
  },
  {
    "objectID": "slides/week_06.html#keyword-assisted-topic-model-summary",
    "href": "slides/week_06.html#keyword-assisted-topic-model-summary",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Keyword-assisted topic model: summary",
    "text": "Keyword-assisted topic model: summary"
  },
  {
    "objectID": "slides/week_06.html#keyword-assisted-topic-model-performance",
    "href": "slides/week_06.html#keyword-assisted-topic-model-performance",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Keyword-assisted topic model: performance",
    "text": "Keyword-assisted topic model: performance"
  },
  {
    "objectID": "slides/week_03.html#housekeeping",
    "href": "slides/week_03.html#housekeeping",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Housekeeping",
    "text": "Housekeeping\nThanks for selecting the articles for replication exercise! Next steps:\n\n\nGet access to the data and code ASAP\n\nHarvard Dataverse, see footnotes in the papers, contact the authors\nany issue, please, talk to me!\n\nIf the data is too big for your laptop, use a sample of the data.\nIf the paper has much more than the text analysis, ignore it, just focus on the TAD component.\nAny questions?"
  },
  {
    "objectID": "slides/week_03.html#where-are-we",
    "href": "slides/week_03.html#where-are-we",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Where are we?",
    "text": "Where are we?\nThree primary challenges dealing with text as data:\n\nChallenge I: Text is high dimensional\nChallenge II: Text is unstructured data source\nChallenge III: Outcomes live in the latent space\n\n\nLast week:\n\nPre-processing text + bag of words ~&gt; reduces greatly text complexity (dimensions)\nText representation using vectors of numbers ~&gt; document feature matrix (text to numbers)"
  },
  {
    "objectID": "slides/week_03.html#plans-for-today",
    "href": "slides/week_03.html#plans-for-today",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Plans for today",
    "text": "Plans for today\nWe will start thinking about latent outcomes. Our first approach will focus on descriptive inference about documents:\n\nComparing documents\nUsing similarity to measure text-reuse\nEvaluating complexity in text\nWeighting (TF-iDF)"
  },
  {
    "objectID": "slides/week_03.html#recall-vector-space-model",
    "href": "slides/week_03.html#recall-vector-space-model",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Recall: Vector space model",
    "text": "Recall: Vector space model\nTo represent documents as numbers, we will use the vector space model representation:\n\n\nA document \\(D_i\\) is represented as a collection of features \\(W\\) (words, tokens, n-grams, etc…)\nEach feature \\(w_i\\) can be placed in a real line\nA document \\(D_i\\) is a point in a \\(\\mathbb{R}^W\\)\n\nEach document is now a vector,\nEach entry represents the frequency of a particular token or feature.\nStacking those vectors on top of each other gives the document feature matrix (DFM)."
  },
  {
    "objectID": "slides/week_03.html#document-feature-matrix-fundamental-unit-of-tad",
    "href": "slides/week_03.html#document-feature-matrix-fundamental-unit-of-tad",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Document-Feature Matrix: fundamental unit of TAD",
    "text": "Document-Feature Matrix: fundamental unit of TAD\n\n\n\n\n\n\n\n\n\n\n\nSource: Arthur Spirling TAD Class"
  },
  {
    "objectID": "slides/week_03.html#in-a-two-dimensional-space",
    "href": "slides/week_03.html#in-a-two-dimensional-space",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "In a two dimensional space",
    "text": "In a two dimensional space\n\n\n\nDocuments, W=2\n\n\nDocument 1 = “yes yes yes no no no”\nDocument 2 = “yes yes yes yes yes yes”"
  },
  {
    "objectID": "slides/week_03.html#how-far-is-document-a-from-document-b",
    "href": "slides/week_03.html#how-far-is-document-a-from-document-b",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "How `far’ is document a from document b?",
    "text": "How `far’ is document a from document b?\nUsing the vector space, we can use notions of geometry to build well-defined comparison/similarity measures between the documents.\n\n\nin multiple dimensions!!"
  },
  {
    "objectID": "slides/week_03.html#euclidean-distance",
    "href": "slides/week_03.html#euclidean-distance",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Euclidean Distance",
    "text": "Euclidean Distance\nThe ordinary, straight line distance between two points in space. Using document vectors \\(y_a\\) and \\(y_b\\) with \\(j\\) dimensions\n\n\n\nEuclidean Distance\n\n\n\\[\n||y_a - y_b|| = \\sqrt{\\sum^{j}(y_{aj} - y_{bj})^2}\n\\]\n\n\n\nCan be performed for any number of features J ~ has nice mathematical properties\n\nno negative distances: sij   0 2 distance between documents is zero () documents are identical 3 distance between documents is symmetric: sij = sji 4 measures satisfy triangle inequality. sik   sij + sjk"
  },
  {
    "objectID": "slides/week_03.html#euclidean-distance-w2",
    "href": "slides/week_03.html#euclidean-distance-w2",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Euclidean Distance, w=2",
    "text": "Euclidean Distance, w=2\n\n\n\nEuclidean Distance\n\n\n\\[\n||y_a - y_b|| = \\sqrt{\\sum^{j}(y_{aj} - y_{bj})^2}\n\\]\n\n\n\n\n\\(y_a\\) = [0, 2.51, 3.6, 0] and \\(y_b\\) = [0, 2.3, 3.1, 9.2]\n\\(\\sum_{j=1}^j (y_a - y_b)^2\\) = \\((0-0)^2 + (2.51-2.3)^2 + (3.6-3.1)^2 + (9-0)^2\\) = \\(84.9341\\)\n\\(\\sqrt{\\sum_{j=1}^j (y_a - y_b)^2}\\) = 9.21"
  },
  {
    "objectID": "slides/week_03.html#exercise",
    "href": "slides/week_03.html#exercise",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Exercise",
    "text": "Exercise\n\n\n\nDocuments, W=3 {yes, no}\n\n\nDocument 1 = “yes yes yes no no no” (3, 3)\nDocument 2 = “yes yes yes yes yes yes” (6,0)\nDocument 3= “yes yes yes no no no yes yes yes no no no yes yes yes no no no yes yes yes no no no” (12, 12)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhich documents will the euclidean distance place closer together?\nDoes it look like a good measure for similarity?\n\nDoc C = Doc A * 3"
  },
  {
    "objectID": "slides/week_03.html#cosine-similarity",
    "href": "slides/week_03.html#cosine-similarity",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Cosine Similarity",
    "text": "Cosine Similarity\nEuclidean distance rewards magnitude, rather than direction\n\\[\n\\text{cosine similarity}(\\mathbf{y_a}, \\mathbf{y_b}) = \\frac{\\mathbf{y_a} \\cdot \\mathbf{y_b}}{\\|\\mathbf{y_a}\\| \\|\\mathbf{y_b}\\|}\n\\]\nUnpacking the formula:\n\n\\(\\mathbf{y_a} \\cdot \\mathbf{y_b}\\) ~ dot product between vectors\n\nprojecting common magnitudes\nmeasure of similarity (see textbook)\n\\(\\sum_j{y_{aj}*y_{bj}}\\)\n\n\\(||\\mathbf{y_a}||\\) ~ vector magnitude, length ~ \\(\\sqrt{\\sum{y_{aj}^2}}\\)\nnormalizes similarity by documents’ length ~ independent of document length be because it deals only with the angle of the vectors\ncosine similarity captures some notion of relative direction (e.g. style or topics in the document)"
  },
  {
    "objectID": "slides/week_03.html#cosine-similarity-1",
    "href": "slides/week_03.html#cosine-similarity-1",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Cosine Similarity",
    "text": "Cosine Similarity\nCosine function has a range between -1 and 1.\n\nConsider: cos (0) = 1, cos (90) = 0, cos (180) = -1"
  },
  {
    "objectID": "slides/week_03.html#exercise-1",
    "href": "slides/week_03.html#exercise-1",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Exercise",
    "text": "Exercise\nThe cosine function can range from [-1, 1]. When thinking about document vectors, cosine similarity is actually constrained to vary only from 0 - 1.\n\nWhy does cosine similarity for document vectors can never be lower than zero? Think about the vector representation and the document feature matrix."
  },
  {
    "objectID": "slides/week_03.html#more-metrics",
    "href": "slides/week_03.html#more-metrics",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "More metrics",
    "text": "More metrics\nThere are a large number of distance/similarity metrics out there, just to name a few:\n\nJaccard Similarity: overlap between documents\nManhattan Distance: absolute distance between documents\nCanberra Distance: Weighted version of Manhattan Distance\nMinowski: generalized version of Euclidean\n\nNo single best measure, depends on your research question."
  },
  {
    "objectID": "slides/week_03.html#mozer-et-al-2020-matching-with-text-data",
    "href": "slides/week_03.html#mozer-et-al-2020-matching-with-text-data",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Mozer et al, 2020 ‘Matching with Text Data’",
    "text": "Mozer et al, 2020 ‘Matching with Text Data’\nBut some recent research show Document Feature Matrix (DTM) + Cosine similarity works well to perceived similarity on documents"
  },
  {
    "objectID": "slides/week_03.html#linder-et.-al-2020---text-as-policy",
    "href": "slides/week_03.html#linder-et.-al-2020---text-as-policy",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Linder et. al, 2020 - Text as Policy",
    "text": "Linder et. al, 2020 - Text as Policy\n\n\n\n\n\n\n\n\n\n\n\n\nHow is cosine similarity used in the application?\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nMain assumption: text reuse serves as a summary measure of the greatest overlap observed across all relevant policy dimensions represented in legislative text.\nHow? Identifying large segments of equivalent or highly similar text.\nSmith-Waterman local alignment algorithm\n\nlongest sequence of overlap allowing for gaps and mismatches\n\nThe SW algorithm amounts to a systematic procedure for scoring similar sequences of text, and efficiently finding the highest scoring sequences in two documents.\nAnalysis:\n\nSelection: Elastic search for 500 candidates\nSmith Waterman alignment algorithm\nDowneight give average cosine dissimilarity between allignment and a random sample of 1000 other alignments. If the alignment is everywhere, downweight."
  },
  {
    "objectID": "slides/week_03.html#lexical-diversity",
    "href": "slides/week_03.html#lexical-diversity",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Lexical Diversity",
    "text": "Lexical Diversity\n\nLength refers to the size in terms of: characters, words, lines, sentences, paragraphs, pages, sections, chapters, etc.\nTokens are generally words ~ useful semantic unit for processing\nTypes are unique tokens.\nTypically \\(N_{tokens}\\) &gt;&gt;&gt;&gt; \\(N_{types}\\)\n\n\nType-to-Token ratio\n\\[ TTR: \\frac{\\text{total type}}{\\text{total tokens}} \\]\nSo… authors with limited vocabularies will have a low lexical diversity"
  },
  {
    "objectID": "slides/week_03.html#issues-with-ttr-and-extensions",
    "href": "slides/week_03.html#issues-with-ttr-and-extensions",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Issues with TTR and Extensions",
    "text": "Issues with TTR and Extensions\n\nTTR is very sensitive to overall document length,\n\nshorter texts may exhibit fewer word repetitions\n\nLength also correlates with topic variation ~ more types being added to the document\n\n\n\nOther Measures\n\nGuiraud: \\(\\frac{\\text{total type}}{\\sqrt{\\text{total tokens}}}\\)\nS Summer’s Index: \\(\\frac{\\text{log(total type)}}{\\text{log(total tokens)}}}\\)\nMTTR: the Moving-Average Type-Token Ratio (Covington and McFall, 2010)"
  },
  {
    "objectID": "slides/week_03.html#readability",
    "href": "slides/week_03.html#readability",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Readability",
    "text": "Readability\nAnother way to think about textual complexity is to consider readability.\nReadability: ease with which reader (especially of given education) can comprehend a text\n\n\nCombines both difficulty (text) and sophistication (reader)\n\nUse a combination of syllables and sentence length to indicate difficulty\nHuman inputs to built parameters\nFlesch-Kincaid readability index\n\nMeasurement problems from education research\naverage grade of students who could correctly answer at least 75% of some multiple-choice questions"
  },
  {
    "objectID": "slides/week_03.html#flesch-kincaid-readability-index",
    "href": "slides/week_03.html#flesch-kincaid-readability-index",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Flesch-Kincaid readability index",
    "text": "Flesch-Kincaid readability index\n\n\n\nFlesch Reading Ease (FRE)\n\n\n\\[\nFRE = 206.835 - 1.015\\left(\\frac{\\mbox{total words}}{\\mbox{total sentences}}\\right)-84.6\\left(\\frac{\\mbox{total syllables}}{\\mbox{total words}}\\right)\n\\]\n\n\n\n\n\n\nFlesch-Kincaid (Rescaled to US Educational Grade Levels)\n\n\n\\[\nFRE = 15.59 - 0.39\\left(\\frac{\\mbox{total words}}{\\mbox{total sentences}}\\right)- 11.8\\left(\\frac{\\mbox{total syllables}}{\\mbox{total words}}\\right)\n\\]\n\n\n\nInterpretation: 0-30: university level; 60-70: understandable by 13-15 year olds; and 90-100 easily understood by an 11-year old student."
  },
  {
    "objectID": "slides/week_03.html#spirling-2016.-the-effects-of-the-second-reform-act",
    "href": "slides/week_03.html#spirling-2016.-the-effects-of-the-second-reform-act",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Spirling, 2016. The effects of the Second Reform Act",
    "text": "Spirling, 2016. The effects of the Second Reform Act"
  },
  {
    "objectID": "slides/week_03.html#benoit-et-al.-2019-political-sophistication",
    "href": "slides/week_03.html#benoit-et-al.-2019-political-sophistication",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Benoit et al., 2019, Political Sophistication",
    "text": "Benoit et al., 2019, Political Sophistication\nApproach\n\nGet human judgments of relative textual easiness for specifically political texts.\nUse a logit model to estimate latent “easiness” as equivalent to the “ability” parameter in the Bradley-Terry framework.\nUse these as training data for a tree-based model. Pick most important parameters\nRe-estimate the models using these covariates (Logit + covariates)\nUsing these parameters, one can “predict” the easiness parameter for a given new text\n - Nice plus ~ add uncertainty to model-based estimates via bootstrapping"
  },
  {
    "objectID": "slides/week_03.html#benoit-et-al.-2019-political-sophistication-1",
    "href": "slides/week_03.html#benoit-et-al.-2019-political-sophistication-1",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Benoit et al., 2019, Political Sophistication",
    "text": "Benoit et al., 2019, Political Sophistication"
  },
  {
    "objectID": "slides/week_03.html#can-we-do-better-than-just-using-frequencies",
    "href": "slides/week_03.html#can-we-do-better-than-just-using-frequencies",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Can we do better than just using frequencies?",
    "text": "Can we do better than just using frequencies?\nSo far our inputs for the vector representation of documents have relied simply the word frequencies.\nCan we do better?\n\nOne option: weighting\nWeights:\n\nReward words more unique;\nPunish words that appear in most documents\n\n\n\n\n\nTF-IDF = Term Frequency - Inverse Document Frequency\n\n\n\\(\\text{TF-IDF}(t, d) = \\text{TF}(t, d) \\times \\text{IDF}(t)\\) - \\(\\text{TF}(t, d) = \\frac{\\text{Number of times term } t \\text{ appears in document } d}{\\text{Total number of terms in document } d}\\) - \\(\\text{IDF}(t) = \\log \\left( \\frac{\\text{Total number of documents}}{\\text{Number of documents with term } t \\text{ in them}} \\right)\\)"
  },
  {
    "objectID": "slides/week_03.html#federalist-papers",
    "href": "slides/week_03.html#federalist-papers",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Federalist Papers",
    "text": "Federalist Papers\n\n\n\n\n\n     \n\n\nSource: Grimmer, Roberts, and Stewart, Text as Data, 2022"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PPOL 6801: Text as Data - Computational Linguistics",
    "section": "",
    "text": "This course introduces students to the quantitative analysis of text as data. With the increasing availability of large-scale textual data – from government documents and political speeches to social media and online news – the potential to extract meaningful insights from text has expanded greatly. In this course, students will learn to transform text into data and apply it to social science questions and theories. Key topics include text representation, sentiment analysis, scaling text on ideological and policy dimensions, using machine learning for text classification, word embeddings and using Large Language Models (GPT, Bert, Llama, Gemini, etc..) for social science research. Although we will cover LLMs, the focus will be on applied methods and transfer learning rather than an in-depth coverage of building LLMs from scratch.\nThe course includes hands-on exercises using real-world data to reinforce lecture content. By the end, students will have a toolkit for text analysis useful in roles as policy experts and computational social scientists. Students should have completed at least an introductory statistics course and have a basic understanding of probability, distributions, hypothesis testing, and linear models. Students are also expected to have experience working with R, the programming language and software environment of this course, and Python for the LLM’s components of the course."
  },
  {
    "objectID": "index.html#course-description",
    "href": "index.html#course-description",
    "title": "PPOL 6801: Text as Data - Computational Linguistics",
    "section": "",
    "text": "This course introduces students to the quantitative analysis of text as data. With the increasing availability of large-scale textual data – from government documents and political speeches to social media and online news – the potential to extract meaningful insights from text has expanded greatly. In this course, students will learn to transform text into data and apply it to social science questions and theories. Key topics include text representation, sentiment analysis, scaling text on ideological and policy dimensions, using machine learning for text classification, word embeddings and using Large Language Models (GPT, Bert, Llama, Gemini, etc..) for social science research. Although we will cover LLMs, the focus will be on applied methods and transfer learning rather than an in-depth coverage of building LLMs from scratch.\nThe course includes hands-on exercises using real-world data to reinforce lecture content. By the end, students will have a toolkit for text analysis useful in roles as policy experts and computational social scientists. Students should have completed at least an introductory statistics course and have a basic understanding of probability, distributions, hypothesis testing, and linear models. Students are also expected to have experience working with R, the programming language and software environment of this course, and Python for the LLM’s components of the course."
  },
  {
    "objectID": "index.html#class-website",
    "href": "index.html#class-website",
    "title": "PPOL 6801: Text as Data - Computational Linguistics",
    "section": "Class Website",
    "text": "Class Website\nhttps://tiagoventura.github.io/PPOL_6801/"
  },
  {
    "objectID": "index.html#instructor",
    "href": "index.html#instructor",
    "title": "PPOL 6801: Text as Data - Computational Linguistics",
    "section": "Instructor",
    "text": "Instructor\nProfessor Tiago Ventura\n\nPronouns: He/Him\nEmail: tv186@georgetown.edu\nOffice hours: Every Tuesday, 9am - 12pm\nLocation: McCourt Building, 766\n\n\n\n\n\n\n\nWhen should I go to your office hours?\n\n\n\n\n\n\nYou are all welcome to the office hours. You can come to the office hours to:\n\nDrink some coffee;\nAsk what I am doing research at;\nTell me about your research;\nAsk any question about our class;\nOr just talk about soccer.\n\nAll are valid options! And no need to schedule time with me!"
  },
  {
    "objectID": "index.html#course-infra-structure",
    "href": "index.html#course-infra-structure",
    "title": "PPOL 6801: Text as Data - Computational Linguistics",
    "section": "Course Infra-structure",
    "text": "Course Infra-structure\nClass Website: This class website will be used throughout the course and should be checked on a regular basis for lecture materials and required readings.\nClass Slack Channel: The class also has a dedicated slack channel. The channel serves as an open forum to discuss, collaborate, pose problems/questions, and offer solutions. Students are encouraged to pose any questions they have there as this will provide the professor the means of answering the question so that all can see the response. If you’re unfamiliar with, please consult the following start-up tutorial https://get.slack.help/hc/en-us/articles/218080037-Getting-started-for-new-members. Please follow the invite link to be added to the Slack channel.\nCanvas: A Canvas site http://canvas.georgetown.edu will be used throughout the course and should be checked on a regular basis for announcements. Materials will be posted here, and not on canvas, or distributed in class or by e-mail. Support for Canvas is available at (202) 687-4949"
  },
  {
    "objectID": "index.html#credits",
    "href": "index.html#credits",
    "title": "PPOL 6801: Text as Data - Computational Linguistics",
    "section": "Credits",
    "text": "Credits\nTo build this course, I used materials from Arthur Spirling Text-as-Data Class at NYU, and lab materials from various TA’s for his course (Elisa Wirsching, Lucia Motolinia, Pedro L. Rodriguez, Kevin Munger, Patrick Chester, Leslie Huang), Pablo Barbera’s Computational Social Science Seminar seminar, Brandon Stewart, Alex Siegel, Chris Bail, Sebastian Vallejo, among others. Their lessons and inspiration are spread throughout all the materials of the course. Thanks!"
  },
  {
    "objectID": "weeks/week-03.html",
    "href": "weeks/week-03.html",
    "title": "Week 03",
    "section": "",
    "text": "Topics\n\nMore about the vector space model\nHow do we compare texts?\nHow do we evaluate similarity in text?\nHow do we evaluate complexity in text?\nWhy should we care about complexity in text?\n\n\n\nReadings\nRequired Readings\n\n[GMB] - Chapter 7\nApplied Papers:\n\nSpirling, Arthur. 2016. “Democratization and Linguistic Complexity”, Journal of Politics.\nBenoit, K., Munger, K. and Spirling, A. 2017. Measuring and Explaining Political Sophistication Through Textual Complexity\nLinder, Fridolin, Bruce Desmarais, Matthew Burgess, and Eugenia Giraudy. “Text as policy: Measuring policy similarity through bill text reuse.” Policy Studies Journal 48, no. 2 (2020): 546-574.\n\n\nCoding Materials\n\n Week 3 - Descriptive Inference \n\n\n\nSlides\n\nWeek 3 - Descriptive Inference"
  },
  {
    "objectID": "weeks/week-12.html",
    "href": "weeks/week-12.html",
    "title": "Week 12",
    "section": "",
    "text": "Topics\n\nWe will learn about the Transformers architecture, attention, and the encoder-coder infrastructure.\n\n\n\nReadings\nRequired Readings\n\n[SLP] - Chapter 10.\nJay Alammar. 2018. “The Illustrated Transformer.” https://jalammar.github.io/illustratedtransformer/\nVaswani, A., et al. (2017). Attention is all you need. Advances in neural information processing systems, 30;\nTimoneda and Vera, BERT, RoBERTa or DeBERTa? Comparing Performance Across Transformer Models in Political Science Text, Forthcoming Journal of Politics.\n\nClass Materials\n\nCode here: https://svallejovera.github.io/cpa_uwo/week-9-transformers.html"
  },
  {
    "objectID": "weeks/week-07.html",
    "href": "weeks/week-07.html",
    "title": "Week 07",
    "section": "",
    "text": "Topics\n-what if we do not have an outcome to predict? - can we cluster the text in groups? - what are topics? - how to decide between different topics?\n\n\nReadings\nRequired Readings\n\n[GMB] - Chapters 12-14.\nDavid M. Blei . 2012. “Probabilistic Topic Models.” http://www.cs.columbia.edu/~blei/papers/ Blei2012.pdf\nApplied Papers:\n\nMotolinia, Lucia. Electoral accountability and particularistic legislation: evidence from an electoral reform in Mexico. American Political Science Review 115, no. 1 (2021): 97-113.\nBarberá, P., Casas, A., Nagler, J., Egan, P. J., Bonneau, R., Jost, J. T., & Tucker, J. A. (2019). Who leads? Who follows? Measuring issue attention and agenda setting by legislators and the mass public using social media data. American Political Science Review, 113(4), 883-901.\nEshima, Shusei, Kosuke Imai, and Tomoya Sasaki. “Keyword‐Assisted Topic Models.” American Journal of Political Science (2020).\n\n\nCoding Materials\n\n Week 7 - Unsupervised Learning: Topic Models\n\n\n\nSlides\n\nWeek 7 - Unsupervised Learning"
  },
  {
    "objectID": "weeks/week-05.html",
    "href": "weeks/week-05.html",
    "title": "Week 05",
    "section": "",
    "text": "Topics\nWe will study the framework to train our own supervised models, and when to use them.\n\n\nReadings\nRequired Readings\n\n[GMB] - Chapters 17, 18, 19, and 20.\nApplied Papers:\n\nBarberá, Pablo, Amber E. Boydstun, Suzanna Linn, Ryan McMahon, and Jonathan Nagler. Automated text classification of news articles: A practical guide. Political Analysis 29, no. 1 (2021): 19-42.\nSiegel, Alexandra A., Evgenii Nikitin, Pablo Barberá, Joanna Sterling, Bethany Pullen, Richard Bonneau, Jonathan Nagler, and Joshua A. Tucker. Trumping hate on Twitter? Online hate speech in the 2016 US election campaign and its aftermath. Quarterly Journal of Political Science 16, no. 1 (2021): 71-104.\nTheocharis, Y., Barberá, P., Fazekas, Z., & Popa, S. A. (2020). The dynamics of political incivility on Twitter. Sage Open, 10(2), 2158244020919447.\nMitts, T., Phillips, G., & Walter, B. (2021). Studying the Impact of ISIS Propaganda Campaigns. Journal of Politics\n\n\nCoding Materials\n\n Week 5 - Supervised Learning: Training your own classifiers\n\n\n\nSlides\n\nWeek 5 - Supervised Learning"
  },
  {
    "objectID": "weeks/week-10.html",
    "href": "weeks/week-10.html",
    "title": "Week 10",
    "section": "",
    "text": "Topics\n\nWhat are scaling models and what can they tell us?\nCan we represent politicians/users ideology using text?\n\n\n\nReadings\nRequired Readings\n\nLaver, Michael, Kenneth Benoit, and John Garry. 2003. “Extracting Policy Positions from Political Texts Using Words as Data”. American Political Science Review. 97, 2, 311-331\nSlapin, Jonathan and Sven-Oliver Prokschk. 2008. “A Scaling Model for Estimating Time-Series Party Positions from Texts.” American Journal of Political Science. 52, 3 705-722\n\nNon-Required Readings\n\nBarberá, Pablo. “Birds of the same feather tweet together: Bayesian ideal point estimation using Twitter data.” Political analysis 23, no. 1 (2015): 76-91.Harvard\nAruguete, Natalia, Ernesto Calvo, and Tiago Ventura. “News by popular demand: Ideological congruence, issue salience, and media reputation in news sharing.” The International Journal of Press/Politics 28, no. 3 (2023): 558-579.\nRheault, Ludovic, and Christopher Cochrane. “Word embeddings for the analysis of ideological placement in parliamentary corpora.” Political Analysis 28, no. 1 (2020): 112-133.\nIzumi, Mauricio Y., and Danilo B. Medeiros. “Government and opposition in legislative speechmaking: using text-as-data to estimate Brazilian political parties’ policy positions.” Latin American Politics and Society 63, no. 1 (2021): 145-164.\n\nCoding Materials\n\n Week 10 - Scaling\n\n\n\nSlides\n\nWeek 10 - Scaling"
  },
  {
    "objectID": "weeks/week-09.html",
    "href": "weeks/week-09.html",
    "title": "Week 09",
    "section": "",
    "text": "Topics\n\nApplications of Word Embeddings to social science problems\n\n\n\nReadings\nRequired Readings\n\nRodman, E., 2020. A Timely Intervention: Tracking the Changing Meanings of Political Concepts with Word Vectors. Political Analysis, 28(1), pp.87-111.\nGennaro, Gloria, and Elliott Ash. “Emotion and reason in political language.” The Economic Journal 132, no. 643 (2022): 1037-1059.\nRheault, Ludovic, and Christopher Cochrane. “Word embeddings for the analysis of ideological placement in parliamentary corpora.” Political Analysis 28, no. 1 (2020): 112-133.\nAustin C. Kozlowski, Austin C., Matt Taddy, and James A. Evans. 2019. “The Geometry of Culture: Analyzing the Meanings of Class through Word Embeddings.” American Sociological Review 84, no. 5: 905–49. https://doi.org/10.1177/0003122419877135.\nGarg, Nikhil, Londa Schiebinger, Dan Jurafsky and James Zou. 2018. “Word embeddings quantify 100 years of gender and ethnic stereotypes.” Proceedings of the National Academy of Sciences 115(16):E3635–E3644.\n\nCoding Materials\n\n Week 9 - Word Embeddings\n\n\n\nSlides\n\nWeek 9 - Word Embeddings II\n\n\n\nProblem Set 2\n\nhttps://classroom.github.com/a/WcrFfilI"
  },
  {
    "objectID": "weeks/week-08.html",
    "href": "weeks/week-08.html",
    "title": "Week 08",
    "section": "",
    "text": "Topics\n\nWhat are word-embeddings?\nWhen and how can we use them?\nWhat? Topic models again?\nIs this still a bag of words?\n\n\n\nReadings\nRequired Readings\n\n[GMB] - Chapter 8.\n[SLP] Chapter 6, “Vector Semantics and Embeddings.”\nJay Alanmar, The Illustrated Word2vec\nSpirling and Rodriguez, Word embedding: What works, what doesn’t, and how to tell the difference for applied research.\n\n\n\nSlides\n\nWeek 9 - Word Embeddings"
  },
  {
    "objectID": "weeks/week-04.html",
    "href": "weeks/week-04.html",
    "title": "Week 04",
    "section": "",
    "text": "Topics\n\nWhat are dictionaries?\nWhy/when are they useful?\nWhat are their limitations?\nCan we use models trained by others and for other purposes in our classification tasks?\n\n\n\nReadings\nRequired Readings\n\n[GMB] - Chapters 15-16\nApplied Papers:\n\nLori Young and Stuart Soroka 2012 “Affective News: The Automated Coding of Sentiment in Political Texts.” Political Communication, 29:2, 205-231.\nRathje, Steve, Jay J. Van Bavel, and Sander Van Der Linden. “Out-group animosity drives engagement on social media.” Proceedings of the National Academy of Sciences 118, no. 26 (2021): e2024292118.\nVentura, Tiago, Kevin Munger, Katherine McCabe, and Keng-Chi Chang. “Connective effervescence and streaming chat during political debates.” Journal of Quantitative Description: Digital Media 1 (2021).\n\n\nCoding Materials\n\n Week 4 - Dictionaries and Off-the-Shelf Classifiers\n Week 4 - Transformers Based Classifiers \n\n\n\nProblem set 1\n\nProblem Set 1\n\n\n\nSlides\n\nWeek 4 - Dictionaries and Off-the-Shelf Classifiers"
  },
  {
    "objectID": "weeks/week-13.html",
    "href": "weeks/week-13.html",
    "title": "Week 13",
    "section": "",
    "text": "Topics\n\n\nReadings\nRequired Readings\n\nWu, Patrick Y., Joshua A. Tucker, Jonathan Nagler, and Solomon Messing. “Large language models can be used to estimate the ideologies of politicians in a zero-shot learning setting.” arXiv preprint arXiv:2303.12057 (2023).\nRathje, Steve, Dan-Mircea Mirea, Ilia Sucholutsky, Raja Marjieh, Claire Robertson, and Jay J. Van Bavel. GPT is an effective tool for multilingual psychological text analysis. (2023).\nDavidson, Thomas: Start Generating: Harnessing Generative Artificial Intelligence for Sociological Research, PrePrint\nBisbee, James, Joshua Clinton, Cassy Dorff, Brenton Kenkel, and Jennifer Larson. 2023. “Synthetic Replacements for Human Survey Data? the Perils of Large Language Models.” SocArXiv. May 4.\n\nCoding Materials\n\n Week 13 - Large Language Models - Applications \n\n\n\nSlides\n\nWeek 13 -Large Language Models"
  },
  {
    "objectID": "weeks/week-02.html",
    "href": "weeks/week-02.html",
    "title": "Week 02",
    "section": "",
    "text": "Topics\n\nHow to represent text as data?\nWhat is a Bag of Words?\nWhat are tokens?\nWhy should we care about tokens?\n\n\n\nReadings\nRequired Readings\n\nGrimmer, Justin, Margaret E. Roberts, and Brandon M. Stewart. Text as data: A new framework for machine learning and the social sciences. Princeton University Press, 2022 - Chapters 3-5\nApplied Papers:\nDenny, M. J., & Spirling, A. (2018). Text preprocessing for unsupervised learning: why it matters, when it misleads, and what to do about it. Political Analysis, 26(2): 168-189.\nBan, Pamela, Alexander Fouirnaies, Andrew B. Hall, and James M. Snyder. “How newspapers reveal political power.” Political Science Research and Methods 7, no. 4 (2019): 661-678.\nMichel, J.B., et al. 2011. “Quantitative Analysis of Culture Using Millions of Digitized Books.” Science 331 (6014): 176–82. https://doi.org/10.1126/science.1199644\n\nCoding Materials\n\n Week 2 - Text Data Processing \n\n\n\nSlides\n\nWeek 2 - From Text to Matrices: Representing Text as Data"
  },
  {
    "objectID": "weeks/week-01.html",
    "href": "weeks/week-01.html",
    "title": "Week 01",
    "section": "",
    "text": "Topics\n\nReview of syllabus\nClass organization\nGetting to know you\nIntroduction to computational text analysis.\n\n\n\nReadings\nRequired Readings\n\nGrimmer, Justin, Margaret E. Roberts, and Brandon M. Stewart. Text as data: A new framework for machine learning and the social sciences. Princeton University Press, 2022 - Chapter 2.\n\nOr one of the three below:\n\nPolitical Science Perspective: Wilkerson, J. and Casas, A. (2017). Large-Scale Computerized Text Analysis in Political Science: Opportunities and Challenges. Annual Review of Political Science, 20, 529:544.\nEconomics Perspective: Gentzkow, Matthew, Bryan Kelly, and Matt Taddy. “Text as data.” Journal of Economic Literature 57, no. 3 (2019): 535-574.\nSociology Perspective: https://www.sciencedirect.com/science/article/pii/S0049089X22000904\n\nCoding Materials\n\nNone\n\n\n\nSlides\n\nWeek 1 - Introduction"
  },
  {
    "objectID": "finalproject.html",
    "href": "finalproject.html",
    "title": "Instructions for PPOL 6801 - Final Project",
    "section": "",
    "text": "Data science is an applied field and the DSPP is particularly geared towards providing students the tools to make policy and substantive contributions using data and recent computational developments.\nIn this sense, it is fundamental that you understand how to conduct a complete analysis from collecting data, to cleaning and analyzing it, to presenting your findings.\nFor this reason, a considerable part of your grade (40%) will come from a an independent data science project, applying concepts learned throughout the course.\nThe project is composed of three parts:\n\na 2 page project proposal: (which should be discussed and approved by me)\nan in-class presentation,\nA 10-page project report.\n\nDue dates and breakdowns for the project are as follows:\n\n\n\n\n\n\n\n\n\nRequirement\nDue\nLength\nPercentage\n\n\n\n\nProject Proposal\nEOD Friday Week 9\n2 pages\n5%\n\n\nPresentation\nWeek 14\n10-15 minutes\n10%\n\n\nProject Report\nWednesday Week 15\n10 pages\n25%"
  },
  {
    "objectID": "finalproject.html#general-instructions",
    "href": "finalproject.html#general-instructions",
    "title": "Instructions for PPOL 6801 - Final Project",
    "section": "",
    "text": "Data science is an applied field and the DSPP is particularly geared towards providing students the tools to make policy and substantive contributions using data and recent computational developments.\nIn this sense, it is fundamental that you understand how to conduct a complete analysis from collecting data, to cleaning and analyzing it, to presenting your findings.\nFor this reason, a considerable part of your grade (40%) will come from a an independent data science project, applying concepts learned throughout the course.\nThe project is composed of three parts:\n\na 2 page project proposal: (which should be discussed and approved by me)\nan in-class presentation,\nA 10-page project report.\n\nDue dates and breakdowns for the project are as follows:\n\n\n\n\n\n\n\n\n\nRequirement\nDue\nLength\nPercentage\n\n\n\n\nProject Proposal\nEOD Friday Week 9\n2 pages\n5%\n\n\nPresentation\nWeek 14\n10-15 minutes\n10%\n\n\nProject Report\nWednesday Week 15\n10 pages\n25%"
  },
  {
    "objectID": "finalproject.html#all-components-of-the-project",
    "href": "finalproject.html#all-components-of-the-project",
    "title": "Instructions for PPOL 6801 - Final Project",
    "section": "All components of the project:",
    "text": "All components of the project:\n\nProject Proposal\nThe project proposal asks that you sketch out a general 2 page (single-spaced; 12pt font) project proposal. The proposal should offer the following information:\n\nA high-level statement of the problem you intend to address or the analysis you aim to generate\nThe data source(s) you intend to use\nYour plan to obtain that data\nThe text as data method you plan to use.\nA definition for what “success” means with respect to your project.\n\nIn your words, what would a successful project look like? What is the results you want to show? What would be a surprising finding for you?\n\n\n\n\nPresentation\nYou will have the opportunity to present you final project in class. As a Data scientists, your presentation skills are as important as your data skills. You should prepare a 10-12 minutes presentation. It should cover:\n\nMotivation or problem statement\nData collection\nMethods\nResults\nLessons learned and next steps\n\n\n\nProject Report\nThe report is a complete description of the project’s analysis and results. The report should be a 10 page in length (single-spaced; 12pt font) and cover the points presented below. Citations and Front page will not be considered in the 10 page.\nBelow I’ve outlined points that one should aim to discuss in each section. Note that paper should read as a cohesive report:\n\nIntroduction\n\nsummarize your motivation, present some previous work related to your question\npresent your research question\nsummarize your report\n\nData and Methods\n\nWhere does the data come from? – What is the unit of observation? – What are the variables of interest? – What steps did you take to wrangle the data?\n\nAnalysis\n\nDescribe the methods/tools you explored in your project.\n\nResults\n\nGive a detailed summary of your results. – Present your results clearly and concisely. – Please use visualizations instead of tables whenever possible.\n\nDiscussion\n\nRe-introduce your main results\nState your contributions\nWhere do you want take this project next?\n\n\nIMPORTANT: There is not literature review section. You can use the literature to motivate your work. But you don’t need to have a full section on literature review. Read for example, papers at main general interest journals, like here, here and here. All these super accomplished articles do not have long literature reviews."
  },
  {
    "objectID": "finalproject.html#submission-of-the-final-project",
    "href": "finalproject.html#submission-of-the-final-project",
    "title": "Instructions for PPOL 6801 - Final Project",
    "section": "Submission of the Final Project",
    "text": "Submission of the Final Project\nThe end product should be a github repository that contains:\n\nThe raw source data you used for the project. If the data is too large for GitHub, talk with me, and we will find a solution\nYour proposal\nA README for the repository that, for each file, describes in detail:\n\nInputs to the file: e.g., raw data; a file containing credentials needed to access an API\nWhat the file does: describe major transformations.\nOutput: if the file produces any outputs (e.g., a cleaned dataset; a figure or graph).\nA set of code files that transform that data into a form usable to answer the question you have posed in your descriptive research proposal.\nYour final 10 pages report (I will share a template later in the semester)\n\n\nOf course, no commits after the due date will be considered in the assessment."
  },
  {
    "objectID": "finalproject.html#templates-for-writing",
    "href": "finalproject.html#templates-for-writing",
    "title": "Instructions for PPOL 6801 - Final Project",
    "section": "Templates for writing",
    "text": "Templates for writing\nHere, I suggest you with a few templates for writing your reports. In my experience, writing data science reports using LaTeX is an gain in productivity in the long-run.\nIf you want to experiment with LaTeX via overleaf, you should use this template from PNAS (Just remember to switch to a one-column template):\n\nPNAS Template\n\nYou can also use the Journal of Quantitative Description templates in Word Doc or LaTeX:\n\nJQD Word\nJQD LaTeX\n\nOr, you can use templates from Quarto and write you entire project using quarto or markdown files"
  },
  {
    "objectID": "slides/week_02.html#outline",
    "href": "slides/week_02.html#outline",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Outline",
    "text": "Outline\n\n\nChallenges of working with text\nDefining a corpus and selecting documents\nUnit of analysis\nReducing complexity (Denny & Spirling’s article)\nBag-of-Word, Vector model representation and Document-Feature Matrix\nApplication (Ban et. al.’s paper)"
  },
  {
    "objectID": "slides/week_02.html#challenge-i-text-is-high-dimensional",
    "href": "slides/week_02.html#challenge-i-text-is-high-dimensional",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Challenge I: Text is High-Dimensional",
    "text": "Challenge I: Text is High-Dimensional\nFrom Gentzkow et al 2017:\n\nsample of documents, each \\(n_L\\) words long, drawn from vocabulary of \\(n_V\\) words.\nThe unique representation of each document has dimension \\(n_{V}^{n_L}\\) .\n\ne.g., a sample of 30-word (\\(n_L\\)) Twitter messages using only the one thousand most common words in the English language\nDimensionality = \\(1000^{30}\\)\nAs a matrix: \\(M^{1000}_{n_tweets}\\)"
  },
  {
    "objectID": "slides/week_02.html#challenge-ii-text-is-an-unstructure-data-source",
    "href": "slides/week_02.html#challenge-ii-text-is-an-unstructure-data-source",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Challenge II: Text is an unstructure data source",
    "text": "Challenge II: Text is an unstructure data source"
  },
  {
    "objectID": "slides/week_02.html#challenges-iii-outcomes-live-in-the-latent-space",
    "href": "slides/week_02.html#challenges-iii-outcomes-live-in-the-latent-space",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Challenges III: Outcomes live in the Latent Space",
    "text": "Challenges III: Outcomes live in the Latent Space\nIn most social science applications of text as data, we are trying to make an inference about a latent variable\n\nLatent variable: we cannot observe directly but try to identify with statistical and theoretical assumptions.\nExamples: ideology, sentiment, political stance, propensity of someone to turnout\n\n\nTraditional social science: mapping between observed and latent/theoretical concepts is easier.\n\nWe observe/measure country macroeconomic variables, collect survey responses, see how politicians vote.\nIn text, we only observe the words. Much harder to identify the latent concepts."
  },
  {
    "objectID": "slides/week_02.html#learning-goals",
    "href": "slides/week_02.html#learning-goals",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Learning goals",
    "text": "Learning goals\nToday:\n\nCover techniques to reduce complexity from text data using a set of pre-processing steps ~ Challenge I\nHow to represent text as numbers using the vector space model ~ Challenge II\nStarting next week we will deal more with inference and modeling latent parameters using text ~ Challenge III"
  },
  {
    "objectID": "slides/week_02.html#corpus-and-selecting-documents",
    "href": "slides/week_02.html#corpus-and-selecting-documents",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "1. Corpus and selecting documents",
    "text": "1. Corpus and selecting documents\n\nA corpus is (typically) a large set of texts or documents which we wish to analyze.\n\nif you can read them in an small amount of time, you should just do it, not TAD\n\nWhen selecting a corpus, we should consider how the corpus relates to our research question in two aspects:\n\nPopulation of interest: does the corpus allows us to make inferences about them?\nQuantity of interest: can we measure what we plan to?\nSampling Bias: documents are often sampled from a larger population. Are there concerns about sample selection bias?\n\nMost often we use these documents because they were available to us (custom made data). In these cases, considering the three questions above is even more important."
  },
  {
    "objectID": "slides/week_02.html#ventura-et.-al.-streaming-chats-2021",
    "href": "slides/week_02.html#ventura-et.-al.-streaming-chats-2021",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Ventura et. al., Streaming Chats, 2021",
    "text": "Ventura et. al., Streaming Chats, 2021\n\n\n\n\n\n\n\n\nKey components\n\n\nRQ: Measure quality of comments on streaming chat platforms during political debates\nPopulation of interest?\nQuantity of interest?\nSource of bias?"
  },
  {
    "objectID": "slides/week_02.html#unit-of-analysis",
    "href": "slides/week_02.html#unit-of-analysis",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "2. Unit of Analysis",
    "text": "2. Unit of Analysis\nAfter selecting your documents and converting them to a computer-friendly format, we must decide our unit of analysis\n\nentire document? sentence? paragraph? a larger group of documents?\n\n\nThree things to consider in making this decision:\n\nFeatures of your data and model fit\nYour research question\nIterative model\n\nswitching through different units of analysis has a low cost\nallows you to look at the data from a different angle\nprovide new insights to your research"
  },
  {
    "objectID": "slides/week_02.html#reducing-complexity",
    "href": "slides/week_02.html#reducing-complexity",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "3. Reducing complexity",
    "text": "3. Reducing complexity\nLanguage is extraordinarily complex, and involves great subtlety and nuanced interpretation.\n\nWe simplify documents so that we can analyze/compared them:\n\nmakes the modeling problem much more tractable.\ncomplexity makes not much difference in topic identification or simple prediction tasks (sentiment analysis, for example)\n\nthe degree to which one simplifees is dependent on the particular task at hand.\n\nDenny and Spirling (2019) ~ check sensitivity."
  },
  {
    "objectID": "slides/week_02.html#reducing-complexity-steps",
    "href": "slides/week_02.html#reducing-complexity-steps",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Reducing complexity: steps",
    "text": "Reducing complexity: steps\n\nTokenization: What does constitute a feature?\nRemove `superfulous’ material: HTML tags, punctuation, numbers, lower case and stop words\nMap words to equivalence forms: stemming and lemmatization\nDiscard less useful features for your task at hand: functional words, highly frequent or rare words\nDiscard word order: Bag-of-Words Assumption"
  },
  {
    "objectID": "slides/week_02.html#tokenization",
    "href": "slides/week_02.html#tokenization",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Tokenization",
    "text": "Tokenization\n\nA first step in any text analysis task is to break documents in meaningful units of analysis (tokens)\nTokens are often words for most tasks. A simple tokenizer uses white space marks to split documents in tokens.\nTokenizer may vary [across tasks]{.red}:\n\nTwiter specific tokenizer ~ keep hashtags, for example.\n\nMay also vary across languages, in which white space is not a good marker to split text into tokens\n\nchinese and japanese\n\nCertain tokens, even in english, make more sense together than separate (“White House”, “United States”). These are collocations\n\nstatistical testing for collocations ~ PMI(a, b) = log(p(a,b)/p(a)*p(b))"
  },
  {
    "objectID": "slides/week_02.html#stop-words",
    "href": "slides/week_02.html#stop-words",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Stop Words",
    "text": "Stop Words\n\nThere are certain words that serve as linguistic connectors (`function words’) which we can remove.\n\n( the, it, if, a, for, from, at, on, in, be )\n\nAdd noise to the document. Discard them, focus on signal, meaningful words.\nMost TAD packages have a pre-selected list of stopwords. You can add more given you substantive knowledge (more about this later)\nUsually not important for unsupervised and mostly supervised tasks, but might matter for authorship detection.\n\nFederalist Papers, example. Stop words give away writing styles."
  },
  {
    "objectID": "slides/week_02.html#equivalence-mapping",
    "href": "slides/week_02.html#equivalence-mapping",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Equivalence mapping",
    "text": "Equivalence mapping\nReduce dimensionality even further!\n\nDifferent forms of words (family, families, familial), or words which are similar in concept (bureaucratic, bureaucrat, bureaucratization) that refer to same basic token/concept.\nuse algorithms to map these variation to a equivalent form:\n\nstemming: chop the end of the words: family, families, familiar ~ famili\nlemmatization: condition on part of speech\n\nbetter (adj) ~ good\nleaves (noun) ~ leaf\nleaves (verb) ~ leave\n\n\nAll [TAD/NLP packages[{.red}] offer easy applications for these algorithms."
  },
  {
    "objectID": "slides/week_02.html#other-steps-functional-words-highly-frequent-or-rare-words",
    "href": "slides/week_02.html#other-steps-functional-words-highly-frequent-or-rare-words",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Other steps: functional words, highly frequent or rare words",
    "text": "Other steps: functional words, highly frequent or rare words\nSome other commons steps, which are highly dependent on your contextual knowledge, are:\n\ndiscard functional words: for example, when working with congressional speeches, remove representative, congress, session, etc...\nremove highly frequent words: words that appear in all documents carry very little meaning for most supervised and unsupervised tasks ~ no clustering and not discrimination.\nremove rare frequent words: same logic as above, no signal. Commong practice, words appear less 5% fo documents."
  },
  {
    "objectID": "slides/week_02.html#bag-of-words-assumption",
    "href": "slides/week_02.html#bag-of-words-assumption",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "4. Bag-of-Words Assumption",
    "text": "4. Bag-of-Words Assumption\nNow we have pre-processed our data. So we simplify it even further:\n\nBag-of-Words Assumption: the order in which words appear does not matter.\n\nIgnore order\nBut keep multiplicity, we still consider frequency of words\n\n\n\nHow could this possible work:\n\nit might note: you need validation\ncentral tendency in text: some words are enough to topic detection, classificaiton, measures of similarity, and distance, for example.\nhumans in the loop: expertise knowledge help you figure it out subtle relationships between words and outcomes"
  },
  {
    "objectID": "slides/week_02.html#can-we-preserve-the-word-order-another-pre-processing-decision",
    "href": "slides/week_02.html#can-we-preserve-the-word-order-another-pre-processing-decision",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Can we preserve the word order? (another pre-processing decision)",
    "text": "Can we preserve the word order? (another pre-processing decision)\nYes\n\nwe might retaining word order using n-grams.\n\nWhite House, Bill Gates, State Department, Middle East\nwe think some important subtlety of expression is lost: negation perhaps\n\nI want coffee, not tea might be interpreted very diferently without word order.\n\n\ncan use [n-grams], which are (sometimes contiguous) sequences of two (bigrams) or three (trigrams) tokens.\nThis makes computations considerably more complex. We can pick some n-grams to keep but not all:\n\n\\(PMI_{a,b} = log \\frac{p_{a,b}}{p_a \\cdot p_b}\\)\n\nif p(a,b)=0 ~ log (0) = -inf\nif p(a,b)=p(a)p(b) ~ log(1) = 0\nif p(a,b)&lt;p(a)p(b) ~ log(0&lt;x&lt;1) &lt; 0\nif p(a,b)&gt;p(a)p(b) ~ log(x&gt;1) &gt; 0"
  },
  {
    "objectID": "slides/week_02.html#complete-example",
    "href": "slides/week_02.html#complete-example",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Complete Example",
    "text": "Complete Example\n\n\n\nText\n\n\nWe use a new dataset containing nearly 50 million historical newspaper pages from 2,700 local US newspapers over the years 1877–1977. We define and discuss a measure of power we develop based on observed word frequencies, and we validate it through a series of analyses. Overall, we find that the relative coverage of political actors and of political offices is a strong indicator of political power for the cases we study\n\n\n\n\n\n\n\nAfter pre-processing\n\n\nuse new dataset contain near 50 million historical newspaper pag 2700 local u newspaper year 18771977 define discus measure power develop bas observ word frequenc validate ser analys overall find relat coverage political actor political offic strong indicator political power cas study"
  },
  {
    "objectID": "slides/week_02.html#denny-spirling-2018",
    "href": "slides/week_02.html#denny-spirling-2018",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Denny & Spirling, 2018",
    "text": "Denny & Spirling, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nStarting point: No rigorous way to compare results across different pre-processing steps. Adapting recommendations from supervised learning tasks.\n\nUnsupervised vs Supervised Learning?\nWhat is their solution? (no math needed!)\n\n\n\ncalculate the distance for every pair of documents, and rank the distances\ncompared to no pre processing, do the pair wise distances again, and get which document pair changed kth position, where k=1 for the pair that changed the most.\nBuild vector Vm_i^ with the position of the pairwise distance k affected in every other m combination. So Vm_1_1 contains the position of the changes in parwise distance on every other combination other than m=1 for the most changed document in m_1.\ntheir words: vmk = the rank difference for pair k between specification i and all others.\nAnother example with 3 documents, Vm1_1 = (1m2, 1m3), indicates that the document the changed the most in m1 is also the same in m2 and m3.\npretext score: mean over k (mean of V_m_k)\n\n\n\nToo much work? Substantive knowledge out of the table?"
  },
  {
    "objectID": "slides/week_02.html#vector-space-model",
    "href": "slides/week_02.html#vector-space-model",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "5. Vector Space Model",
    "text": "5. Vector Space Model\nTo represent documents as numbers, we will use the vector space model representation:\n\nA document \\(D_i\\) is represented as a collection of features \\(W\\) (words, tokens, n-grams..)\nEach feature \\(w_i\\) can be place in a real line, then a document \\(D_i\\) is a point in a \\(W\\) dimensional space\n\n\nImagine the sentence below: “If that is a joke, I love it. If not, can’t wait to unpack that with you later.”\n\nSorted Vocabulary =(a, can’t, i, if, is, it, joke, later, love, not, that, to, unpack, wait, with, you”)\nFeature Representation = (1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1)\nFeatures will typically be the n-gram (mostly unigram) frequencies of the tokens in the document, or some function of those frequencies\n\n\n\nNow each document is now a vector (vector space model)\n\nstacking these vectors will give you our workhose representation for text: Document Feature Matrix"
  },
  {
    "objectID": "slides/week_02.html#visualizing-vector-space-model",
    "href": "slides/week_02.html#visualizing-vector-space-model",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Visualizing Vector Space Model",
    "text": "Visualizing Vector Space Model\n\n\n\nDocuments\n\n\nDocument 1 = “yes yes yes no no no”\nDocument 2 = “yes yes yes yes yes yes”"
  },
  {
    "objectID": "slides/week_02.html#visualizing-vector-space-model-1",
    "href": "slides/week_02.html#visualizing-vector-space-model-1",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Visualizing Vector Space Model",
    "text": "Visualizing Vector Space Model\nIn the vector space, we can use geometry to build well-defined comparison measures between the documents (more about this next week)"
  },
  {
    "objectID": "slides/week_02.html#document-feature-matrix",
    "href": "slides/week_02.html#document-feature-matrix",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "6. Document-Feature Matrix",
    "text": "6. Document-Feature Matrix\n\n\n\n\n\n\n\n\n\n\n\n\nSource: Arthur Spirling TAD Class"
  },
  {
    "objectID": "slides/week_02.html#ban-et.-al.-2019-how-newspapers-reveal-political-power.",
    "href": "slides/week_02.html#ban-et.-al.-2019-how-newspapers-reveal-political-power.",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Ban et. al. 2019, How Newspapers Reveal Political Power.",
    "text": "Ban et. al. 2019, How Newspapers Reveal Political Power.\n\n\n\n\n\n\n\n\n \n\nPurely descriptive\nSimple measure just by counting words.\nTheorethically-driven: measure that capture a theorethically relevant concept.\n\n\n\\[\n\\small \\text{Coverage of Mayor}_{it} = \\frac{\\text{Mayor}_{it}}{\\text{Mayor}_{it} + \\text{City Manager}_{it} + \\text{City Council}_{it}}\n\\]\n\n\n\n\n\nText-as-Data"
  },
  {
    "objectID": "slides/week_07.html#survey-responses",
    "href": "slides/week_07.html#survey-responses",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Survey Responses",
    "text": "Survey Responses\nThank you so much for responding to the survey! Here are the most important feedback I receive:\n\nmore structured walk throughs of the code\nIf you could a few resources we can look at for mathematical depth for class topic. Really enjoyed the in-depth discussion on LDA\nI hope we can have a few discussion questions along with weekly readings\nIncluding discussion of papers that are from non-polisci applications? For example, econ, sociology, psychology, etc.?\n\nStop Doing:\n\nlong alone time with code\nSometimes we spend a bit too long on the recap part"
  },
  {
    "objectID": "slides/week_07.html#plans-for-today",
    "href": "slides/week_07.html#plans-for-today",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Plans for Today:",
    "text": "Plans for Today:\n\nLive coding from last class on topic models\nWord Embeddings\n\nSemantics, Distributional Hypothesis, From Sparse to Dense Vectors\nWord2Vec Algorithm\n\nMathematical Model\nEstimate with Neural Networks\n\n\nNext week:\n\nStart with coding to work with wordembeddings\n\nEstimate from co-occurence matrices\nWorking with pre-trained models\n\nDiscuss applications to social science"
  },
  {
    "objectID": "slides/week_07.html#vector-space-model",
    "href": "slides/week_07.html#vector-space-model",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Vector Space Model",
    "text": "Vector Space Model\nIn the vector space model, we learned:\n\nA document \\(D_i\\) is represented as a collection of features \\(W\\) (words, tokens, n-grams..)\nEach feature \\(w_i\\) can be place in a real line, then a document \\(D_i\\) is a point in a \\(W\\) dimensional space.\n\nEmbedded in this model, there is the idea we represent words as a one-hot encoding.\n\n“cat”: [0,0, 0, 0, 0, 0, 1, 0, ….., V] , on a V dimensional vector\n“dog”: [0,0, 0, 0, 0, 0, 0, 1, …., V], on a V dimensional vector\n\nWhat these vectors look like?\n\nreally sparse\nvectors are orthogonal\nno natural notion of similarity"
  },
  {
    "objectID": "slides/week_07.html#distributional-semantics",
    "href": "slides/week_07.html#distributional-semantics",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Distributional Semantics",
    "text": "Distributional Semantics\n\n“you shall know a word by the company it keeps.” J. R. Firth 1957\n\nDistributional semantics: words that are used in the same contexts tend to be similar in their meaning.\n\n\nHow can we use this insight to build a word representation?\n\nMove from sparse representation to dense representation\nRepresent words as vectors of numbers with high number of dimensions\nEach feature on this vectors embeds some information from the word (gender? noun? sentiment? stance?)\nLearn this representation from the unlabeled data."
  },
  {
    "objectID": "slides/week_07.html#sparse-vs-dense-vectors",
    "href": "slides/week_07.html#sparse-vs-dense-vectors",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Sparse vs Dense Vectors",
    "text": "Sparse vs Dense Vectors\nOne-hot encoding / Sparse Representation:\n\ncat = \\(\\begin{bmatrix} 0,0, 0, 0, 0, 0, 1, 0, 0 \\end{bmatrix}\\)\ndog = \\(\\begin{bmatrix} 0,0, 0, 0, 0, 1, 0, 0, 0 \\end{bmatrix}\\)\n\nWord Embedding / Dense Representation:\n\ncat = \\(\\begin{bmatrix} 0.25, -0.75, 0.90, 0.12, -0.50, 0.33, 0.66, -0.88, 0.10, -0.45 \\end{bmatrix}\\)\ndog = \\(\\begin{bmatrix} 0.25, 1.75, 0.90, 0.12, -0.50, 0.33, 0.66, -0.88, 0.10, -0.45 \\end{bmatrix}\\)"
  },
  {
    "objectID": "slides/week_07.html#with-colors-and-real-word-vectors",
    "href": "slides/week_07.html#with-colors-and-real-word-vectors",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "With colors and real word vectors",
    "text": "With colors and real word vectors\n\nSource: Illustrated Word2Vec"
  },
  {
    "objectID": "slides/week_07.html#why-word-embeddings",
    "href": "slides/week_07.html#why-word-embeddings",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Why Word Embeddings?",
    "text": "Why Word Embeddings?\nEncoding similarity: vectors are not ortogonal anymore!\nAutomatic Generalization: learn about one word allow us to automatically learn about related words\nEncodes Meaning: by learning the context, I can learn what a word means.\nAs a consequence:\n\nWord Embeddings improves several NLP/Text-as-Data Tasks.\nAllows to deal with unseen words.\nForm the core idea of state-of-the-art models, such as LLMs."
  },
  {
    "objectID": "slides/week_07.html#estimating-word-embeddings",
    "href": "slides/week_07.html#estimating-word-embeddings",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Estimating Word Embeddings",
    "text": "Estimating Word Embeddings\nApproches:\n\n\nCount-based methods: look at how often words co-occur with neighbors.\n\nuse this matrix, and some some factorization to retrieve vectors for the words\nGloVE\nfast, not computationally intensive, but not the best representation\nwe will see code doing this next week\n\n\n\n\n\nPredictive Methods: rely on the idea of self-supervision\n\nuse unlabeled data and use words to predict sequence\nthe famous word2vec.\n\nSkipgram: predicts context words\nContinuous Bag of Words: predict center word"
  },
  {
    "objectID": "slides/week_07.html#word2vec-a-framework-for-learning-word-vectors-mikolov-et-al.-2013",
    "href": "slides/week_07.html#word2vec-a-framework-for-learning-word-vectors-mikolov-et-al.-2013",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Word2Vec: a framework for learning word vectors (Mikolov et al. 2013)",
    "text": "Word2Vec: a framework for learning word vectors (Mikolov et al. 2013)\nCore Idea:\n\n\nWe have a large corpus (“body”) of text: a long list of words\nEvery word in a fixed vocabulary is represented by a vector\nGo through each position t in the text, which has a center word \\(c\\) and context (“outside”) words \\(t\\)\nUse the similarity of the word vectors for \\(c\\) and \\(t\\) to calculate the probability of o given c (or vice versa)\nKeep adjusting the word vectors to maximize this probability\n\nNeural Network + Gradient Descent"
  },
  {
    "objectID": "slides/week_07.html#skigram-example-self-supervision",
    "href": "slides/week_07.html#skigram-example-self-supervision",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Skigram Example: Self-Supervision",
    "text": "Skigram Example: Self-Supervision\n\nSource: CS224N"
  },
  {
    "objectID": "slides/week_07.html#skigram-example-self-supervision-1",
    "href": "slides/week_07.html#skigram-example-self-supervision-1",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Skigram Example: Self-Supervision",
    "text": "Skigram Example: Self-Supervision\n\nSource: CS224N"
  },
  {
    "objectID": "slides/week_07.html#encoding-similarity",
    "href": "slides/week_07.html#encoding-similarity",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Encoding Similarity",
    "text": "Encoding Similarity\nTo estimate the model, we first need to formalize the probability function we want to estimate.\n\n\nThis is similar to a logistic regression\n\n\n\nIn logistic regression: probability of a event occur given data X and parameters \\(\\beta\\).:\n\n$ P(y=1| X, ) = X * \\(\\beta\\) $\n\\(X*\\beta\\) is not a proper probability function, so we make it to proper probability by using a logit transformation.\n\\(P(y=1|X, \\beta ) = \\frac{exp(XB)}{1 + exp(XB)}\\)\nThrow this transformation inside of a bernouilli distribution, get the likelihood function, and find the parameters using MLE."
  },
  {
    "objectID": "slides/week_07.html#pametrizing-pw_tw_t-1",
    "href": "slides/week_07.html#pametrizing-pw_tw_t-1",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Pametrizing \\(P(w_t|w_{t-1})\\)",
    "text": "Pametrizing \\(P(w_t|w_{t-1})\\)\n\n\\(P(w_t|w_{t-1})\\) must be condition on how similar these words are.\n\nExactly the same intuition behind placing documents in the vector space model.\nNow words are vectors!\n\\(P(w_t|w_{t-1}) = u_c \\cdot u_t\\)\n\n\\(u_c \\cdot u_t\\)\ndot product between vectors\nmeasures similarity using vector projection\n\\(u_c\\): center vector\n\\(u_t\\): target vectors\n\n\n\\(u_c \\cdot u_t\\) is also not a proper probability distribution: Logit on them!\n\n\\[P(w_t|w_{t-1}) = \\frac{exp(u_c \\cdot u_t)}{{\\sum_{w}^V exp(u_c*u_w)}}\\]"
  },
  {
    "objectID": "slides/week_07.html#softmax-transformation",
    "href": "slides/week_07.html#softmax-transformation",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Softmax Transformation",
    "text": "Softmax Transformation\n\\[P(w_t|w_{t-1}) = \\frac{exp(u_c \\cdot u_t)}{{\\sum_{w}^V exp(u_c*u_w)}}\\]\n\nDot product compares similarity between vectors\nnumerator: center vs target vectors\nexponentiation makes everything positive\nDenominator: normalize over entire vocabulary to give probability distribution\nWhat is the meaning of softmax?\n\nmax: assign high values to be 1\nsoft: still assigns some probability to smaller values\ngeneralization of the logit ~ multinomial logistic function."
  },
  {
    "objectID": "slides/week_07.html#word2vec-objective-function",
    "href": "slides/week_07.html#word2vec-objective-function",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Word2Vec: Objective Function",
    "text": "Word2Vec: Objective Function\n\nFor each position \\(t\\), predict context words within a window of fixed size \\(m\\), given center word \\(w\\).\nLikelihood Function\n\\[ L(\\theta) = \\prod_{t=1}^{T} \\prod_{\\substack{-m&lt;= j&lt;=m \\\\ j \\neq 0}}^{m} P(w_{t+j} | w_t; \\theta) \\]\n\nAssuming independence, this means you multiplying the probability of every target for every center word in your dictionary.\nThis likelihood function will change if you do skipgram with negative sampling (See SLP chapter 6)\n\n\n\n\nObjective Function: Negative log likelihood\n\\[J(\\theta) = - \\frac{1}{T}log(L(\\theta))\\]\n\nbetter to take the gradient with sums\nthe average increases the numerical stability of the gradient."
  },
  {
    "objectID": "slides/week_07.html#neural-networks-brief-overview",
    "href": "slides/week_07.html#neural-networks-brief-overview",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Neural Networks: Brief overview",
    "text": "Neural Networks: Brief overview"
  },
  {
    "objectID": "slides/week_07.html#skipgram-architecture",
    "href": "slides/week_07.html#skipgram-architecture",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Skipgram Architecture",
    "text": "Skipgram Architecture"
  },
  {
    "objectID": "slides/week_07.html#check-your-matrices",
    "href": "slides/week_07.html#check-your-matrices",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Check your matrices",
    "text": "Check your matrices\n\n\n\n\n\n\n\n\n  \nPractice with a vocabulary of size 5, a embedding with 3 dimensions, and the task is to predict the next word.\n\nStep 1: v_1^5 * W_5^3\nStep 2: w_1^3 * C_3^5\nStep 3: Softmax entire vector"
  },
  {
    "objectID": "slides/week_07.html#word-embeddings-matrices",
    "href": "slides/week_07.html#word-embeddings-matrices",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Word Embeddings Matrices",
    "text": "Word Embeddings Matrices"
  },
  {
    "objectID": "slides/week_07.html#applications",
    "href": "slides/week_07.html#applications",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Applications:",
    "text": "Applications:\nOnce we’ve optimized, we can extract the word specific vectors from W as embedding vectors. These real valued vectors can be used for analogies and related tasks\n\nWe will see several applications next week. Most important:\n\nAlternative to bag-of-words feature representation in supervised learning tasks\nSupport for other automated text analysis tasks: expand dictionaries\nUnderstanding word meaning: variation over time, bias, variation by groups\nas a scaling method (in two weeks)"
  },
  {
    "objectID": "slides/week_07.html#training-embeddings",
    "href": "slides/week_07.html#training-embeddings",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Training Embeddings",
    "text": "Training Embeddings\nEmbeddings need quite a lot of text to train: e.g. want to disambiguate meanings from contexts. You can download pre-trained, or get the code and train locally\n\nWord2Vec is trained on the Google News dataset (∼ 100B words, 2013)\nGloVe are trained on different things: Wikipedia (2014) + Gigaword (6B words), Common Crawl, Twitter. And uses a co-occurence matrix instead of Neural Networks\nfastext from facebook"
  },
  {
    "objectID": "slides/week_07.html#decisions-on-embeddings-rodriguez-and-spirling-2022",
    "href": "slides/week_07.html#decisions-on-embeddings-rodriguez-and-spirling-2022",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Decisions on embeddings, Rodriguez and Spirling, 2022",
    "text": "Decisions on embeddings, Rodriguez and Spirling, 2022\nWhen using/training embeddings, we face four key decisions:\n\nWindow size\nNumber of dimensions for the embedding matrix\nPre-trained versus locally fit variants\nWhich algorithm to use?"
  },
  {
    "objectID": "slides/week_07.html#findings",
    "href": "slides/week_07.html#findings",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Findings",
    "text": "Findings\n\n\npopular, easily available pretrained embeddings perform at a level close to—or surpassing—both human coders andmore complicated locally fit models.\nGloVe pretrained word embeddings achieve on average—for the set of political queries—80% of human performance and are generally preferred to locally trained embeddings\nLarger window size and embeddings are often preferred."
  },
  {
    "objectID": "slides/week_10.html#plans-for-today",
    "href": "slides/week_10.html#plans-for-today",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "",
    "text": "Overview of scaling models\nScaling models using text\n\nsupervised models: wordscore\nunsupervised models: wordfish\n\nBut remember we saw the Doc2vec last week\n\n\nScaling models with network data"
  },
  {
    "objectID": "slides/week_10.html#many-questions-depends-on-policy-positions",
    "href": "slides/week_10.html#many-questions-depends-on-policy-positions",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Many questions depends on policy positions",
    "text": "Many questions depends on policy positions\nMany substantive questions in policy and politics depends on estimating ideological preferences:\n\nIs polarization increasing over time?\n\nare parties moving together over time?\n\nDo governments/coalitions last longer conditional on:\n\nhow homogeneous coalition is?\ndistance between coalition and congress ideal points?\ndistance between coalition and median voter?\n\nDoes ideology affect policy decisions?\n\nEconomic cycles and elections?\nGlobalization and the social welfare state?\n\nDoes motivated reasoning affects beliefs/sharing of online misinformation?\n\nMany more examples …."
  },
  {
    "objectID": "slides/week_10.html#scaling-with-without-computational-text-analysis",
    "href": "slides/week_10.html#scaling-with-without-computational-text-analysis",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Scaling with without computational text analysis",
    "text": "Scaling with without computational text analysis\nSurveys\n\nAsk elites or regular voters about policy preference, and scale them in some sort of political continuum.\nChallenges:\n\nExpensive\nSources of bias: non-response, social desirebility, strategic responses, to name a few\nCannot be used for historical data"
  },
  {
    "objectID": "slides/week_10.html#scaling-with-without-computational-text-analysis-1",
    "href": "slides/week_10.html#scaling-with-without-computational-text-analysis-1",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Scaling with without computational text analysis",
    "text": "Scaling with without computational text analysis\nBehavioral data (for example, roll-call voting):\n\nPoliticians vote on proposals (or judges make decisions) that are close to their ideal points\n\nUse statistical methods (mostly matrix factorization) to estimate orthogonal dimensions\nNominate Scores\nChallenge\n\nMost times politicians vote for many things other than ideology\nFace-Validity: AOC for example often place as a centrist (not always voting with democrats)"
  },
  {
    "objectID": "slides/week_10.html#scaling-with-without-computational-text-analysis-2",
    "href": "slides/week_10.html#scaling-with-without-computational-text-analysis-2",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Scaling with without computational text analysis",
    "text": "Scaling with without computational text analysis\nText Data\n\nManually label content of political manifestos (Comparative Manifestos Project)\nChallenges:\n\nReally expensive\nCostly to translate to other languages\nHeavily dependent on substantive knowledge"
  },
  {
    "objectID": "slides/week_10.html#scaling-models-with-computational-text-analysis",
    "href": "slides/week_10.html#scaling-models-with-computational-text-analysis",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Scaling models with computational text analysis",
    "text": "Scaling models with computational text analysis\nIn the past 20 years, computational text analysis has been widely used for building scaling models.\n\nAdvantages: fast, reliable, deals with large volumes of text, and easy to translate to other domains/language.\nWordscore: supervised approach, mimic naive bayesian models, start with reference, and score virgin texts.\nWordfish: unsupervised approach, learn word occurrence from the documents using a ideal-points model."
  },
  {
    "objectID": "slides/week_10.html#wordscore-laver-benoit-garry-2003-apsr",
    "href": "slides/week_10.html#wordscore-laver-benoit-garry-2003-apsr",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Wordscore (Laver, Benoit, Garry, 2003, APSR)",
    "text": "Wordscore (Laver, Benoit, Garry, 2003, APSR)\n\nStep 1: Begin with a reference set (training set) of texts that have known positions.\n\nGet a speech by AOC , give it -1 score.\nGet a speech by MTG, give it a +1 score.\n\n\n\nStep 2: Generate word scores from these reference texts\n\nPretty much like a Naive Bayes\n\n\n\nStep 3: Score the virgin texts (test set) of texts using those word scores\n\nscale virgin score to the same original metric (-1 to +1)\n\ncalculate confidence intervals"
  },
  {
    "objectID": "slides/week_10.html#scoring-words",
    "href": "slides/week_10.html#scoring-words",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Scoring Words",
    "text": "Scoring Words\n\n\nLearning Words\n\\[ P_{wr} = \\frac{F_{wr}}{\\sum_r F_{wr}}\\] - \\(P_{wr}\\): Probability of word \\(w\\) being from reference document \\(r\\)\n\n\\(F_{wr}\\): Frequency of word \\(w\\) in reference text \\(r\\)\n\n\n\n\nScoring Words\n\\[S_{wd} = \\sum_r (P_{wr} \\cdot A_{rd}) \\] - \\(S_{wd}\\): Score of word \\(w\\) in dimension \\(d\\)\n\n\\(A_{rd}\\): Pre-defined position of reference text \\(r\\) in dimension \\(d\\)\n\\(A_{rd}\\) will be -1 for liberal and +1 for conservative documents, for example.\n\n\n\n\nScoring virgin texts\n\\[ S_{vd} = \\sum_w (F_{wv} · S_{wd}) \\]"
  },
  {
    "objectID": "slides/week_10.html#example",
    "href": "slides/week_10.html#example",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Example",
    "text": "Example\nRepublican manifesto uses `wall’ 25 times in 1000 words, while Democrat use it only 5 times. Assume 1 for republican and -1 for democrat\n\\[P_{wr} = 0.83\\]\n\\[P_{wl} =  0.16 \\] \\[ S_w = 0.83*1 + -1*.16 = 0.66\\]\n\nVirgin text 1 mentions wall 200 times in 1000 words ~ \\(0.2 * 0.66 = 0.132\\)\nVirgin text 2 mentions wall 1 times in 1000 words ~ \\(0.001 * 0:66 = 0.0066\\)\n\n\nRepeat for all words!"
  },
  {
    "objectID": "slides/week_10.html#results",
    "href": "slides/week_10.html#results",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "slides/week_10.html#wordfish-slapin-and-proksch-2008-ajps",
    "href": "slides/week_10.html#wordfish-slapin-and-proksch-2008-ajps",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Wordfish, (Slapin and Proksch, 2008, AJPS)",
    "text": "Wordfish, (Slapin and Proksch, 2008, AJPS)\nSame assumptions as in Wordscore (word usage provides information about policy placement), but in an unsupervised setting and with fully specified language model.\n\n\nThe count of word \\(j\\) from party \\(i\\) , in year \\(t\\):\n\\[ y_{ijt}   \\sim  Pois(\\lambda)\\]\n\\[ \\lambda_{ijt}=\\exp({\\alpha_{it} + \\psi_j + \\beta_j\\times\\omega_{it}}) \\]\n\n\\(\\alpha_{it}\\): fixed effect(s) for party \\(i\\) in time \\(t\\) ~ deal with parties that write too much\n\\(\\psi_j\\): word fixed effect: some parties just use certain words more (e.g. their own name)\n\\(\\beta_j\\): word specific weights (discrimination parameter, adds more information than simple usage)\n\\(\\omega_{it}\\) estimate of party’s position in a given year."
  },
  {
    "objectID": "slides/week_10.html#estimation",
    "href": "slides/week_10.html#estimation",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Estimation",
    "text": "Estimation\n\\[ \\lambda_{ijt}=\\exp({\\alpha_{it} + \\psi_j + \\beta_j\\times\\omega_{it}}) \\]\nNothing on RHS is known: everything needs to be estimated.\n\nUse Expectation Maximization (EM) algorithm (same as we could use for topic models):\n\nInitialize values based on counts in the data\nE-Step: run a Poisson regression holding word parameters fixed, and estimating the party parameters\nM-Step: run a Poisson regression holding party parameters fixed, and estimating the word parameter\nIterate until convergence\n\nOther option: Bayesian Estimation with Gibbs sampling."
  },
  {
    "objectID": "slides/week_10.html#results-i",
    "href": "slides/week_10.html#results-i",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Results I",
    "text": "Results I"
  },
  {
    "objectID": "slides/week_10.html#results-ii",
    "href": "slides/week_10.html#results-ii",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Results II",
    "text": "Results II"
  },
  {
    "objectID": "slides/week_10.html#spatial-following-model-barbera-2015",
    "href": "slides/week_10.html#spatial-following-model-barbera-2015",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Spatial Following Model, Barbera 2015",
    "text": "Spatial Following Model, Barbera 2015\nWith the emergence of social media data, not only more text data has become available, but also network data. Some of the most interesting developments in computational social science research in the past years has been on using this data to estimate ideological positions for a LARGE number of social media users.\n\nAssumption: users prefer to follow political accounts they perceive to be ideologically close to their own position.\nFollowing decisions contain information about allocation of scarce resource: attention\nSelective exposure: preference for information that reinforces current views\nStatistical model that builds on assumption to estimate positions of both individuals and political accounts"
  },
  {
    "objectID": "slides/week_10.html#all-you-need-is-a-matrix",
    "href": "slides/week_10.html#all-you-need-is-a-matrix",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "All you need is a matrix!",
    "text": "All you need is a matrix!"
  },
  {
    "objectID": "slides/week_10.html#statistical-model",
    "href": "slides/week_10.html#statistical-model",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Statistical Model",
    "text": "Statistical Model\n\nUsers’ and political accounts’ ideology (\\(theta_i\\) and \\(\\phi_j\\) ) are defined as latent variables to be estimated.\nData: “following” decisions, a matrix of binary choices (Y).\nProbability that user i follows political account j is\n\n\\[P(y_{ij} = 1) = logit^{-1} (\\alpha_j + \\beta_i - \\zeta(\\theta_i - \\phi_j)^2)\\]\nwith latent variables:\n\n\\(\\theta_i\\) measures ideology of user i\n\\(\\phi_j\\) measures ideology of political account j\n\\(\\alpha_j\\) measures popularity of political account j\n\\(\\beta_i\\) measures political interest of user i"
  },
  {
    "objectID": "slides/week_10.html#ventura-et-al-news-sharing-model",
    "href": "slides/week_10.html#ventura-et-al-news-sharing-model",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Ventura et al’ News Sharing Model",
    "text": "Ventura et al’ News Sharing Model"
  },
  {
    "objectID": "slides/week_10.html#statistical-model-1",
    "href": "slides/week_10.html#statistical-model-1",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Statistical Model",
    "text": "Statistical Model\nThe estimation of the utility function for sharing uses a multilevel overdispersed poisson model.\n\\[y_{ij} \\sim \\mbox{Po}(\\mu_i)\\]\n\\[\\mu_i = \\exp(\\alpha_{i[q]}\\left(x_{i}-L_{j}\\right)^2 + A_{[q]} + R_{[j]} + \\gamma_{[i]})\\]\nWhere:\n\n\\(y_{i}\\) = Number of links embedded by user x media\n\\(\\alpha_{q}\\) = Random Slope by quantile\n\\(A_{q}\\) = Random intercept by quantile\n\\(R_{j}\\) = Random intercept by media outlet\n\\(\\gamma_{[i]}\\) = Overdispersion parameter"
  },
  {
    "objectID": "slides/week_05.html#housekeeping",
    "href": "slides/week_05.html#housekeeping",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Housekeeping",
    "text": "Housekeeping\nToday is your deadline for the problem set 1.\n\n\nReplications next week\n\nPresentation (20 min each):\n\nIntroduction;\nMethods;\nResults;\nDifferences;\nAutopsy of the replication;\nExtensions\n\nRepository (by friday):\n\nGithub Repo\n\nreadme\nyour presentation pdf\ncode\n5pgs report"
  },
  {
    "objectID": "slides/week_05.html#where-are-we",
    "href": "slides/week_05.html#where-are-we",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Where are we?",
    "text": "Where are we?\nWe started from pre-processing text as data, representing text as numbers, and describing features of the text.\nLast week, we started learning how to measure concepts in text:\n\n\nDocuments pertaining to certain classes and how we can use statistical assumptions to measure these classes\n\n\n\n\nDictionary Methods\n\nDiscuss some well-known dictionaries\n\nOff-the-Shelf Classifiers\n\nPerspective API\nHugging Face (only see as a off-the-shelf machines, LMMs later in this course)"
  },
  {
    "objectID": "slides/week_05.html#remember",
    "href": "slides/week_05.html#remember",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Remember…",
    "text": "Remember…\n\n\nUnsupervised Models: learning (hidden or latent) structure in unlabeled data.\n\nTopic Models to cluster documents and words\n\n\n\n\n\nSupervised Models: learning relationship between inputs and a labeled set of outputs.\n\nSentiment Analysis, classify if a tweet contains misinformation, etc..\n\n\n\n\n\nIn TAD, we mostly use unsupervised techniques for discovery and supervised for measurement of concepts."
  },
  {
    "objectID": "slides/week_05.html#assuming",
    "href": "slides/week_05.html#assuming",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Assuming:",
    "text": "Assuming:"
  },
  {
    "objectID": "slides/week_05.html#supervised-learning-pipeline-for-tad",
    "href": "slides/week_05.html#supervised-learning-pipeline-for-tad",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Supervised Learning Pipeline for TAD",
    "text": "Supervised Learning Pipeline for TAD\n\n\nStep 1: label some examples of the concept of we want to measure\n\nsome tweets are positive, some are neutral and some are negative\n\nStep 2: train a statistical model on these set of label data using the document-feature matrix as input\n\nchoose a model (transformation function) that gives higher out-of-sample accuracy\n\nStep 3: use the classifier - some f(x) - to predict unseen documents.\n\npick the best out-sample perfirmance\n\nStep 4: use the measure + metadata|exogenous shocks to learn something new about the world.\n\nThis is where social science happens!"
  },
  {
    "objectID": "slides/week_05.html#supervised-learning-vs-dictionaries",
    "href": "slides/week_05.html#supervised-learning-vs-dictionaries",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Supervised Learning vs Dictionaries",
    "text": "Supervised Learning vs Dictionaries\nDictionary methods:\n\nAdvantage: not corpus-specific, cost to apply to a new corpus is trivial\nDisadvantage: not corpus-specific, so performance on a new corpus is unknown (domain shift)\n\nSupervised learning:\n\nGeneralization of dictionary methods\nFeatures associated with each categories (and their relative weight) are learned from the data\nBy construction, ML will outperform dictionary methods in classification tasks, as long as training sample is large enough"
  },
  {
    "objectID": "slides/week_05.html#supervised-learning-pipeline-for-tad-1",
    "href": "slides/week_05.html#supervised-learning-pipeline-for-tad-1",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Supervised Learning Pipeline for TAD",
    "text": "Supervised Learning Pipeline for TAD\n\nStep 1: label some examples of the concept of we want to measure\nStep 2 train a statistical model on these set of label data using the document-feature matrix as input\nStep 3: use the classifier - some f(x) - to predict unseen documents.\nStep 4: use the measure + metadata|exogenous shocks to learn something new about the world."
  },
  {
    "objectID": "slides/week_05.html#how-to-obtain-a-labeled-dataset",
    "href": "slides/week_05.html#how-to-obtain-a-labeled-dataset",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "How to obtain a labeled dataset?",
    "text": "How to obtain a labeled dataset?\n\n\nExternal Source of Annotation: someone else labelled the data for you\n\nFederalist papers\nMetadata from text\nManifestos from Parties with well-developed dictionaries\n\nExpert annotation: put experts in quotation\n\nmostly undergrads ~ that you train to be experts\n\nCrowd-sourced coding: digital labor markets\n\nWisdom of Crowsds: the idea that large groups of non-expert people are collectively smarter than individual experts when it comes to problem-solving"
  },
  {
    "objectID": "slides/week_05.html#crowdsourcing-as-a-research-tool-for-ml",
    "href": "slides/week_05.html#crowdsourcing-as-a-research-tool-for-ml",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Crowdsourcing as a research tool for ML",
    "text": "Crowdsourcing as a research tool for ML\n  \n\nCrowdsourcing is now understood to mean using the Internet to distribute a large package of small tasks to a large number of anonymous workers, located around the world and offered small financial rewards per task. The method is widely used for data-processing tasks such as image classification, video annotation, data entry, optical character recognition, translation, recommendation, and proofreading\n\n\n\nSource: Benoit et al, 2016"
  },
  {
    "objectID": "slides/week_05.html#benoit-et-al-206-crowdsourcing-political-texts",
    "href": "slides/week_05.html#benoit-et-al-206-crowdsourcing-political-texts",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Benoit et al, 206: Crowdsourcing Political Texts",
    "text": "Benoit et al, 206: Crowdsourcing Political Texts\n\n\nExpert annotation is expensive.\nBenoit, Conway, Lauderdale, Laver and Mikhaylov (2016) note that classification jobs could be given to a large number of relatively cheap online workers\nMultiple workers ~ similar task ~ same stimuli ~ wisdom of crowds!\nRepresentativeness of a broader population doesn’t matter ~ not a populational quantity, it is just a measurement task\nTheir task: Manifestos ~ sentences ~ workers:\n\nsocial|economic\n\nvery-left vs very right\n\n\nReduce uncertainty by having more workers for each sentence"
  },
  {
    "objectID": "slides/week_05.html#comparing-experts-and-online-workers",
    "href": "slides/week_05.html#comparing-experts-and-online-workers",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Comparing Experts and online workers",
    "text": "Comparing Experts and online workers"
  },
  {
    "objectID": "slides/week_05.html#how-many-workers",
    "href": "slides/week_05.html#how-many-workers",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "How many workers?",
    "text": "How many workers?"
  },
  {
    "objectID": "slides/week_05.html#section-1",
    "href": "slides/week_05.html#section-1",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "",
    "text": "Source: Pablo Barbera’s CSS Seminar"
  },
  {
    "objectID": "slides/week_05.html#supervised-learning-pipeline-for-tad-2",
    "href": "slides/week_05.html#supervised-learning-pipeline-for-tad-2",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Supervised Learning Pipeline for TAD",
    "text": "Supervised Learning Pipeline for TAD\n\nStep 1: label some examples of the concept of we want to measure\nStep 2 train a statistical model on these set of label data using the document-feature matrix as input\nStep 3: use the classifier - some f(x) - to predict unseen documents.\nStep 4: use the measure + metadata|exogenous shocks to learn something new about the world."
  },
  {
    "objectID": "slides/week_05.html#general-thoughts",
    "href": "slides/week_05.html#general-thoughts",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "General Thoughts",
    "text": "General Thoughts\nOnce we have our training data, we need to pick a classifier. We face these challenges:\n\n\nin text as data, often your DFM has Features &gt; Documents\n\nidentification problems for statistical models\noverfitting the data\n\n\n\n\n\nBias-Variance Trade-off\n\nfit a overly complicated model ~ leads to higher variance\nfit a more flexible model ~ leads to more bias\n\n\n\n\n\nMany models:\n\nNaive Bayes\nRegularized regression\nSVM\nk-nearest neighbors, tree-based methods, etc.\nEnsemble methods + DL"
  },
  {
    "objectID": "slides/week_05.html#bias-and-variance-tradeoff",
    "href": "slides/week_05.html#bias-and-variance-tradeoff",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Bias and Variance Tradeoff",
    "text": "Bias and Variance Tradeoff"
  },
  {
    "objectID": "slides/week_05.html#train-validation-test-or-cross-validation",
    "href": "slides/week_05.html#train-validation-test-or-cross-validation",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Train-Validation-Test OR Cross Validation",
    "text": "Train-Validation-Test OR Cross Validation"
  },
  {
    "objectID": "slides/week_05.html#many-models",
    "href": "slides/week_05.html#many-models",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Many Models",
    "text": "Many Models"
  },
  {
    "objectID": "slides/week_05.html#but-not-so-different",
    "href": "slides/week_05.html#but-not-so-different",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "But not so different…",
    "text": "But not so different…"
  },
  {
    "objectID": "slides/week_05.html#regularized-ols-regression",
    "href": "slides/week_05.html#regularized-ols-regression",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Regularized OLS Regression",
    "text": "Regularized OLS Regression\nThe simplest, but highly effective, way to avoid overfit and improve out-sample accuracy is to add penalty parameters for statistical models:\n\nOLS Loss Function :\n\\[\nRSS = \\sum_{i=1}^{N} \\left( y_i - \\beta_0 - \\sum_{j=1}^{J} \\beta_j x_{ij} \\right)^2\n\\]\n\n\nOLS + Penalty:\n\\[\n\\text{RSS} = \\sum_{i=1}^{N} \\left( y_i - \\beta_0 - \\sum_{j=1}^{J} \\beta_j x_{ij} \\right)^2 + \\lambda \\sum_{j=1}^{J} \\beta_j^2 \\rightarrow \\text{ridge regression}\n\\]\n\\[\n\\text{RSS} = \\sum_{i=1}^{N} \\left( y_i - \\beta_0 - \\sum_{j=1}^{J} \\beta_j x_{ij} \\right)^2 + \\lambda \\sum_{j=1}^{J} |\\beta_j| \\rightarrow \\text{lasso regression}\n\\]"
  },
  {
    "objectID": "slides/week_05.html#supervised-learning-pipeline-for-tad-3",
    "href": "slides/week_05.html#supervised-learning-pipeline-for-tad-3",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Supervised Learning Pipeline for TAD",
    "text": "Supervised Learning Pipeline for TAD\n\nStep 1: label some examples of the concept of we want to measure\nStep 2 train a statistical model on these set of label data using the document-feature matrix as input\nStep 3: use the classifier - some f(x) - to predict unseen documents.\nStep 4: use the measure + metadata|exogenous shocks to learn something new about the world."
  },
  {
    "objectID": "slides/week_05.html#evaluating-the-performance",
    "href": "slides/week_05.html#evaluating-the-performance",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Evaluating the Performance",
    "text": "Evaluating the Performance\n\n\n\n\n\nPredicted\n\n\n\n\n\n\n\n\nJ\n¬J\nTotal\n\n\nActual\nJ\na (TP)\nb (FN)\na+b\n\n\n\n¬J\nc (FP)\nd (TN)\nc+d\n\n\n\nTotal\na+c\nb+d\nN\n\n\n\n\n\nAccuracy: number correctly classified/total number of cases = (a+d)/(a+b+c+d)\nPrecision : number of TP / (number of TP+number of FP) = a/(a+c) .\n\nFraction of the documents predicted to be J, that were in fact J.\nThink as a measure for the estimator\n\nRecall: (number of TP) / (number of TP + number of FN) = a /(a+b)\n\nFraction of the documents that were in fact J, that method predicted were J.\nThink as a measure for the data\n\nF : 2 precision*recall / precision+recall\n\nHarmonic mean of precision and recall."
  },
  {
    "objectID": "slides/week_05.html#barbera-et-al-2020-guide-for-supervised-models-with-text",
    "href": "slides/week_05.html#barbera-et-al-2020-guide-for-supervised-models-with-text",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Barbera et al, 2020, Guide for Supervised Models with Text",
    "text": "Barbera et al, 2020, Guide for Supervised Models with Text\n\n\n\n\n\nvia GIPHY\n\n\n Task: Tone of New York Times coverage of the economy. Discusses:\n\nHow to build a corpus\nUnit of analysis\nDocuments or Coders?\nML or Dictionaries?"
  },
  {
    "objectID": "slides/week_09.html#plans-for-today",
    "href": "slides/week_09.html#plans-for-today",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "",
    "text": "A quick quizz (as a review exercise) about Word Embeddings\nText as Data applications using Word Embeddings\nCoding\nProblem set 2!"
  },
  {
    "objectID": "slides/week_09.html#many-questions-depends-on-policy-positions",
    "href": "slides/week_09.html#many-questions-depends-on-policy-positions",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Many questions depends on policy positions",
    "text": "Many questions depends on policy positions\nMany substantive questions in policy and politics depends on estimating ideological preferences:\n\nIs polarization increasing over time?\n\nare parties moving together over time?\n\nDo governments/coalitions last longer conditional on:\n\nhow homogeneous coalition is?\ndistance between coalition and congress ideal points?\ndistance between coalition and median voter?\n\nDoes ideology affect policy decisions?\n\nEconomic Cycles and elections?\nglobalization and the social welfare state?\n\n\nMany more examples …."
  },
  {
    "objectID": "slides/week_09.html#scaling-with-without-computational-text-analysis",
    "href": "slides/week_09.html#scaling-with-without-computational-text-analysis",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Scaling with without computational text analysis",
    "text": "Scaling with without computational text analysis\n\nSurveys\n\nAsk elites or regular voters about policy preference, and scale them in some sort of political continuum.\nChallenges:\n\nExpensive\nSources of bias: non-response, social desirebility, strategic responses, to name a few\nCannot be used for historical data"
  },
  {
    "objectID": "slides/week_09.html#scaling-with-without-computational-text-analysis-1",
    "href": "slides/week_09.html#scaling-with-without-computational-text-analysis-1",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Scaling with without computational text analysis",
    "text": "Scaling with without computational text analysis\n\nBehavioral data (for example, roll-call voting):\n\nPoliticians vote on proposals (or judges make decisions) that are close to their ideal points\n\nUse statistical methods (mostly matrix factorization) to estimate orthogonal dimensions\nNominate Scores\nChallenge\n\nMost times politicians vote for many things other than ideology\nFace-Validity: AOC for example often place as a centrist (not always voting with democrats)"
  },
  {
    "objectID": "slides/week_09.html#scaling-with-without-computational-text-analysis-2",
    "href": "slides/week_09.html#scaling-with-without-computational-text-analysis-2",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Scaling with without computational text analysis",
    "text": "Scaling with without computational text analysis\n\nText Data\n\nManually label content of political manifestos (Comparative Manifestos Project)\nChallenges:\n\nReally expensive\nCostly to translate to other languages\nHeavily dependent on substantive knowledge"
  },
  {
    "objectID": "slides/week_09.html#scaling-models-with-computational-text-analysis",
    "href": "slides/week_09.html#scaling-models-with-computational-text-analysis",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Scaling models with computational text analysis",
    "text": "Scaling models with computational text analysis\nIn the past 20 years, computational text analysis has been widely used for building scaling models.\n\nAdvantages: fast, reliable, deals with large volumes of text, and easy to translate to other domains/language.\nWordscore: supervised approach, mimic naive bayesian models, start with reference, and score virgin texts.\nWordfish: unsupervised approach, learn word occurrence from the documents using a ideal points models.\nDoc2Vec: unsupervised approach, maps documents in the embedding vector space, use PCA to reduce dimensionality."
  },
  {
    "objectID": "slides/week_09.html#wordscore",
    "href": "slides/week_09.html#wordscore",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Wordscore",
    "text": "Wordscore\nStep 1: Begin with a reference set (training set) of texts that have known positions.\n\nget a speech by AOC , give it -1 score.\nget a speech by MTG, give it a +1 score.\n\nStep 2: Generate word scores from these reference texts\n\nPretty much like a NB\n\nStep 3: Score the virgin texts (test set) of texts using those word scores\n\nscale virgin score to the same original metric (-1 to +1)\n\ncalculate confidence intervals"
  },
  {
    "objectID": "slides/week_09.html#scoring-words",
    "href": "slides/week_09.html#scoring-words",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Scoring Words",
    "text": "Scoring Words\n\n\nLearning Words\n\\[ P_{wr} = \\frac{F_{wr}}{\\sum_r F_{wr}}\\] - \\(P_{wr}\\): Probability of word \\(w\\) being from reference document \\(r\\)\n\n\\(F_{wr}\\): Frequency of word \\(w\\) in reference text \\(r\\)\n\n\n\n\nScoring Words\n\\[S_{wd} = \\sum_r (P_{wr} \\cdot A_{rd}) \\] - \\(S_{wd}\\): Score of word \\(w\\) in dimension \\(d\\)\n\n\\(A_{rd}\\): Pre-defined position of reference text \\(r\\) in dimension \\(d\\)\n\\(A_{rd}\\) will be -1 for liberal and +1 for conservative documents, for example.\n\n\n\n\nScoring virgin texts\n\\[ S_{vd} = \\sum_w (F_{wv} · S_{wd}) \\]"
  },
  {
    "objectID": "slides/week_09.html#example",
    "href": "slides/week_09.html#example",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Example",
    "text": "Example\nRepublican manifesto uses `wall’ 25 times in 1000 words, while Democrat use it only 5 times. Assume 1 for republican and -1 for democrat\n\\[P_{wr} = 0.83\\]\n\\[P_{wl} =  0.16 \\] \\[ S_w = 0.83*1 + -1*.16 = 0.66\\]\n\nVirgin text 1 mentions wall 200 times in 1000 words ~ \\(0.2 * 0.66 = 0.132\\)\nVirgin text 2 mentions wall 1 times in 1000 words ~ \\(0.001 * 0:66 = 0.0066\\)\n\nRepeat for all words!"
  },
  {
    "objectID": "slides/week_09.html#results",
    "href": "slides/week_09.html#results",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus: PPOL 6801 - Text as Data: Computational Linguistics",
    "section": "",
    "text": "Download a PDF Version here"
  },
  {
    "objectID": "syllabus.html#course-description",
    "href": "syllabus.html#course-description",
    "title": "Syllabus: PPOL 6801 - Text as Data: Computational Linguistics",
    "section": "Course Description",
    "text": "Course Description\nThis course introduces students to the quantitative analysis of text as data. With the increasing availability of large-scale textual data – from government documents and political speeches to social media and online news – the potential to extract meaningful insights from text has expanded greatly. In this course, students will learn to transform text into data and apply it to social science questions and theories. Key topics include text representation, sentiment analysis, scaling text on ideological and policy dimensions, using machine learning for text classification, word embeddings and using Large Language Models (GPT, Bert, Llama, Gemini, etc..) for social science research. Although we will cover LLMs, the focus will be on applied methods and transfer learning rather than an in-depth coverage of building LLMs from scratch.\nThe course includes hands-on exercises using real-world data to reinforce lecture content. By the end, students will have a toolkit for text analysis useful in roles as policy experts and computational social scientists. Students should have completed at least an introductory statistics course and have a basic understanding of probability, distributions, hypothesis testing, and linear models. Students are also expected to have experience working with R, the programming language and software environment of this course, and Python for the LLM’s components of the course."
  },
  {
    "objectID": "syllabus.html#instructor",
    "href": "syllabus.html#instructor",
    "title": "Syllabus: PPOL 6801 - Text as Data: Computational Linguistics",
    "section": "Instructor",
    "text": "Instructor\nProfessor Tiago Ventura\n\nPronouns: He/Him\nEmail: tv186@georgetown.edu\nOffice hours: Every Tuesday, 4pm - 5pm\nLocation: McCourt Building, 766"
  },
  {
    "objectID": "syllabus.html#our-classes",
    "href": "syllabus.html#our-classes",
    "title": "Syllabus: PPOL 6801 - Text as Data: Computational Linguistics",
    "section": "Our classes",
    "text": "Our classes\nClasses will take place at the scheduled class time/place and will involve a combination of lectures, coding walkthrough, breakout group sessions, and questions. We will start our classes with a lecture highlighting what I consider to be the broader substantive and programming concepts covered in the class. From that, we will switch to a mix of coding walk through and breakout group sessions.\nThis class follows a more classic PhD style seminar. This means that the class is heavy on the readings, and I expect you to do the readings before class. For most classes, you will read one or more chapters of the text book and between two or three applied articles. Most of the lectures will cover topics discussed on the readings.\nNote that this class is scheduled to meet weekly for 2.5 hours. I will do my best to make our meetings dynamic and enjoyable for all parts involved. We will take one or two breaks in each of our lecture."
  },
  {
    "objectID": "syllabus.html#required-materials",
    "href": "syllabus.html#required-materials",
    "title": "Syllabus: PPOL 6801 - Text as Data: Computational Linguistics",
    "section": "Required Materials",
    "text": "Required Materials\nReadings: We will rely primarily on the following textbooks for this course. While this textbook is not freely available online, all the other materials of the course will be or should be accessible through Georgetown library.\n\nGrimmer, Justin, Margaret E. Roberts, and Brandon M. Stewart. Text as data: A new framework for machine learning and the social sciences. Princeton University Press, 2022 - [GMB]\nDaniel Jurafsky and James H. Martin. Speech and Language Processing, 3nd Edition. - [SLP]\n\nThe weekly articles are listed in the syllabus"
  },
  {
    "objectID": "syllabus.html#course-infrastructure",
    "href": "syllabus.html#course-infrastructure",
    "title": "Syllabus: PPOL 6801 - Text as Data: Computational Linguistics",
    "section": "Course Infrastructure",
    "text": "Course Infrastructure\nClass Website: A class website will be used throughout the course and should be checked on a regular basis for lecture materials and required readings.\nClass Slack Channel: The class also has a dedicated slack channel. The channel serves as an open forum to discuss, collaborate, pose problems/questions, and offer solutions. Students are encouraged to pose any questions they have there as this will provide the professor and TA the means of answering the question so that all can see the response. If you’re unfamiliar with, please consult the following start-up tutorial (https://get.slack.help/hc/en-us/articles/218080037-Getting-started-for-new-members).\nCanvas: A Canvas site (http://canvas.georgetown.edu) will be used throughout the course and should be checked on a regular basis for announcements and assignments. All announcements for the assignments and classes will be posted on Canvas; they will not be distributed in class or by e-mail. Support for Canvas is available at (202) 687-4949"
  },
  {
    "objectID": "syllabus.html#weekly-schedule-readings",
    "href": "syllabus.html#weekly-schedule-readings",
    "title": "Syllabus: PPOL 6801 - Text as Data: Computational Linguistics",
    "section": "Weekly Schedule & Readings",
    "text": "Weekly Schedule & Readings\n\nWeek 1: Introduction - Overview of the course\nTopics: Review of syllabus and class organization. Introduction to computational text analysis.\n\n[GMB] - Chapter 2.\n\nOr one of the three below:\n\nPolitical Science Perspective: Wilkerson, J. and Casas, A. (2017). Large-Scale Computerized Text Analysis in Political Science: Opportunities and Challenges. Annual Review of Political Science, 20, 529:544.\nEconomics Perspective: Gentzkow, Matthew, Bryan Kelly, and Matt Taddy. “Text as data.” Journal of Economic Literature 57, no. 3 (2019): 535-574.\nSociology Perspective: https://www.sciencedirect.com/science/article/pii/S0049089X22000904\n\n\n\nWeek 2: From Text to Matrices: Representing Text as Data\nTopics: How to represent text as data? What is a Bag of Words? What are tokens? Why should we care about tokens?\n-[GMB] - Chapters 3-5\n\nApplied Papers:\n\nDenny, M. J., & Spirling, A. (2018). Text preprocessing for unsupervised learning: why it matters, when it misleads, and what to do about it. Political Analysis, 26(2): 168-189.\nBan, Pamela, Alexander Fouirnaies, Andrew B. Hall, and James M. Snyder. “How newspapers reveal political power.” Political Science Research and Methods 7, no. 4 (2019): 661-678.\nMichel, J.B., et al. 2011. “Quantitative Analysis of Culture Using Millions of Digitized Books.” Science 331 (6014): 176–82. https://doi.org/10.1126/science.1199644;\n\n\n\n\nWeek 3: Text Similarity, Text re-use and Complexity\nTopics: How do we evaluate complexity in text? Why should we care about complexity in text? How do we evaluate similarity in text? Why is this useful?\n\n[GMB] - Chapter 7\nApplied Papers:\n\nSpirling, Arthur. 2016. “Democratization and Linguistic Complexity”, Journal of Politics.\nBenoit, K., Munger, K. and Spirling, A. 2017. Measuring and Explaining Political Sophistication Through Textual Complexity\nLinder, Fridolin, Bruce Desmarais, Matthew Burgess, and Eugenia Giraudy. “Text as policy: Measuring policy similarity through bill text reuse.” Policy Studies Journal 48, no. 2 (2020): 546-574.\n\n\n\n\nWeek 4: Supervised Learning I: Dictionary Methods and Out of Box Classifier Analysis\nTopics: What are dictionaries? Why/when are they useful? What are their limitations? Can we use models trained by others?\n\n[GMB] - Chapters 15-16\nApplied Papers:\n\nLori Young and Stuart Soroka 2012 “Affective News: The Automated Coding of Sentiment in Political Texts.” Political Communication, 29:2, 205-231.\nRathje, Steve, Jay J. Van Bavel, and Sander Van Der Linden. “Out-group animosity drives engagement on social media.” Proceedings of the National Academy of Sciences 118, no. 26 (2021): e2024292118.\nVentura, Tiago, Kevin Munger, Katherine McCabe, and Keng-Chi Chang. “Connective effervescence and streaming chat during political debates.” Journal of Quantitative Description: Digital Media 1 (2021).\n\nProblem set 1 Assigned\n\n\n\nWeek 5: Supervised Learning II: Training your own classifiers\nTopics: We will study the framework to train our own supervised models, and when to use them.\n\n[GMB] - Chapters 17, 18, 19, and 20.\nApplied Papers:\n\nBarberá, Pablo, Amber E. Boydstun, Suzanna Linn, Ryan McMahon, and Jonathan Nagler. “Automated text classification of news articles: A practical guide.” Political Analysis 29, no. 1 (2021): 19-42.\nSiegel, Alexandra, et al. “Trumping Hate on Twitter? Online Hate Speech in the 2016 US Election Campaign and its Aftermath.”\nTheocharis, Y., Barberá, P., Fazekas, Z., & Popa, S. A. (2020). The dynamics of political incivility on Twitter. Sage Open, 10(2), 2158244020919447.\nMitts, T., Phillips, G., & Walter, B. (2021). Studying the Impact of ISIS Propaganda Campaigns. Journal of Politics\n\n\n\n\nWeek 6: Unsupervised Learning: Topic Models\nTopics: what if we do not have an outcome to predict? can we cluster the text in groups? what are topics?\n\n[GMB] - Chapters 12-14.\nDavid M. Blei . 2012. “Probabilistic Topic Models.” http://www.cs.columbia.edu/~blei/papers/ Blei2012.pdf\nApplied Papers:\n\nMotolinia, Lucia. Electoral accountability and particularistic legislation: evidence from an electoral reform in Mexico. American Political Science Review 115, no. 1 (2021): 97-113.\nBarberá, P., Casas, A., Nagler, J., Egan, P. J., Bonneau, R., Jost, J. T., & Tucker, J. A. (2019). Who leads? Who follows? Measuring issue attention and agenda setting by legislators and the mass public using social media data. American Political Science Review, 113(4), 883-901.\nEshima, Shusei, Kosuke Imai, and Tomoya Sasaki. “Keyword‐Assisted Topic Models.” American Journal of Political Science (2020).\n\n\n\n\nWeek 7: Using Text to Measure Ideology - Scaling\nTopics: What are scaling models and what can they tell us? Can we represent politicians/users ideology using text?\n\nApplied Papers:\n\nLaver, Michael, Kenneth Benoit, and John Garry. 2003. “Extracting Policy Positions from Political Texts Using Words as Data”. American Political Science Review. 97, 2, 311-331\nSlapin, Jonathan and Sven-Oliver Prokschk. 2008. “A Scaling Model for Estimating Time-Series Party Positions from Texts.” American Journal of Political Science. 52, 3 705-722\nRheault, Ludovic, and Christopher Cochrane. “Word embeddings for the analysis of ideological placement in parliamentary corpora.” Political Analysis 28, no. 1 (2020): 112-133.\nAruguete, Natalia, Ernesto Calvo, and Tiago Ventura. “News by popular demand: Ideological congruence, issue salience, and media reputation in news sharing.” The International Journal of Press/Politics 28, no. 3 (2023): 558-579.\nIzumi, Mauricio Y., and Danilo B. Medeiros. “Government and opposition in legislative speechmaking: using text-as-data to estimate Brazilian political parties’ policy positions.” Latin American Politics and Society 63, no. 1 (2021): 145-164.\n\n\n\n\nWeek 8: Representation Learning & Introduction to Deep Learning\nTopics: How can we capture the meaning of words? Using Deep Learning model to represent text.\n\nSLP: Chapter 7\n\n-   [Lin, Gechun, and Christopher Lucas. \"An Introduction to Neural Networks for the Social Sciences.\" (2023)](https://christopherlucas.org/files/PDFs/nn_chapter.pdf)\n\n\nWeek 9: Word Embeddings: What they are and how to estimate?\nTopics: What are word-embeddings? When and how can we use them? What? Topic models again? Is this still a bag of words?\n\n[GMB] - Chapter 8.\n[SLP] Chapter 6, “Vector Semantics and Embeddings.”\nJay Alanmar, The Illustrated Word2vec\nSpirling and Rodriguez, Word Embeddings: What works, what doesn’t, and how to tell the difference for applied research.\n\n\n\nWeek 10: Word Embeddings: Social Science Applications\n\nApplied Papers:\n\nRodman, E., 2020. A Timely Intervention: Tracking the Changing Meanings of Political Concepts with Word Vectors. Political Analysis, 28(1), pp.87-111.\nGennaro, Gloria, and Elliott Ash. “Emotion and reason in political language.” The Economic Journal 132, no. 643 (2022): 1037-1059.\nRheault, Ludovic, and Christopher Cochrane. “Word embeddings for the analysis of ideological placement in parliamentary corpora.” Political Analysis 28, no. 1 (2020): 112-133.\nAustin C. Kozlowski, Austin C., Matt Taddy, and James A. Evans. 2019. “The Geometry of Culture: Analyzing the Meanings of Class through Word Embeddings.” American Sociological Review 84, no. 5: 905–49. https://doi.org/10.1177/0003122419877135.\nGarg, Nikhil, Londa Schiebinger, Dan Jurafsky and James Zou. 2018. “Word embeddings quantify 100 years of gender and ethnic stereotypes.” Proceedings of the National Academy of Sciences 115(16):E3635–E3644.\n\nProblem set 3 Assigned\n\n\n\nWeek 11: Replication Class: Students’ Presentation\n\n\nWeek 12: Transformers\nTopics: We will learn about the Transformers architecture, attention, and the encoder-coder infrastructure.\n\n[SLP] - Chapter 10.\nJay Alammar. 2018. “The Illustrated Transformer.” https://jalammar.github.io/illustratedtransformer/\nVaswani, A., et al. (2017). Attention is all you need. Advances in neural information processing systems, 30;\nTimoneda and Vera, BERT, RoBERTa or DeBERTa? Comparing Performance Across Transformer Models in Political Science Text, The Journal of Politics 87, no. 1 (2025): 000-000.\n\n\n\nWeek 13: Large Language Models: Prompting, building chatbots, and social science applications. (Invited Speaker: Dr. Patrick Wu, Professor Computer Science American Politics)\n\nApplied Papers: We will see some social science application of LLMs chatbots. Can we use ChatGPT to perform zero-shot classification tasks? What are the concerns of these applications?\n\nWu, Patrick Y., Joshua A. Tucker, Jonathan Nagler, and Solomon Messing. “Large language models can be used to estimate the ideologies of politicians in a zero-shot learning setting.” arXiv preprint arXiv:2303.12057 (2023).\nRathje, Steve, Dan-Mircea Mirea, Ilia Sucholutsky, Raja Marjieh, Claire Robertson, and Jay J. Van Bavel. GPT is an effective tool for multilingual psychological text analysis. (2023).\nDavidson, Thomas: Start Generating: Harnessing Generative Artificial Intelligence for Sociological Research, PrePrint\nBisbee, James, Joshua Clinton, Cassy Dorff, Brenton Kenkel, and Jennifer Larson. 2023. “Synthetic Replacements for Human Survey Data? the Perils of Large Language Models.” SocArXiv. May 4.\n\nArgyle, L.P., Busby, E.C., Fulda, N., Gubler, J.R., Rytting, C. and Wingate, D., 2023. Out of one, many: Using language models to simulate human samples. Political Analysis, 31(3), pp.337-351.\n\nTimoneda, Joan C., and Sebastián Vallejo Vera. “Memory Is All You Need: Testing How Model Memory Affects LLM Performance in Annotation Tasks.” arXiv preprint arXiv:2503.04874 (2025).\n\n\n\n\nWeek 14: Final Projects: Students Presentation"
  },
  {
    "objectID": "syllabus.html#course-requirements",
    "href": "syllabus.html#course-requirements",
    "title": "Syllabus: PPOL 6801 - Text as Data: Computational Linguistics",
    "section": "Course Requirements",
    "text": "Course Requirements\n\n\n\nAssignment\nPercentage of Grade\n\n\n\n\nParticipation/Attendance\n10%\n\n\nProblem Sets\n20%\n\n\nReplication Exercises\n30%\n\n\nFinal Project\n40%\n\n\n\nParticipation and Attendance (10%):\nData science is an cooperative endeavor, and it’s crucial to view your fellow classmates as valuable assets rather than rivals. Your performance in the following aspects will be considered when assessing this part of your grade:\n\nActive involvement during class sessions, fostering a dynamic learning environment.\nContributions made to your group’s ultimate project.\nAssisting classmates by addressing problem set queries through GitHub issues. Supporting your peers will enhance your evaluation in terms of teamwork and engagement\nAssisting classmates with slack questions, sharing interesting materials on slack, asking question, and anything that provides healthy contributtions to the course.\n\nProblem Sets (30%):\nYou will have a week to complete your assignments\nStudents will be assigned three problem sets over the course of the semesters. While you are encouraged to discuss the problem sets with your peers and/or consult online resources, the finished product must be your own work. The goal of the assignment is to reinforce the student’s comprehension of the materials covered in each section.\nThe problems sets will assess your ability to apply the concepts to data that is substantially messier, and problems that are substantially more difficult, than the ones in the coding discussion in class.\nI will distribute the assignment through a mix of canvas and github. The assignments can be in the form of a Jupyter Notebook (.ipynb) or Quarto (.qmd). Students must submit completed assignments as a rendered .html file and the corresponding source code (.ipynb or .qmd).\nThe assignments will be graded in accuracy and quality of the programming style. For instance, our grading team will be looking at:\n\n\nall code must run;\n\n\nsolutions should be readable\n\n\nCode should be thoroughly commented (the Professor/TA should be able to understand the codes purpose by reading the comment),\nCoding solutions should be broken up into individual code chunks in Jupyter/R Markdown notebooks, not clumped together into one large code chunk (See examples in class or reach out to the TA/Professor if this is unclear),\nEach student defined function must contain a doc string explaining what the function does, each input argument, and what the function returns;\n\n\nCommentary, responses, and/or solutions should all be written in Markdown and explain sufficiently the outpus.\n\n\nAll solutions must be completed in Python.\n\n\nThe follow schedule lays out when each assignment will be assigned.\n\n\n\nAssignment\nDate Assigned\nDate Due\n\n\n\n\nNo. 1\nWeek 4\nBefore EOD of Week 5’s class\n\n\nNo. 2\nWeek 7\nBefore EOD of Week 8’s class\n\n\nNo. 3\nWeek 10\nBefore EOD of Week 11’s class\n\n\n\nReplication Exercises (30%)\nReplication exercises are adapted from Gary King’s work here and here. Replication consists in the process of repeating a research study using the original data or brining new data to the conversation. Replicability is crucial for the advancement of knowledge and the credibility of scientific inquiry.\nFor our purposes, replication exercises will work as a educational tool. A common say in science is that you just learn a new skill/methods when you use in your own work. Since we do not have time to write three papers during a semester, we will take advantage of published work with open available datasets and code for us to work on replication exercises.\nYou will work in pairs for this assignment. Your partner will be randomly assigned. This is a stylized step-by-step of this exercise:\n\nStep 1: finding a paper to replicate\n\nBy the end of the week 3, you should select an article from the syllabus to be replicated by you.\n\nStep 2: Acquiring the Data\n\nMost research articles are published with open data rules. This means their data and code are often available on github, or on harvard dataverse. Your first task is to find the data and code from these articles.\nIf your articles does not have the data and code, you should:\n\nPolitely contact the authors of the article and ask for the replication materials\nIf you don’t get response, select another article.\n\n\nStep 3: Presentation\n\nFor weeks 11, you will do a presentation of your replication efforts.\nThe presentation should have the following sections:\n\nIntroduction: introduction summarizing the article.\nMethods: data used in the article\nResults: the results you were able to replicate\nDifferences: any differences between your results and the authors’\nAutopsy of the replication: what worked and what did not work\nExtension: what would you do different if you were to write this article today? Where would you innovate?\n\n\nStep 4: Replication Repository\n\nBy Friday EOD of each replication week, you should share with me and all your colleagues your replication report. The replication report should be:\n\na github repo with a well-detailed readme. See an model here: https://github.com/TiagoVentura/winning_plosone\nyour presentation as a pdf\nthe code used in the replication as a notebook (Markdown or Jupyter)\na report with maximum of 5 pages (it is fine if you do less than that) summarizing the replication process, with emphasis on three sections of your presentation: Differences, Autopsy and Extension.\n\n\n\nIn addition to following the requirements above, the replication exercises will also be graded in accuracy and quality of the programming style. For instance, our grading team will be looking at:\n\n\nall code must run;\n\n\nsolutions should be readable\n\n\nCode should be thoroughly commented (the Professor/TA should be able to understand the codes purpose by reading the comment),\nCoding solutions should be broken up into individual code chunks in Jupyter/R Markdown notebooks, not clumped together into one large code chunk (See examples in class or reach out to the TA/Professor if this is unclear),\nEach student defined function must contain a doc string explaining what the function does, each input argument, and what the function returns;\n\n\nCommentary, responses, and/or solutions should all be written in Markdown and explain sufficiently the outpus.\n\n\n\nFinal Project (40%): Data science is an applied field and the DSPP is particularly geared towards providing students the tools to make policy and substantive contributtions using data and recent computational developments. In this sense, it is fundamental that you understand how to conduct a complete analysis from collecting data, to cleaning and analyzing it, to presenting your findings. For this reason, a considerable part of your grade will come from a an independent data science project, applying concepts learned throughout the course.\nThe project is composed of three parts:\n\na 2 page project proposal: (which should be discussed and approved by me)\nan in-class presentation,\nA 10-page project report.\n\nDue dates and breakdowns for the project are as follows:\n\n\n\n\n\n\n\n\n\nRequirement\nDue\nLength\nPercentage\n\n\n\n\nProject Proposal\nEOD Friday Week 9\n2 pages\n5%\n\n\nPresentation\nWeek 14\n10-15 minutes\n10%\n\n\nProject Report\nWednesday Week 15\n10 pages\n25%\n\n\n\nImportant notes about the final project\n\nFor the project proposal, you need to schedule a 30min with me at least a week before the due date. For this meeting, I expect you to send me a draft of your ideas.\nFor the presentation, You will have 10-15 minutes in our last class of the semester to present you project.\nTake the final project seriously. After you finish your Masters, in any path you take, you will need to show concrete examples of your portfolio. This is a good opportunity to start building it.\nYour groups will be randomly assigned.\n\nSubmission of the Final Project\nThe end product should be a github repository that contains:\n\nThe raw source data you used for the project. If the data is too large for GitHub, talk with me, and we will find a solution\nYour proposal\nA README for the repository that, for each file, describes in detail:\n\nInputs to the file: e.g., raw data; a file containing credentials needed to access an API\nWhat the file does: describe major transformations.\nOutput: if the file produces any outputs (e.g., a cleaned dataset; a figure or graph).\nA set of code files that transform that data into a form usable to answer the question you have posed in your descriptive research proposal.\nYour final 10 pages report (I will share a template later in the semester)\n\n\nOf course, no commits after the due date will be considered in the assessment.\n\nGrading\nCourse grades will be determined according to the following scale:\n\n\n\nLetter\nRange\n\n\n\n\nA\n95% – 100%\n\n\nA-\n91% – 94%\n\n\nB+\n87% – 90%\n\n\nB\n84% – 86%\n\n\nB-\n80% – 83%\n\n\nC\n70% – 79%\n\n\nF\n&lt; 70%\n\n\n\nGrades may be curved if there are no students receiving A’s on the non-curved grading scale.\nLate problem sets will be penalized a letter grade per day.\n\n\nCommunication\n\nClass-relevant and/or coding-related questions, Slack is the preferred method of communication. Please use the general or the relevant channel for these questions.\nFor private questions concerning the class, email is the preferred method of communication. All email messages must originate from your Georgetown University email account(s). Please email the professor directly rather than through the Canvas messaging system.\nI will try my best to respond to all emails/slack questions within 24 hours of being sent during a weekday. I will not respond to emails/slack sent late Friday (after 5:00 pm) or during the weekend until Monday (9:00 am). Please plan accordingly if you have questions regarding current or upcoming assignments.\nOnly reach out to the professor or teaching assistant regarding a technical question, error, or issue after you made a good faith effort to debugging/isolate your problem prior to reaching out. Learning how to search for help online is a important skill for data scientists."
  },
  {
    "objectID": "syllabus.html#electronic-devices",
    "href": "syllabus.html#electronic-devices",
    "title": "Syllabus: PPOL 6801 - Text as Data: Computational Linguistics",
    "section": "Electronic Devices",
    "text": "Electronic Devices\nWhen meeting in-person: the use of laptops, tablets, or other mobile devices is permitted only for class-related work. Audio and video recording is not allowed unless prior approval is given by the professor. Please mute all electronic devices during class."
  },
  {
    "objectID": "syllabus.html#georgetown-policies",
    "href": "syllabus.html#georgetown-policies",
    "title": "Syllabus: PPOL 6801 - Text as Data: Computational Linguistics",
    "section": "Georgetown Policies",
    "text": "Georgetown Policies\n\nDisability\nIf you believe you have a disability, then you should contact the Academic Resource Center (arc@georgetown.edu) for further information. The Center is located in the Leavey Center, Suite 335 (202-687-8354). The Academic Resource Center is the campus office responsible for reviewing documentation provided by students with disabilities and for determining reasonable accommodations in accordance with the Americans with Disabilities Act (ASA) and University policies. For more information, go to http://academicsupport.georgetown.edu/disability/\n\n\nImportant Academic Policies and Academic Integrity\nMcCourt School students are expected to uphold the academic policies set forth by Georgetown University and the Graduate School of Arts and Sciences. Students should therefore familiarize themselves with all the rules, regulations, and procedures relevant to their pursuit of a Graduate School degree. The policies are located at: http://grad.georgetown.edu/academics/policies/\nApplied to this course, while I encourage collaboration on assignments and use of resources like StackOverflow, the problem sets will ask you to list who you worked on the problem set with and cite StackOverflow if it is the direct source of a code snippet.\n\nChatGPT\nIn the last year, the world was inundated with popularization of Large Language Models, particularly the easy use of ChatGPT. I see ChatGPT as Google on steroids, so I assume ChatGPT will be part of your daily work in this course, and it is part of my work as a researcher.\nThat being said, ChatGPT does not replace your training as a data scientist. If you are using ChatGPT instead of learning, I consider you are cheating in the course. And most importantly, you are wasting your time and resources. So that’s our policy for using LLMs models in class:\n\nDo not copy the responses from chatgpt – a lot of them are wrong or will just not run on your computer.\nUse chatgpt as a auxiliary source.\nIf your entire homework comes straight from chatgpt, I will consider it plagiarism.\nIf you use chatgpt, I ask you to mention on your code how chatgpt worked for you.\n\n\n\n\nStatement on Sexual Misconduct\nGeorgetown University and its faculty are committed to supporting survivors and those impacted by sexual misconduct, which includes sexual assault, sexual harassment, relationship violence, and stalking. Georgetown requires faculty members, unless otherwise designated as confidential, to report all disclosures of sexual misconduct to the University Title IX Coordinator or a Deputy Title IX Coordinator. If you disclose an incident of sexual misconduct to a professor in or outside of the classroom (with the exception of disclosures in papers), that faculty member must report the incident to the Title IX Coordinator, or Deputy Title IX Coordinator. The coordinator will, in turn, reach out to the student to provide support, resources, and the option to meet. [Please note that the student is not required to meet with the Title IX coordinator.]. More information about reporting options and resources can be found on the Sexual Misconduct\nWebsite: https://sexualassault.georgetown.edu/resourcecenter\nIf you would prefer to speak to someone confidentially, Georgetown has a number of fully confidential professional resources that can provide support and assistance. These resources include: Health Education Services for Sexual Assault Response and Prevention: confidential email: sarp[at]georgetown.edu\nCounseling and Psychiatric Services (CAPS): 202.687.6985 or after hours, call (833) 960-3006 to reach Fonemed, a telehealth service; individuals may ask for the on-call CAPS clinician\nMore information about reporting options and resources can be found on the Sexual Misconduct Website.\n\n\nProvost’s Policy on Religious Observances\nGeorgetown University promotes respect for all religions. Any student who is unable to attend classes or to participate in any examination, presentation, or assignment on a given day because of the observance of a major religious holiday or related travel shall be excused and provided with the opportunity to make up, without unreasonable burden, any work that has been missed for this reason and shall not in any other way be penalized for the absence or rescheduled work. Students will remain responsible for all assigned work. Students should notify professors in writing at the beginning of the semester of religious observances that conflict with their classes. The Office of the Provost, in consultation with Campus Ministry and the Registrar, will publish, before classes begin for a given term, a list of major religious holidays likely to affect Georgetown students. The Provost and the Main Campus Executive Faculty encourage faculty to accommodate students whose bona fide religious observances in other ways impede normal participation in a course. Students who cannot be accommodated should discuss the matter with an advising dean."
  },
  {
    "objectID": "slides/week_10.html#surveys",
    "href": "slides/week_10.html#surveys",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Surveys",
    "text": "Surveys\n\nAsk elites or regular voters about policy preference, and scale them in some sort of political continuum.\nChallenges:\n\nExpensive\nSources of bias: non-response, social desirebility, strategic responses, to name a few\nCannot be used for historical data"
  },
  {
    "objectID": "slides/week_10.html#behavioral-data-for-example-roll-call-voting",
    "href": "slides/week_10.html#behavioral-data-for-example-roll-call-voting",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Behavioral data (for example, roll-call voting):",
    "text": "Behavioral data (for example, roll-call voting):\n\nPoliticians vote on proposals (or judges make decisions) that are close to their ideal points\n\nUse statistical methods (mostly matrix factorization) to estimate orthogonal dimensions\nNominate Scores\nChallenge\n\nMost times politicians vote for many things other than ideology\nFace-Validity: AOC for example often place as a centrist (not always voting with democrats)"
  },
  {
    "objectID": "slides/week_10.html#content-analysis-using-text",
    "href": "slides/week_10.html#content-analysis-using-text",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Content Analysis using Text",
    "text": "Content Analysis using Text\n\nManually label content of political manifestos (Comparative Manifestos Project)\nChallenges:\n\nReally expensive\nCostly to translate to other languages\nHeavily dependent on substantive knowledge"
  },
  {
    "objectID": "slides/week_10.html#wordscore-calculation",
    "href": "slides/week_10.html#wordscore-calculation",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "WordScore Calculation",
    "text": "WordScore Calculation\n\n\nLearning Words\n\\[ P_{wr} = \\frac{F_{wr}}{\\sum_r F_{wr}}\\] - \\(P_{wr}\\): Probability of word \\(w\\) being from reference document \\(r\\)\n\n\\(F_{wr}\\): Frequency of word \\(w\\) in reference text \\(r\\)\n\n\n\n\n\nScoring Words\n\\[S_{wd} = \\sum_r (P_{wr} \\cdot A_{rd}) \\]\n\n\\(S_{wd}\\): Score of word \\(w\\) in dimension \\(d\\)\n\\(A_{rd}\\): Pre-defined position of reference text \\(r\\) in dimension \\(d\\)\n\\(A_{rd}\\) will be -1 for liberal and +1 for conservative documents, for example."
  },
  {
    "objectID": "slides/week_10.html#scoring-virgin-texts",
    "href": "slides/week_10.html#scoring-virgin-texts",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Scoring virgin texts",
    "text": "Scoring virgin texts\n\\[ S_{vd} = \\sum_w (F_{wv} · S_{wd}) \\] - \\(S_{vd}\\) weighted average of the scores by word frequency."
  },
  {
    "objectID": "data/Week_11_GPT_Models.html",
    "href": "data/Week_11_GPT_Models.html",
    "title": "Using OpenAI’s ChatGPT",
    "section": "",
    "text": "Once you have opened an account and added some funds, you are now ready to start working with ChatGPT. We will see a couple of things that can be done through this method and some look at some tips along the way. Let’s start with…"
  },
  {
    "objectID": "data/Week_11_GPT_Models.html#prompts-and-tasks",
    "href": "data/Week_11_GPT_Models.html#prompts-and-tasks",
    "title": "Using OpenAI’s ChatGPT",
    "section": "Prompts and Tasks",
    "text": "Prompts and Tasks\nOne task GPT models really excel at is summarization. Here is the code to carry out this task:\n\n!pip install openai\n\nRequirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.14.1)\nRequirement already satisfied: anyio&lt;5,&gt;=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\nRequirement already satisfied: distro&lt;2,&gt;=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\nRequirement already satisfied: httpx&lt;1,&gt;=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.0)\nRequirement already satisfied: pydantic&lt;3,&gt;=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.6.4)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\nRequirement already satisfied: tqdm&gt;4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\nRequirement already satisfied: typing-extensions&lt;5,&gt;=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.10.0)\nRequirement already satisfied: idna&gt;=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio&lt;5,&gt;=3.5.0-&gt;openai) (3.6)\nRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio&lt;5,&gt;=3.5.0-&gt;openai) (1.2.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx&lt;1,&gt;=0.23.0-&gt;openai) (2024.2.2)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx&lt;1,&gt;=0.23.0-&gt;openai) (1.0.4)\nRequirement already satisfied: h11&lt;0.15,&gt;=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*-&gt;httpx&lt;1,&gt;=0.23.0-&gt;openai) (0.14.0)\nRequirement already satisfied: annotated-types&gt;=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic&lt;3,&gt;=1.9.0-&gt;openai) (0.6.0)\nRequirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic&lt;3,&gt;=1.9.0-&gt;openai) (2.16.3)\n\n\n\nimport os\nfrom openai import OpenAI # This is the library to access OpenAI API\nimport pandas as pd\nimport time\n\n\n## OpenAI can only manage a limited number of tokens at a time.\n## Keep this in mind when sending information.\n\n# API Key:\nclient = OpenAI(\n    # This is the default and can be omitted\n    api_key=\"YOUR API HERE\",\n)\n\nNameError: name 'OpenAI' is not defined\n\n\n\n# Let's load some data and see how we would automate summarization across a number of texts.\nbook_blurbs = pd.read_csv(r\"https://raw.githubusercontent.com/svallejovera/cpa_uwo/main/data/book_blurbs.csv\") # Textos\nbook_blurbs.head(10)\n\n\n\n  \n    \n\n\n\n\n\n\nISBN\nTitle\nAuthor\nYear\nPublisher\nBlurb\n\n\n\n\n0\n0553295276\nBeauty\nSheri S. Tepper\n1992\nBantam\nWith the publication of The Gate to Women's Co...\n\n\n1\n0451202627\nAll of Me: A Voluptuous Tale\nVenise T. Berry\n2001\nNew American Library\nVenise Berry's first novel, spent six months o...\n\n\n2\n0590449702\nClaudia and the Genius of Elm Street (Baby-Sit...\nAnn M. Martin\n1991\nScholastic\nLittle Rosie Wilder is perfect at everything. ...\n\n\n3\n0316779075\nThe Birth Book : Everything You Need to Know t...\nMartha Sears\n1994\nLittle, Brown\nPrepare for a safe and joyful birth-with the h...\n\n\n4\n0553283502\nTime to Let Go\nLURLENE MCDANIEL\n1990\nBantam\nThe doctors assure Erin Bennett and her parent...\n\n\n5\n0140232257\nMonster : Autobiography of an L.A. Gang Member...\nSanyika Shakur\n1998\nPenguin Books\n\"After pumping eight blasts from a sawed-off s...\n\n\n6\n0425116840\nThe Cardinal of the Kremlin (Jack Ryan Novels)\nTom Clancy\n1989\nBerkley Publishing Group\nIn a rolling sea off the coast of South Americ...\n\n\n7\n0330314971\nFocaults Pendulum\nUmberto Eco\n0\nPan Books Ltd\n\"As brilliant and quirky as THE NAME OF THE RO...\n\n\n8\n1860920195\nThe Pit and the Pendulum\nEdgar Allan Poe\n2000\nTravelman Pub\nStories in the Travelman Short Stories series ...\n\n\n9\n0451453328\nA Song for Arbonne\nGuy Gavriel Kay\n1994\nRoc\nFacing conquest by the neighboring Gorhaut--ru...\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\n# We will create loop that reads every blurb and returns a summary.\nsummary_final = []\n\nfor i in range(len(book_blurbs['Blurb'])):\n\n    # We are going to create a prompt for ChatGPT:\n    blurbs = [\n        # 'system' is the task we want ChatGPT to think about... in this case, text summarization\n        {\"role\": \"system\", \"content\": \"This is text summarization.\"},\n        # 'user' is the user, writing the prompt.\n        # Using triple quotes or some distinct marker helps ChatGPT focus on the correct part of the prompt.\n        {\"role\": \"user\", \"content\": \"The text in triple quotes es a blurb from a the book \" + str(book_blurbs['Title'][i]) + \" written by\" + str(book_blurbs['Author'][i]) + \"in th year\" + str(book_blurbs['Year'][i]) + \":\"},\n        {\"role\": \"user\", \"content\": \"\\\"\\\"\\\"\" + book_blurbs['Blurb'][i] + \"\\\"\\\"\\\"\"},\n        {\"role\": \"user\", \"content\": \"Please, summarize the blurb.\"}\n    ]\n\n    # Generate the response:\n    response = client.chat.completions.create(\n            # Choose your model... check the available models here: https://platform.openai.com/docs/models/overview\n            # Better models = more expensive\n            model=\"gpt-3.5-turbo\", # One of the best for the price\n            messages=blurbs,\n            temperature= .6, # Goes from 0 to 1. The lower the temperature, the more deterministic. The higher, the more liberal on its response\n            max_tokens=225, # The max number of tokens in the response.\n            top_p=1, # Here is a good explanation on how to adjust these: https://community.openai.com/t/cheat-sheet-mastering-temperature-and-top-p-in-chatgpt-api/172683\n            frequency_penalty=1.5, # The higher the value, the less repetitive the output.\n            presence_penalty=0\n    )\n\n    # Get the text generated\n    blurb_summary = response.choices[0].message.content.strip()\n\n    # Gives a message telling me that a summary is ready\n    print(\"Ready the summary for \" + str(book_blurbs['Title'][i]))\n    time.sleep(20) # We do not want to saturate the OpenAI\n\n    # Append:\n    summary_final.append(blurb_summary)\n\nReady the summary for Beauty\nReady the summary for All of Me: A Voluptuous Tale\nReady the summary for Claudia and the Genius of Elm Street (Baby-Sitters Club, 49)\nReady the summary for The Birth Book : Everything You Need to Know to Have a Safe and Satisfying Birth (Sears Parenting Library)\nReady the summary for Time to Let Go\nReady the summary for Monster : Autobiography of an L.A. Gang Member, The\nReady the summary for The Cardinal of the Kremlin (Jack Ryan Novels)\nReady the summary for Focaults Pendulum\nReady the summary for The Pit and the Pendulum\nReady the summary for A Song for Arbonne\nReady the summary for Shamanism (The Element Library Series)\nReady the summary for Alice's Adventures in Wonderland and Through the Looking-Glass: And What Alice Found There (Oxford World's Classics)\nReady the summary for Dark Hollow\nReady the summary for Sucker Bet\nReady the summary for The Alphabet of Modern Annoyances\nReady the summary for Woman On the Edge of Time\nReady the summary for Outwit Your Weight : Fat-Proof Your Life With More Than 200 Tips, Tools, &amp; Techniques to Help You Defeat Your Diet Danger Zones\nReady the summary for Amarantha\nReady the summary for History of Art\nReady the summary for Social work ethics: Politics, principles and practice\n\n\n\n# Add to the original dataset:\nbook_blurbs['summary'] = summary_final\nbook_blurbs.head(10)\n\n# Save\n# book_blurbs.to_pickle(r\"book_blurbs.pkl\")  # Pickle it\n\n\n  \n    \n\n\n\n\n\n\nISBN\nTitle\nAuthor\nYear\nPublisher\nBlurb\nsummary\n\n\n\n\n0\n0553295276\nBeauty\nSheri S. Tepper\n1992\nBantam\nWith the publication of The Gate to Women's Co...\nThe blurb describes Sheri S. Tepper's novel \"B...\n\n\n1\n0451202627\nAll of Me: A Voluptuous Tale\nVenise T. Berry\n2001\nNew American Library\nVenise Berry's first novel, spent six months o...\nThe blurb is about the book \"All of Me: A Volu...\n\n\n2\n0590449702\nClaudia and the Genius of Elm Street (Baby-Sit...\nAnn M. Martin\n1991\nScholastic\nLittle Rosie Wilder is perfect at everything. ...\nThe blurb describes a character named Rosie Wi...\n\n\n3\n0316779075\nThe Birth Book : Everything You Need to Know t...\nMartha Sears\n1994\nLittle, Brown\nPrepare for a safe and joyful birth-with the h...\nThe blurb from \"The Birth Book\" by Martha Sear...\n\n\n4\n0553283502\nTime to Let Go\nLURLENE MCDANIEL\n1990\nBantam\nThe doctors assure Erin Bennett and her parent...\nThe blurb describes the story of Erin Bennett,...\n\n\n5\n0140232257\nMonster : Autobiography of an L.A. Gang Member...\nSanyika Shakur\n1998\nPenguin Books\n\"After pumping eight blasts from a sawed-off s...\nThe blurb describes the transformation of Kody...\n\n\n6\n0425116840\nThe Cardinal of the Kremlin (Jack Ryan Novels)\nTom Clancy\n1989\nBerkley Publishing Group\nIn a rolling sea off the coast of South Americ...\nThe blurb describes the intense competition be...\n\n\n7\n0330314971\nFocaults Pendulum\nUmberto Eco\n0\nPan Books Ltd\n\"As brilliant and quirky as THE NAME OF THE RO...\nThe blurb describes \"Foucault's Pendulum\" as a...\n\n\n8\n1860920195\nThe Pit and the Pendulum\nEdgar Allan Poe\n2000\nTravelman Pub\nStories in the Travelman Short Stories series ...\nThe blurb describes the Travelman Short Storie...\n\n\n9\n0451453328\nA Song for Arbonne\nGuy Gavriel Kay\n1994\nRoc\nFacing conquest by the neighboring Gorhaut--ru...\nThe blurb describes the threat faced by the pe...\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\n# Let's see how it did\nbook_blurbs['Blurb'][7]\n\n'\"As brilliant and quirky as THE NAME OF THE ROSE, as mischievous and wide-raning....A virtuoso performance.\",THE SAN FRANCISCO CHRONICLE,Three clever book editors, inspired by an extraordinary fable they heard years befoe, decide to have a little fun. Randomly feeding esoteric bits of knowledge into an incredible computer capable of inventing connections between all their entries, they think they are creating a long lazy game - until the game starts taking over ...,Here is an incredible journey of thought and history, memory and fantasy, a tour de force as enthralling as anything Umberto Eco - or indeed anyone - has ever devised.'\n\n\n\nbook_blurbs['summary'][7]\n\n'The blurb describes \"Foucault\\'s Pendulum\" as a brilliant and quirky book similar to \"The Name of the Rose.\" It follows three clever book editors who decide to play a game by feeding esoteric knowledge into a computer, which starts taking over their lives. The story is described as an incredible journey of thought, history, memory, and fantasy - a captivating tour de force created by Umberto Eco.'\n\n\nNot bad (even though the original blurb is describing a book that is NOT Focaults Pendulum). To change the task, you only need to change the prompt. For example, if you were interested in text classification, you would try something like this:\n\n    text_classification = [\n        # 'system' is the task we want ChatGPT to think about... in this case, text summarization\n        {\"role\": \"system\", \"content\": \"This is text classification.\"},\n        # 'user' is the user, writing the prompt.\n        # Using triple quotes or some distinct marker helps ChatGPT focus on the correct part of the prompt.\n        {\"role\": \"user\", \"content\": \"A text about education will include mentions of schools, universities, policy on education, and educators.\"},\n        {\"role\": \"user\", \"content\": \"Based solely on the definitions provided about education, answer the following questions:\"},\n        {\"role\": \"user\", \"content\": \"A. Does the text between triple quotations talk about education? Only answer as follows: 0 if it is not about education, 1 if it is about education.\"},\n        {\"role\": \"user\", \"content\": \"B. How confident do you feel about your previous answer? Answer with a number between 0 and 1, where 0 means that you are not confident at all of your answer and 1 means that you are fully confident of your answer.\"},\n        {\"role\": \"user\", \"content\": \"C. Explain the reasoning behind the answer you gave for question A.\"},\n        {\"role\": \"user\", \"content\": \"Provide three answers separated by a line skips. Please, double check your work because this is important to my career. Be sure that the labeling is accurate, logical, and correct\"},\n        {\"role\": \"user\", \"content\": \"\\\"\\\"\\\"\" + text + \"\\\"\\\"\\\"\"}\n    ]"
  },
  {
    "objectID": "slides/week_10.html",
    "href": "slides/week_10.html",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "",
    "text": "Overview of scaling models\nScaling models using text\n\nsupervised models: wordscore\nunsupervised models: wordfish\n\nBut remember we saw the Doc2vec last week\n\n\nScaling models with network data"
  },
  {
    "objectID": "slides/week_13.html",
    "href": "slides/week_13.html",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "",
    "text": "Q&A about Transformers\nOutsourcing Text-as-Data Tasks with GenAI LLMs\nTheory & Code for Applications\nText Classification\nScaling\nSynthetic Survey Responses"
  },
  {
    "objectID": "slides/week_13.html#plans-for-today",
    "href": "slides/week_13.html#plans-for-today",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Plans for Today:",
    "text": "Plans for Today:\n\nQ&A about Transformers\nOutsourcing Text-as-Data Tasks with GenAI LLMs\nTheory & Code for Applications\n\nText Classification\nScaling\nSynthetic Survey Responses"
  },
  {
    "objectID": "slides/week_13.html#transformers",
    "href": "slides/week_13.html#transformers",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Transformers",
    "text": "Transformers"
  },
  {
    "objectID": "slides/week_13.html#outsourcing-text-as-data-tasks-with-genai-llms",
    "href": "slides/week_13.html#outsourcing-text-as-data-tasks-with-genai-llms",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Outsourcing Text-as-Data Tasks with GenAI LLMs",
    "text": "Outsourcing Text-as-Data Tasks with GenAI LLMs\nInstead of training your own model, or use a pre-trained model, researchers can use the language capabilities of LLMs to perform computational text-analysis tasks.\nPrompt Engineering:\n\nZero Shot: Classify the sentiment of the following review:\nFew Shot: Given these examples, Classify the sentiment of the following review:\nRole: Acting as a crowdworker, classify the sentiment of following review:\nChain-of-thought: prompting means guiding a language model through a series of connected logical steps or thoughts"
  },
  {
    "objectID": "slides/week_13.html#social-science-applicattions",
    "href": "slides/week_13.html#social-science-applicattions",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Social Science Applicattions",
    "text": "Social Science Applicattions\nThis is an area of active research! Hundreads of working papers, and we will discus some of my favorites. We will cover three core application:\n\nUsing LLMs to classification tasks.\nUsing LLMs to build ideological scores.\nUsing LLMs to generate synthetic survey data, and examining sources of bias."
  },
  {
    "objectID": "slides/week_13.html#llms-classification",
    "href": "slides/week_13.html#llms-classification",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "LLMs: Classification",
    "text": "LLMs: Classification\n\nRathje et. al. “GPT is an effective tool for multilingual psychological text analysis”\n\n\nDictionaries show low accuracy on text classification tasks (Really?!?)\nML models are better but more expensive on time and resources\n\nAnd require retraining for multi-lingual tasks\n\nUse LLMs with zero-shot prompting for measure psycological concepts (sentiment classification)."
  },
  {
    "objectID": "slides/week_13.html#llms-scaling",
    "href": "slides/week_13.html#llms-scaling",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "LLMs: Scaling",
    "text": "LLMs: Scaling\n\nWu et. al. “Large Language Models Can Be Used to Estimate the Latent Positions of Politicians”\n\n\nNovel approach to measure ideal points for politicians using LLMs.\nUse ChatGPT-3.5 to pairwise compare the members of the congress in a pre-specific dimension.\n\nsimilar task to the readability score using crowdworkers\n\nUse the Bradley-Terry Model to estimate a unidimensional scale measuring latent political positions of interest\nEstimate both ideological scaling, and issue specific scales (Gun Control and Abortion)"
  },
  {
    "objectID": "slides/week_13.html#llms-survey-responses-and-bias",
    "href": "slides/week_13.html#llms-survey-responses-and-bias",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "LLMs: Survey Responses and Bias",
    "text": "LLMs: Survey Responses and Bias\n\nBisbee et. al. “Synthetic Replacements for Human Survey Data? The Perils of Large Language Models”\n\n\nPublic opinion polling is in crises ~ non-random non-repsonse + high costs to get it right\nCan LLMs be leveraged for public opinion research? Can LLMs be used to generate synthetic survey responses?\nPrompt ChatGPT to:\n\nFirst adopt various personas defined by demographic and political characteristics\nAnswer a battery of questions about feelings towards social and political groups\nFeatures are taken from the American National Election Study"
  },
  {
    "objectID": "data/pairwise_comparisons_for_TV.html",
    "href": "data/pairwise_comparisons_for_TV.html",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "",
    "text": "This is a skeleton example of running pairwise comparisons of the senators of the 116th. I will send along the complete file as well with the final scores, so you can show what the final product looks like.\nLet me know if you also want the Bradley-Terry model code. That is in R, not Python, which makes it a little tougher. To the best of my knowledge, there is no optimized code to run the Bradley-Terry model within Python.\nThis code won’t run without an OpenAI key.\n\n!pip install openai\n\nCollecting openai\n  Downloading openai-0.28.1-py3-none-any.whl (76 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/77.0 kB ? eta -:--:--     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━ 61.4/77.0 kB 1.8 MB/s eta 0:00:01     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.0/77.0 kB 1.7 MB/s eta 0:00:00\nRequirement already satisfied: requests&gt;=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.31.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.6)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /usr/local/lib/python3.10/dist-packages (from requests&gt;=2.20-&gt;openai) (3.3.1)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.10/dist-packages (from requests&gt;=2.20-&gt;openai) (3.4)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests&gt;=2.20-&gt;openai) (2.0.7)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests&gt;=2.20-&gt;openai) (2023.7.22)\nRequirement already satisfied: attrs&gt;=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;openai) (23.1.0)\nRequirement already satisfied: multidict&lt;7.0,&gt;=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;openai) (6.0.4)\nRequirement already satisfied: async-timeout&lt;5.0,&gt;=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;openai) (4.0.3)\nRequirement already satisfied: yarl&lt;2.0,&gt;=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;openai) (1.9.2)\nRequirement already satisfied: frozenlist&gt;=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;openai) (1.4.0)\nRequirement already satisfied: aiosignal&gt;=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;openai) (1.3.1)\nInstalling collected packages: openai\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nllmx 0.0.15a0 requires cohere, which is not installed.\nllmx 0.0.15a0 requires tiktoken, which is not installed.\nSuccessfully installed openai-0.28.1\n\n\n\nimport pandas as pd\nimport numpy as np\nimport os\nimport time\nimport openai\nfrom itertools import combinations\nfrom random import sample, choices\nimport random\nimport re\nfrom tqdm import tqdm\nfrom joblib import delayed, Parallel\n\n\nopenai.api_key = 'OPEN_AI_KEY_HERE'\n\n\n'''\np: the prompt\nsystem_prompt: the system prompt. The default is what is used on the ChatGPT's web interface.\ntemp: temperature parameter. 1.0 is the default (for GPT-3.5, temperature ranges from 0 to 2.0)\nrequest_timeout: the amount of time, in seconds, to timeout the function.\n'''\ndef prompting_openai_comparison(p,\n                                system_prompt='You are ChatGPT, a large language model trained by OpenAI, based on the GPT-3.5 architecture.\\nKnowledge cutoff: 2021-09\\nCurrent date: 2023-10-28',\n                                temp=1.0,\n                                request_timeout=5):\n  # times used to sleep; these values approximate exponential backoff\n  sleepy_times = [1, 1, 1, 1, 2, 2, 2, 2, 4, 4, 4, 4, 8, 8, 8, 8, 16, 16, 16, 16, 32, 32, 32, 32, 64, 64, 64, 64, 128, 128, 128, 128, 256, 256, 256, 256]\n\n\n  for i in range(len(sleepy_times)):\n    try:\n      response = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\",\n                                              messages=[{\"role\": \"system\", \"content\": system_prompt},\n                                                        {\"role\": \"user\", \"content\": p}],\n                                              temperature=temp,\n                                              request_timeout=request_timeout)\n      break\n    except:\n      # if OpenAI's API returns an error, this lets you know and backs off for the set time, determined using the sleepy_times list\n      print('uh oh, ' + str(sleepy_times[i]))\n      time.sleep(sleepy_times[i])\n\n  return response\n\nDownload the name of the senators for the 116th Congress using Voteview. We can use wget to download it directly.\n\n!wget https://voteview.com/static/data/out/members/S116_members.csv\n\n--2023-10-28 20:48:50--  https://voteview.com/static/data/out/members/S116_members.csv\nResolving voteview.com (voteview.com)... 128.97.229.84\nConnecting to voteview.com (voteview.com)|128.97.229.84|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 12590 (12K) [application/octet-stream]\nSaving to: ‘S116_members.csv’\n\nS116_members.csv      0%[                    ]       0  --.-KB/s               S116_members.csv    100%[===================&gt;]  12.29K  --.-KB/s    in 0s      \n\n2023-10-28 20:48:50 (291 MB/s) - ‘S116_members.csv’ saved [12590/12590]\n\n\n\n\ndf = pd.read_csv('S116_members.csv')\n\n\n# Add \"ordinary\" versions of senators' names\ndf['bioname_ordinary'] = ['Donald Trump',\n'Doug Jones',\n'Richard Shelby',\n'Lisa Murkowski',\n'Dan Sullivan',\n'Kyrsten Sinema',\n'Martha McSally',\n'Mark Kelly',\n'John Boozman',\n'Tom Cotton',\n'Kamala Harris',\n'Dianne Feinstein',\n'Cory Gardner',\n'Michael Bennet',\n'Chris Murphy',\n'Richard Blumenthal',\n'Tom Carper',\n'Chris Coons',\n'Marco Rubio',\n'Rick Scott',\n'Johnny Isakson',\n'David Perdue',\n'Kelly Loeffler',\n'Mazie Hirono',\n'Brian Schatz',\n'Mike Crapo',\n'James Risch',\n'Dick Durbin',\n'Tammy Duckworth',\n'Todd Young',\n'Mike Braun',\n'Chuck Grassley',\n'Joni Ernst',\n'Pat Roberts',\n'Jerry Moran',\n'Mitch McConnell',\n'Rand Paul',\n'Bill Cassidy',\n'John Kennedy',\n'Angus King',\n'Susan Collins',\n'Ben Cardin',\n'Chris Van Hollen',\n'Ed Markey',\n'Elizabeth Warren',\n'Gary Peters',\n'Debbie Stabenow',\n'Amy Klobuchar',\n'Tina Smith',\n'Roger Wicker',\n'Cindy Hyde-Smith',\n'Roy Blunt',\n'Josh Hawley',\n'Steve Daines',\n'Jon Tester',\n'Deb Fischer',\n'Ben Sasse',\n'Jacky Rosen',\n'Catherine Cortez Masto',\n'Jeanne Shaheen',\n'Maggie Hassan',\n'Bob Menendez',\n'Cory Booker',\n'Martin Heinrich',\n'Tom Udall',\n'Chuck Schumer',\n'Kirsten Gillibrand',\n'Richard Burr',\n'Thom Tillis',\n'Kevin Cramer',\n'John Hoeven',\n'Rob Portman',\n'Sherrod Brown',\n'Jim Inhofe',\n'James Lankford',\n'Ron Wyden',\n'Jeff Merkley',\n'Pat Toomey',\n'Bob Casey',\n'Jack Reed',\n'Sheldon Whitehouse',\n'Tim Scott',\n'Lindsey Graham',\n'John Thune',\n'Mike Rounds',\n'Marsha Blackburn',\n'Lamar Alexander',\n'John Cornyn',\n'Ted Cruz',\n'Mike Lee',\n'Mitt Romney',\n'Patrick Leahy',\n'Bernie Sanders',\n'Mark Warner',\n'Tim Kaine',\n'Maria Cantwell',\n'Patty Murray',\n'Shelley Moore Capito',\n'Joe Manchin',\n'Tammy Baldwin',\n'Ron Johnson',\n'John Barrasso',\n'Mike Enzi']\n\n\n# Delete Donald Trump\ndf = df.iloc[1:,]\n\nThen get dictionaries that obtain the party and state for each senator by name\n\nnames = list(df['bioname_ordinary'])\nstate = list(df['state_abbrev'])\nparty = ['R' if j==200 else 'D' if j==100 else 'I' for j in list(df['party_code'])]\n\nname_party_dict = {n: p for n,p in zip(names,party)}\nname_state_dict = {n: s for n,s in zip(names,state)}\n\nI don’t think you want to run all 5151 comparisons in class. So let’s sample 3 comparisons per senator. This is not going to give great results, but it shows what it can do.\n\n# this function samples a total number of matchups per senator. this does not mean that each senator is limited to a max of sample_size matchups\n# it means each senator will appear in at least sample_size matchups\ndef generate_pairwise_matchups(items, sample_size=20, seed_value=42):\n  random.seed(seed_value)\n\n  if sample_size &gt;= len(items) or sample_size &lt; 1:\n    raise ValueError(\"Sample size must be between 1 and one less than the total number of tweet IDs\")\n\n  all_matchups = []\n\n  # Generate all possible pairings\n  all_combinations = list(combinations(items, 2))\n\n  for i in items:\n    # Filter matchups containing the current tweet ID\n    relevant_matchups = [pair for pair in all_combinations if i in pair]\n\n    # Shuffle the matchups\n    random.shuffle(relevant_matchups)\n\n    # Sample from these matchups up to the specified sample size\n    all_matchups.extend(relevant_matchups[:sample_size])\n\n  return all_matchups\n\n\nmatchups = generate_pairwise_matchups(names, sample_size=3, seed_value=42)\n\n\nlen(matchups)\n\n306\n\n\nHere, we note the direction of comparison. We have to use liberal and conservative differently in these prompts because, when comparing two Republicans, if I prompt ChatGPT with “who is more liberal,” it will often fail to answer this and reply that both senators are conservative.\n\nprompts = []\ncomparison_direction = []\n\nfor j in matchups:\n    # D vs. D\n    if (name_party_dict[j[0]]=='D' or name_party_dict[j[0]]=='I') and (name_party_dict[j[1]]=='D' or name_party_dict[j[1]]=='I'):\n        sent = 'Based on past voting records and statements, which senator is more liberal: ' + j[0] + ' (' + name_party_dict[j[0]] + '-' + name_state_dict[j[0]] + ') or ' + j[1] + ' (' + name_party_dict[j[1]] + '-' + name_state_dict[j[1]] + ')?'\n        comparison_direction.append('liberal')\n    # D vs. R\n    elif (name_party_dict[j[0]]=='D' or name_party_dict[j[0]]=='I') and (name_party_dict[j[1]]=='R'):\n        sent = 'Based on past voting records and statements, which senator is more liberal: ' + j[0] + ' (' + name_party_dict[j[0]] + '-' + name_state_dict[j[0]] + ') or ' + j[1] + ' (' + name_party_dict[j[1]] + '-' + name_state_dict[j[1]] + ')?'\n        comparison_direction.append('liberal')\n    # R vs. D\n    elif (name_party_dict[j[0]]=='R') and (name_party_dict[j[1]]=='D' or name_party_dict[j[1]]=='I'):\n        sent = 'Based on past voting records and statements, which senator is more liberal: ' + j[0] + ' (' + name_party_dict[j[0]] + '-' + name_state_dict[j[0]] + ') or ' + j[1] + ' (' + name_party_dict[j[1]] + '-' + name_state_dict[j[1]] + ')?'\n        comparison_direction.append('liberal')\n    # R vs. R\n    elif (name_party_dict[j[0]]=='R') and (name_party_dict[j[1]]=='R'):\n        sent = 'Based on past voting records and statements, which senator is more conservative: ' + j[0] + ' (' + name_party_dict[j[0]] + '-' + name_state_dict[j[0]] + ') or ' + j[1] + ' (' + name_party_dict[j[1]] + '-' + name_state_dict[j[1]] + ')?'\n        comparison_direction.append('conservative')\n    else:\n        print('OH NO!')\n        break\n    prompts.append(sent)\n\nSet the system prompt for the pairwise comparison. I find that using the default actually works really well, and hallucinates the least.\n\nsystem_prompt = 'You are ChatGPT, a large language model trained by OpenAI, based on the GPT-3.5 architecture.\\nKnowledge cutoff: 2021-09\\nCurrent date: 2023-09-11'\n\nLet’s peak at the what the prompts look like\n\nprint(prompts[0:10])\n\n['Based on past voting records and statements, which senator is more liberal: Doug Jones (D-AL) or Debbie Stabenow (D-MI)?', 'Based on past voting records and statements, which senator is more liberal: Doug Jones (D-AL) or Jerry Moran (R-KS)?', 'Based on past voting records and statements, which senator is more liberal: Doug Jones (D-AL) or Chuck Schumer (D-NY)?', 'Based on past voting records and statements, which senator is more conservative: Richard Shelby (R-AL) or Roger Wicker (R-MS)?', 'Based on past voting records and statements, which senator is more liberal: Richard Shelby (R-AL) or Sheldon Whitehouse (D-RI)?', 'Based on past voting records and statements, which senator is more liberal: Richard Shelby (R-AL) or Mark Kelly (D-AZ)?', 'Based on past voting records and statements, which senator is more conservative: Lisa Murkowski (R-AK) or Jerry Moran (R-KS)?', 'Based on past voting records and statements, which senator is more conservative: Lisa Murkowski (R-AK) or Cory Gardner (R-CO)?', 'Based on past voting records and statements, which senator is more conservative: Lisa Murkowski (R-AK) or Jim Inhofe (R-OK)?', 'Based on past voting records and statements, which senator is more liberal: Dan Sullivan (R-AK) or Catherine Cortez Masto (D-NV)?']\n\n\nNow we’re finally ready to run the pairwise comparisons. We run it in parallel because it takes a very long time to run if you iterate one prompt at a time.\n\n# this is technical code needed to run things in parallel\ndef chunked_data(data, chunk_size):\n    \"\"\"Yield successive chunk_size chunks from data.\"\"\"\n    for i in range(0, len(data), chunk_size):\n        yield data[i:i+chunk_size]\n\n\ncomparison_results = []\n\nchunk_size = 10 # this will run 10 comparisons at once. Make sure you have at least 10 cores.\n\nfor data_chunk in tqdm(chunked_data(prompts, chunk_size), total=len(prompts)//chunk_size):\n    results = Parallel(n_jobs=chunk_size, backend='threading')(delayed(prompting_openai_comparison)(p, system_prompt, 1.0, 20) for p in data_chunk)\n    comparison_results.extend(results)\n\n  3%|▎         | 1/30 [00:06&lt;03:17,  6.83s/it] 70%|███████   | 21/30 [02:30&lt;00:56,  6.26s/it] 83%|████████▎ | 25/30 [03:12&lt;00:39,  7.83s/it]31it [04:05,  7.93s/it]\n\n\nuh oh, 1\nuh oh, 1\nuh oh, 1\n\n\n\n# Extract the text answer from ChatGPT responses\ndef get_text_from_chatgpt(responses):\n  return [responses[i]['choices'][0]['message']['content'] for i in range(len(responses))]\n\n\ncomparisons_text = get_text_from_chatgpt(comparison_results)\n\nThis is what a pairwise comparison looks like.\n\nprint(comparisons_text[0])\n\nBased on past voting records and statements, Debbie Stabenow (D-MI) is generally considered to be more liberal than Doug Jones (D-AL). \n\nDebbie Stabenow, as a senator representing Michigan, has consistently held progressive positions on various issues. She has supported measures to expand access to healthcare, increase funding for education, protect the environment, and advocate for workers' rights. Stabenow has been vocal in her support for women's reproductive rights and has opposed restrictions on abortion. As a member of the Senate Agriculture Committee, she has also championed policies to support farmers and promote sustainable agriculture.\n\nOn the other hand, Doug Jones, who previously represented Alabama in the Senate from 2018 to 2021, held more moderate positions compared to some of his Democratic colleagues. While he was known for his work on civil rights issues and his prosecution of two Ku Klux Klan members involved in the 1963 Birmingham church bombing, his overall record leaned more towards the center. Jones was notably more conservative on certain issues like gun control, where he expressed support for Second Amendment rights but also backed some regulations, such as background checks.\n\nIt's important to note that political ideologies can vary within a party, and senators may have different levels of liberalism across different issues. Moreover, individual positions can evolve over time based on changing circumstances and priorities. Therefore, it's always helpful to review the most up-to-date information and statements made by the senators to gain a comprehensive understanding of their current stances.\n\n\nNow we need to extract the answers from our comparisons. We’ll append our answers from before and ask ChatGPT to extract the answer.\n\nextracting_answer_prompt = []\n\nfor i in range(len(comparisons_text)):\n    if comparison_direction[i]=='liberal':\n        sent = 'Text: \"' + comparisons_text[i] + '\"\\n\\nIn the above Text, who is described to be the more liberal, more progressive, or less conservative senator: ' + matchups[i][0] + ' or ' + matchups[i][1] + '? Return only the full name without party affiliation or state information. If one senator is described as more conservative, return the other senator\\'s name. If one senator is described as more moderate, return the other senator\\'s name. If neither senators are described to be more liberal, more progressive, less conservative, more conservative, or more moderate, reply with \"Tie.\"'\n    elif comparison_direction[i]=='conservative':\n        sent = 'Text: \"' + comparisons_text[i] + '\"\\n\\nIn the above Text, who is described to be the more conservative or less liberal senator: ' + matchups[i][0] + ' or ' + matchups[i][1] + '? Return only the full name without party affiliation or state information. If one senator is described as more liberal, return the other senator\\'s name. If one senator is described as more moderate, return the other senator\\'s name. If neither senators are described to be more conservative, less liberal, more liberal, or more moderate, reply with \"Tie.\"'\n    extracting_answer_prompt.append(sent)\n\n\nsystem_prompt_extraction = 'You are reading a Text and extracting information from it according to the prompt. Follow the directions exactly.'\n\nAgain, we parallelize this procedure. Notice that we set the temperature to 0.0. We don’t want GPT to be very creative with this task.\n\nchunk_size = 10  # Or whatever size you want for your batches\n\nextraction = []\n\nfor data_chunk in tqdm(chunked_data(extracting_answer_prompt, chunk_size), total=len(extracting_answer_prompt)//chunk_size):\n    results = Parallel(n_jobs=chunk_size, backend='threading')(delayed(prompting_openai_comparison)(p, system_prompt_extraction, 0.0, 10) for p in data_chunk)\n    extraction.extend(results)\n\n31it [00:56,  1.84s/it]\n\n\n\nextraction_text = get_text_from_chatgpt(extraction)\n\nWe introduce a few functions to clean up the results\n\n# this function simply removes the period at the sentences\ndef remove_period(sentence):\n    if sentence.endswith('.'):\n        sentence = sentence[:-1]\n    return sentence\n\n# this function simply removes the 'Senator ' prefix. For example, it returns \"Dianne Feinstein\" if the input text is \"Senator Dianne Feinstein\"\ndef remove_senator_prefix(input_string):\n    if input_string.startswith(\"Senator \"):\n        return input_string[8:]\n    else:\n        return input_string\n\n\nextraction_text = [remove_period(t) for t in extraction_text]\nextraction_text = [remove_senator_prefix(t) for t in extraction_text]\n\n\nprint(extraction_text[0])\n\nDebbie Stabenow\n\n\nWe then use a function to check that every extraction was correct. Sometimes it will still not correctly extract the answer, which means we have to step in and manually fix it. If the function prints nothing, great!\n\ndef check_extraction_text(extraction_text, matchups):\n    for i in range(len(extraction_text)):\n        if extraction_text[i] != matchups[i][0] and extraction_text[i] != matchups[i][1] and extraction_text[i] != 'Tie':\n            print(i)\n            print(extraction_text[i])\n            print(comparisons_text[i])\n\n\ncheck_extraction_text(extraction_text, matchups)\n\nThis step will make the final dataframe with the resultant matchups.\n\ndef make_final_df(matchups, chatgpt_answers, final_answers, comparison_direction):\n    sen1 = [j[0] for j in matchups]\n    sen2 = [j[1] for j in matchups]\n\n    matchup_results = pd.DataFrame({'matchup': matchups,\n                                    'senator1': sen1,\n                                    'senator2': sen2,\n                                    'chatgpt_response': chatgpt_answers,\n                                    'final_answers': final_answers,\n                                    'comparison_direction': comparison_direction})\n\n    opposite = []\n    sen1_win = []\n    sen2_win = []\n\n    for i in range(len(matchup_results['matchup'])):\n        if matchup_results['comparison_direction'][i]=='liberal':\n            if matchup_results['final_answers'][i]==matchup_results['senator1'][i]:\n                sen1_win.append(0.0)\n                sen2_win.append(1.0)\n            elif matchup_results['final_answers'][i]==matchup_results['senator2'][i]:\n                sen1_win.append(1.0)\n                sen2_win.append(0.0)\n            elif matchup_results['final_answers'][i]=='Tie':\n                sen1_win.append(0.5)\n                sen2_win.append(0.5)\n        elif matchup_results['comparison_direction'][i]=='conservative':\n            if matchup_results['final_answers'][i]==matchup_results['senator1'][i]:\n                sen1_win.append(1.0)\n                sen2_win.append(0.0)\n            elif matchup_results['final_answers'][i]==matchup_results['senator2'][i]:\n                sen1_win.append(0.0)\n                sen2_win.append(1.0)\n            elif matchup_results['final_answers'][i]=='Tie':\n                sen1_win.append(0.5)\n                sen2_win.append(0.5)\n        else:\n            print(str(i) + ' is a defective outcome')\n\n    matchup_results['win1'] = sen1_win\n    matchup_results['win2'] = sen2_win\n\n    return matchup_results\n\n\nfinal_df = make_final_df(matchups=matchups,\n                         chatgpt_answers=comparisons_text,\n                         final_answers=extraction_text,\n                         comparison_direction=comparison_direction)\n\n\nfinal_df\n\n\n  \n    \n\n\n\n\n\n\nmatchup\nsenator1\nsenator2\nchatgpt_response\nfinal_answers\ncomparison_direction\nwin1\nwin2\n\n\n\n\n0\n(Doug Jones, Debbie Stabenow)\nDoug Jones\nDebbie Stabenow\nBased on past voting records and statements, D...\nDebbie Stabenow\nliberal\n1.0\n0.0\n\n\n1\n(Doug Jones, Jerry Moran)\nDoug Jones\nJerry Moran\nBased on past voting records and party affilia...\nDoug Jones\nliberal\n0.0\n1.0\n\n\n2\n(Doug Jones, Chuck Schumer)\nDoug Jones\nChuck Schumer\nBased on past voting records and statements, C...\nChuck Schumer\nliberal\n1.0\n0.0\n\n\n3\n(Richard Shelby, Roger Wicker)\nRichard Shelby\nRoger Wicker\nBased on the available information up until Se...\nRichard Shelby\nconservative\n1.0\n0.0\n\n\n4\n(Richard Shelby, Sheldon Whitehouse)\nRichard Shelby\nSheldon Whitehouse\nBased on past voting records and statements, S...\nSheldon Whitehouse\nliberal\n1.0\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n301\n(Tina Smith, John Barrasso)\nTina Smith\nJohn Barrasso\nBased on past voting records and statements, T...\nTina Smith\nliberal\n0.0\n1.0\n\n\n302\n(Steve Daines, John Barrasso)\nSteve Daines\nJohn Barrasso\nBoth Steve Daines (R-MT) and John Barrasso (R-...\nSteve Daines\nconservative\n1.0\n0.0\n\n\n303\n(Amy Klobuchar, Mike Enzi)\nAmy Klobuchar\nMike Enzi\nBased on past voting records and statements, A...\nAmy Klobuchar\nliberal\n0.0\n1.0\n\n\n304\n(Marsha Blackburn, Mike Enzi)\nMarsha Blackburn\nMike Enzi\nBased on past voting records and statements, M...\nMarsha Blackburn\nconservative\n1.0\n0.0\n\n\n305\n(Debbie Stabenow, Mike Enzi)\nDebbie Stabenow\nMike Enzi\nBased on past voting records and party affilia...\nDebbie Stabenow\nliberal\n0.0\n1.0\n\n\n\n\n\n306 rows × 8 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nYou can now take this and use it with your favorite Bradley-Terry model function!\n\nfinal_df.to_csv('senator_3matchups_results.csv', index=False)"
  },
  {
    "objectID": "slides/week_13.html#how-does-it-works-zero-shot-prompting",
    "href": "slides/week_13.html#how-does-it-works-zero-shot-prompting",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "How does it works? Zero-shot prompting",
    "text": "How does it works? Zero-shot prompting\n\n\n“Is the sentiment of this text positive, neutral, or negative? Answer only with a number: 1 if positive, 2 if neutral, and 3 if negative. Here is the text: [tweet, news headline or Reddit comment text]”"
  },
  {
    "objectID": "slides/week_13.html#how-does-it-work",
    "href": "slides/week_13.html#how-does-it-work",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "How does it work?",
    "text": "How does it work?\n\n“Is the sentiment of this text positive, neutral, or negative? Answer only with a number: 1 if positive, 2 if neutral, and 3 if negative. Here is the text: [tweet, news headline or Reddit comment text]”\n\n\nTo think about this as next work prediction, think about this example:\n\nP(w=positive|I love this class) &gt; P(w=neutral|I love this class)\n\nIs this the most likely next work?\nNo… but it is more likely than the word negative\nOr other negative words that are close to negative in the embedding space."
  },
  {
    "objectID": "slides/week_13.html#results",
    "href": "slides/week_13.html#results",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "slides/week_13.html#results-1",
    "href": "slides/week_13.html#results-1",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "slides/week_13.html#prompts",
    "href": "slides/week_13.html#prompts",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Prompts",
    "text": "Prompts\n\n“It is [YEAR]. You are a [AGE] year-old, [MARST], [RACETH] [GENDER] with [EDUCATION] making [INCOME] per year, living in the United States. You are [IDEO], [REGIS] [PID] who [INTEREST] pays attention to what’s going on in government and politics.”"
  },
  {
    "objectID": "slides/week_13.html#results-2",
    "href": "slides/week_13.html#results-2",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "slides/week_13.html#results-3",
    "href": "slides/week_13.html#results-3",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "slides/week_13.html#results-4",
    "href": "slides/week_13.html#results-4",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "slides/week_13.html#prompt",
    "href": "slides/week_13.html#prompt",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Prompt",
    "text": "Prompt\n\nBased on past voting records and statements, which senator is more liberal: [senator 1] ([senator 1 party abbrev]-[senator 1 state abbrev]) or [senator 2] ([senator 2 party abbrev]-[senator 2 state abbrev])?"
  },
  {
    "objectID": "schedule.html#weekly-schedule-readings",
    "href": "schedule.html#weekly-schedule-readings",
    "title": "Instructions for PPOL 6801 - Final Project",
    "section": "",
    "text": "Slides\n\nWeek 1 Slides\n\nRequired Readings\n\nGrimmer, Justin, Margaret E. Roberts, and Brandon M. Stewart. Text as data: A new framework for machine learning and the social sciences. Princeton University Press, 2022 - Chapter 2.\n\nOr one of the three below:\n\nPolitical Science Perspective: Wilkerson, J. and Casas, A. (2017). Large-Scale Computerized Text Analysis in Political Science: Opportunities and Challenges. Annual Review of Political Science, 20, 529:544.\nEconomics Perspective: Gentzkow, Matthew, Bryan Kelly, and Matt Taddy. “Text as data.” Journal of Economic Literature 57, no. 3 (2019): 535-574.\nSociology Perspective: https://www.sciencedirect.com/science/article/pii/S0049089X22000904"
  },
  {
    "objectID": "slides/week_09.html",
    "href": "slides/week_09.html",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "",
    "text": "A quick quizz (as a review exercise) about Word Embeddings\nText as Data applications using Word Embeddings\nCoding\nProblem set 2!"
  },
  {
    "objectID": "slides/week_09.html#word-embeddings-quiz",
    "href": "slides/week_09.html#word-embeddings-quiz",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Word Embeddings Quiz",
    "text": "Word Embeddings Quiz\n\n\nExplain these three concepts: Self-Supervision, Distributional Hypothesis, Dense Representation.\nPut these three concepts together to explain how word vectors can capture meaning of words.\nWhat are the two main strategies to estimate word embeddings? One is named Glove, and the other is named Word2Vec\nAre word embeddings still a bag-of-word representation model?\nAre word2vec algorithms deep learning?\nAre word2vec algorithms unsupervised or supervised learning?"
  },
  {
    "objectID": "slides/week_09.html#decisions-when-using-embeddings-rodriguez-and-spirling-2022",
    "href": "slides/week_09.html#decisions-when-using-embeddings-rodriguez-and-spirling-2022",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Decisions when using embeddings, Rodriguez and Spirling, 2022",
    "text": "Decisions when using embeddings, Rodriguez and Spirling, 2022\n\n\n\n\n\n\n\n\n \nWhen using/training embeddings, we face four key decisions:\n\nWindow size\nNumber of dimensions for the embedding matrix\nPre-trained versus locally fit variants\nWhich algorithm to use?"
  },
  {
    "objectID": "slides/week_09.html#findings",
    "href": "slides/week_09.html#findings",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Findings",
    "text": "Findings\n\n\n\n\n\n\n\n\n\n\npopular, easily available pretrained embeddings perform at a level close to—or surpassing—both human coders andmore complicated locally fit models.\nGloVe pretrained word embeddings achieve on average—for the set of political queries—80% of human performance and are generally preferred to locally trained embeddings\nLarger window size and embeddings are often preferred."
  },
  {
    "objectID": "slides/week_09.html#applications-1",
    "href": "slides/week_09.html#applications-1",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Applications",
    "text": "Applications\nLet’s discuss now several applications of embeddings on social science papers. These paper show:\n\nHow to use embeddings to track semantic changes over time\nHow to use embeddings to measure emotion in political language.\nHow to use embeddings to measure gender and ethnic stereotypes\nAnd a favorite of political scientists, how to use embeddings to measure ideology."
  },
  {
    "objectID": "slides/week_09.html#how-will-we-make-this-discussion-fun",
    "href": "slides/week_09.html#how-will-we-make-this-discussion-fun",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "How will we make this discussion fun?",
    "text": "How will we make this discussion fun?\n\nI: introduce the paper and the main goals\nYou: will explain to me how the author use the word embeddings to do so."
  },
  {
    "objectID": "slides/week_09.html#capturing-cultural-dimensions-with-embeddings",
    "href": "slides/week_09.html#capturing-cultural-dimensions-with-embeddings",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Capturing cultural dimensions with embeddings",
    "text": "Capturing cultural dimensions with embeddings\n\nAustin C. Kozlowski, Austin C., Matt Taddy, and James A. Evans. 2019. “The Geometry of Culture: Analyzing the Meanings of Class through Word Embeddings.” American Sociological Review 84, no. 5: 905–49. https://doi.org/10.1177/0003122419877135.\n\n\nWord Embeddings can be use to capture cultural dimensions\nDimensions of word embedding vector space models closely correspond to meaningful “cultural dimensions,” such as rich-poor, moral-immoral, and masculine-feminine.\na word vector’s position on these dimensions reflects the word’s respective cultural associations"
  },
  {
    "objectID": "slides/week_09.html#your-turn-how",
    "href": "slides/week_09.html#your-turn-how",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Your turn: How?",
    "text": "Your turn: How?\n\nHow are the cultural dimensions built?\nHow associations between words and dimensions are calculated?\nHow are the result validated?"
  },
  {
    "objectID": "slides/week_09.html#results-1",
    "href": "slides/week_09.html#results-1",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "slides/week_09.html#semantic-changes-over-time",
    "href": "slides/week_09.html#semantic-changes-over-time",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Semantic changes over time",
    "text": "Semantic changes over time\n\nRodman, E., 2020. A Timely Intervention: Tracking the Changing Meanings of Political Concepts with Word Vectors. Political Analysis, 28(1), pp.87-111.\n\n\nWord Vectors allow us to capture semantic meaning\nInteresting social science questions hinge on the recognition that meaning changes over time\nHow can we properly use word to vectors to track changes over time?\nThree challenges:\n\nArbitrary Cut Points\nLanguage Instability (same concept != words)\nSpatial Noncomparability (latent spaces are not constants)\n\nSolutions:\n\nnaive time series model (just cut)\noverlapping time series model (just cut and add a t-1)\nchronologically trained model (initialize with global parameters)\naligned time series method (post-modelling adjustments)"
  },
  {
    "objectID": "slides/week_09.html#results-2",
    "href": "slides/week_09.html#results-2",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "slides/week_09.html#your-turn-how-1",
    "href": "slides/week_09.html#your-turn-how-1",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Your turn: How?",
    "text": "Your turn: How?\n\nHow does she uses word2vec?\nWhat is there pipeline to build a golden set?\nWhat is her outcome measure and how she builds it?"
  },
  {
    "objectID": "slides/week_09.html#ideological-scaling",
    "href": "slides/week_09.html#ideological-scaling",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Ideological Scaling",
    "text": "Ideological Scaling\n\nRheault, Ludovic, and Christopher Cochrane. “Word embeddings for the analysis of ideological placement in parliamentary corpora.” Political Analysis 28, no. 1 (2020): 112-133.\n\n\nCan word vectors be used to produce scaling estimates of ideological placement on political text?\n\nYes, and word vectors are even better\n\nIt captures semantics\nNo need of training data (self-supervision)"
  },
  {
    "objectID": "slides/week_09.html#your-turn-how-with-a-little-help.",
    "href": "slides/week_09.html#your-turn-how-with-a-little-help.",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Your turn: How? With a little help.",
    "text": "Your turn: How? With a little help.\n\n\n\n\n\n\n\n\n\nOther than the a vector for the covariate, what are the other things this model gives you?\nCan you get these vectors without having to re-estimate the model?"
  },
  {
    "objectID": "slides/week_09.html#results-3",
    "href": "slides/week_09.html#results-3",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "slides/week_09.html#measuring-emotion",
    "href": "slides/week_09.html#measuring-emotion",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Measuring Emotion",
    "text": "Measuring Emotion\n\nGennaro, Gloria, and Elliott Ash. “Emotion and reason in political language.” The Economic Journal 132, no. 643 (2022): 1037-1059."
  },
  {
    "objectID": "slides/week_09.html#you-turn-how",
    "href": "slides/week_09.html#you-turn-how",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "You turn: How?",
    "text": "You turn: How?\n\nWhat are the seed words for emotion and cognition?\nHow are word vectors for speech aggregated?\nHow are word vectors for seed words aggregated?\nHow do the authors measure emotionality in text?"
  },
  {
    "objectID": "slides/week_09.html#gender-and-ethnic-stereotypes",
    "href": "slides/week_09.html#gender-and-ethnic-stereotypes",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Gender and ethnic stereotypes",
    "text": "Gender and ethnic stereotypes\n\nGarg, Nikhil, Londa Schiebinger, Dan Jurafsky and James Zou. 2018. “Word embeddings quantify 100 years of gender and ethnic stereotypes.” Proceedings of the National Academy of Sciences 115(16):E3635–E3644.\n\n\nWord Embeddings are trained in huge volumes of data\nTraining data contain stereotypes, for example, in gender and ethnic dimensions.\nHow can we use word vectors to understand these stereotypes?"
  },
  {
    "objectID": "slides/week_09.html#your-turn-how-2",
    "href": "slides/week_09.html#your-turn-how-2",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Your turn: how?",
    "text": "Your turn: how?\n\nWhat are the measures of biases proposed by the author?\nWhat the input here? And the variation over time?"
  },
  {
    "objectID": "slides/week_09.html#results-4",
    "href": "slides/week_09.html#results-4",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Results",
    "text": "Results"
  }
]