[
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Week\n\n\nTopic\n\n\nDate\n\n\n\n\n\n\nWeek 01\n\n\nIntroduction, Overview of the course\n\n\nJanuary 17, 2024\n\n\n\n\nWeek 02\n\n\nFrom Text to Matrices: Representing Text as Data\n\n\nJanuary 24, 2024\n\n\n\n\nWeek 03\n\n\nText Similarity, Text re-use and Complexity\n\n\nJanuary 31, 2024\n\n\n\n\nWeek 04\n\n\nSupervised Learning I: Dictionary Methods and Off-the-Shelf Classifiers\n\n\nFebruary 07, 2024\n\n\n\n\nWeek 05\n\n\nSupervised Learning II: Training your own classifiers\n\n\nFebruary 14, 2024\n\n\n\n\nWeek 06\n\n\nReplication Class I: Students’ Presentation\n\n\nFebruary 21, 2023\n\n\n\n\nWeek 07\n\n\nUnsupervised Learning: Topic Models\n\n\nFebruary 28, 2024\n\n\n\n\nWeek 08\n\n\nWord Embeddings: What are they and how to estimate?\n\n\nMarch 13, 2024\n\n\n\n\nWeek 09\n\n\nWord Embeddings: Social Science Applications\n\n\nMarch 20, 2024\n\n\n\n\nWeek 10\n\n\nUsing Text to Measure Ideology - Scaling\n\n\nMarch 27, 2024\n\n\n\n\nWeek 11\n\n\nReplication Class II\n\n\nApril 03, 2024\n\n\n\n\nWeek 12\n\n\nLarge Language Models: Theory and Fine-tuning a Transformers-based model (Invited Speaker Dr. Sebastian Vallejo)\n\n\nApril 10, 2024\n\n\n\n\nWeek 13\n\n\nLarge Language Models: Outsourcing Applications\n\n\nApril 17, 2023\n\n\n\n\nWeek 14\n\n\nPresentations of Final Projects\n\n\nApril 24, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "schedule.html#weekly-schedule",
    "href": "schedule.html#weekly-schedule",
    "title": "Schedule",
    "section": "",
    "text": "Week\n\n\nTopic\n\n\nDate\n\n\n\n\n\n\nWeek 01\n\n\nIntroduction, Overview of the course\n\n\nJanuary 17, 2024\n\n\n\n\nWeek 02\n\n\nFrom Text to Matrices: Representing Text as Data\n\n\nJanuary 24, 2024\n\n\n\n\nWeek 03\n\n\nText Similarity, Text re-use and Complexity\n\n\nJanuary 31, 2024\n\n\n\n\nWeek 04\n\n\nSupervised Learning I: Dictionary Methods and Off-the-Shelf Classifiers\n\n\nFebruary 07, 2024\n\n\n\n\nWeek 05\n\n\nSupervised Learning II: Training your own classifiers\n\n\nFebruary 14, 2024\n\n\n\n\nWeek 06\n\n\nReplication Class I: Students’ Presentation\n\n\nFebruary 21, 2023\n\n\n\n\nWeek 07\n\n\nUnsupervised Learning: Topic Models\n\n\nFebruary 28, 2024\n\n\n\n\nWeek 08\n\n\nWord Embeddings: What are they and how to estimate?\n\n\nMarch 13, 2024\n\n\n\n\nWeek 09\n\n\nWord Embeddings: Social Science Applications\n\n\nMarch 20, 2024\n\n\n\n\nWeek 10\n\n\nUsing Text to Measure Ideology - Scaling\n\n\nMarch 27, 2024\n\n\n\n\nWeek 11\n\n\nReplication Class II\n\n\nApril 03, 2024\n\n\n\n\nWeek 12\n\n\nLarge Language Models: Theory and Fine-tuning a Transformers-based model (Invited Speaker Dr. Sebastian Vallejo)\n\n\nApril 10, 2024\n\n\n\n\nWeek 13\n\n\nLarge Language Models: Outsourcing Applications\n\n\nApril 17, 2023\n\n\n\n\nWeek 14\n\n\nPresentations of Final Projects\n\n\nApril 24, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "slides/week_05.html#housekeeping",
    "href": "slides/week_05.html#housekeeping",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Housekeeping",
    "text": "Housekeeping\nToday is your deadline for the problem set 1.\n\n\nReplications next week\n\nPresentation (20 min each):\n\nIntroduction;\nMethods;\nResults;\nDifferences;\nAutopsy of the replication;\nExtensions\n\nRepository (by friday):\n\nGithub Repo\n\nreadme\nyour presentation pdf\ncode\n5pgs report"
  },
  {
    "objectID": "slides/week_05.html#where-are-we",
    "href": "slides/week_05.html#where-are-we",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Where are we?",
    "text": "Where are we?\nWe started from pre-processing text as data, representing text as numbers, and describing features of the text.\nLast week, we started learning how to measure concepts in text:\n\n\nDocuments pertaining to certain classes and how we can use statistical assumptions to measure these classes\n\n\n\n\nDictionary Methods\n\nDiscuss some well-known dictionaries\n\nOff-the-Shelf Classifiers\n\nPerspective API\nHugging Face (only see as a off-the-shelf machines, LMMs later in this course)"
  },
  {
    "objectID": "slides/week_05.html#remember",
    "href": "slides/week_05.html#remember",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Remember…",
    "text": "Remember…\n\n\nUnsupervised Models: learning (hidden or latent) structure in unlabeled data.\n\nTopic Models to cluster documents and words\n\n\n\n\n\nSupervised Models: learning relationship between inputs and a labeled set of outputs.\n\nSentiment Analysis, classify if a tweet contains misinformation, etc..\n\n\n\n\n\nIn TAD, we mostly use unsupervised techniques for discovery and supervised for measurement of concepts."
  },
  {
    "objectID": "slides/week_05.html#assuming",
    "href": "slides/week_05.html#assuming",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Assuming:",
    "text": "Assuming:"
  },
  {
    "objectID": "slides/week_05.html#supervised-learning-pipeline-for-tad",
    "href": "slides/week_05.html#supervised-learning-pipeline-for-tad",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Supervised Learning Pipeline for TAD",
    "text": "Supervised Learning Pipeline for TAD\n\n\nStep 1: label some examples of the concept of we want to measure\n\nsome tweets are positive, some are neutral and some are negative\n\nStep 2: train a statistical model on these set of label data using the document-feature matrix as input\n\nchoose a model (transformation function) that gives higher out-of-sample accuracy\n\nStep 3: use the classifier - some f(x) - to predict unseen documents.\n\npick the best out-sample perfirmance\n\nStep 4: use the measure + metadata|exogenous shocks to learn something new about the world.\n\nThis is where social science happens!"
  },
  {
    "objectID": "slides/week_05.html#supervised-learning-vs-dictionaries",
    "href": "slides/week_05.html#supervised-learning-vs-dictionaries",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Supervised Learning vs Dictionaries",
    "text": "Supervised Learning vs Dictionaries\nDictionary methods:\n\nAdvantage: not corpus-specific, cost to apply to a new corpus is trivial\nDisadvantage: not corpus-specific, so performance on a new corpus is unknown (domain shift)\n\nSupervised learning:\n\nGeneralization of dictionary methods\nFeatures associated with each categories (and their relative weight) are learned from the data\nBy construction, ML will outperform dictionary methods in classification tasks, as long as training sample is large enough"
  },
  {
    "objectID": "slides/week_05.html#supervised-learning-pipeline-for-tad-1",
    "href": "slides/week_05.html#supervised-learning-pipeline-for-tad-1",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Supervised Learning Pipeline for TAD",
    "text": "Supervised Learning Pipeline for TAD\n\nStep 1: label some examples of the concept of we want to measure\nStep 2 train a statistical model on these set of label data using the document-feature matrix as input\nStep 3: use the classifier - some f(x) - to predict unseen documents.\nStep 4: use the measure + metadata|exogenous shocks to learn something new about the world."
  },
  {
    "objectID": "slides/week_05.html#how-to-obtain-a-labeled-dataset",
    "href": "slides/week_05.html#how-to-obtain-a-labeled-dataset",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "How to obtain a labeled dataset?",
    "text": "How to obtain a labeled dataset?\n\n\nExternal Source of Annotation: someone else labelled the data for you\n\nFederalist papers\nMetadata from text\nManifestos from Parties with well-developed dictionaries\n\nExpert annotation: put experts in quotation\n\nmostly undergrads ~ that you train to be experts\n\nCrowd-sourced coding: digital labor markets\n\nWisdom of Crowsds: the idea that large groups of non-expert people are collectively smarter than individual experts when it comes to problem-solving"
  },
  {
    "objectID": "slides/week_05.html#crowdsourcing-as-a-research-tool-for-ml",
    "href": "slides/week_05.html#crowdsourcing-as-a-research-tool-for-ml",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Crowdsourcing as a research tool for ML",
    "text": "Crowdsourcing as a research tool for ML\n  \n\nCrowdsourcing is now understood to mean using the Internet to distribute a large package of small tasks to a large number of anonymous workers, located around the world and offered small financial rewards per task. The method is widely used for data-processing tasks such as image classification, video annotation, data entry, optical character recognition, translation, recommendation, and proofreading\n\n\n\nSource: Benoit et al, 2016"
  },
  {
    "objectID": "slides/week_05.html#benoit-et-al-206-crowdsourcing-political-texts",
    "href": "slides/week_05.html#benoit-et-al-206-crowdsourcing-political-texts",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Benoit et al, 206: Crowdsourcing Political Texts",
    "text": "Benoit et al, 206: Crowdsourcing Political Texts\n\n\nExpert annotation is expensive.\nBenoit, Conway, Lauderdale, Laver and Mikhaylov (2016) note that classification jobs could be given to a large number of relatively cheap online workers\nMultiple workers ~ similar task ~ same stimuli ~ wisdom of crowds!\nRepresentativeness of a broader population doesn’t matter ~ not a populational quantity, it is just a measurement task\nTheir task: Manifestos ~ sentences ~ workers:\n\nsocial|economic\n\nvery-left vs very right\n\n\nReduce uncertainty by having more workers for each sentence"
  },
  {
    "objectID": "slides/week_05.html#comparing-experts-and-online-workers",
    "href": "slides/week_05.html#comparing-experts-and-online-workers",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Comparing Experts and online workers",
    "text": "Comparing Experts and online workers"
  },
  {
    "objectID": "slides/week_05.html#how-many-workers",
    "href": "slides/week_05.html#how-many-workers",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "How many workers?",
    "text": "How many workers?"
  },
  {
    "objectID": "slides/week_05.html#section-1",
    "href": "slides/week_05.html#section-1",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "",
    "text": "Source: Pablo Barbera’s CSS Seminar"
  },
  {
    "objectID": "slides/week_05.html#supervised-learning-pipeline-for-tad-2",
    "href": "slides/week_05.html#supervised-learning-pipeline-for-tad-2",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Supervised Learning Pipeline for TAD",
    "text": "Supervised Learning Pipeline for TAD\n\nStep 1: label some examples of the concept of we want to measure\nStep 2 train a statistical model on these set of label data using the document-feature matrix as input\nStep 3: use the classifier - some f(x) - to predict unseen documents.\nStep 4: use the measure + metadata|exogenous shocks to learn something new about the world."
  },
  {
    "objectID": "slides/week_05.html#general-thoughts",
    "href": "slides/week_05.html#general-thoughts",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "General Thoughts",
    "text": "General Thoughts\nOnce we have our training data, we need to pick a classifier. We face these challenges:\n\n\nin text as data, often your DFM has Features &gt; Documents\n\nidentification problems for statistical models\noverfitting the data\n\n\n\n\n\nBias-Variance Trade-off\n\nfit a overly complicated model ~ leads to higher variance\nfit a more flexible model ~ leads to more bias\n\n\n\n\n\nMany models:\n\nNaive Bayes\nRegularized regression\nSVM\nk-nearest neighbors, tree-based methods, etc.\nEnsemble methods + DL"
  },
  {
    "objectID": "slides/week_05.html#bias-and-variance-tradeoff",
    "href": "slides/week_05.html#bias-and-variance-tradeoff",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Bias and Variance Tradeoff",
    "text": "Bias and Variance Tradeoff"
  },
  {
    "objectID": "slides/week_05.html#train-validation-test-or-cross-validation",
    "href": "slides/week_05.html#train-validation-test-or-cross-validation",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Train-Validation-Test OR Cross Validation",
    "text": "Train-Validation-Test OR Cross Validation"
  },
  {
    "objectID": "slides/week_05.html#many-models",
    "href": "slides/week_05.html#many-models",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Many Models",
    "text": "Many Models"
  },
  {
    "objectID": "slides/week_05.html#but-not-so-different",
    "href": "slides/week_05.html#but-not-so-different",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "But not so different…",
    "text": "But not so different…"
  },
  {
    "objectID": "slides/week_05.html#regularized-ols-regression",
    "href": "slides/week_05.html#regularized-ols-regression",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Regularized OLS Regression",
    "text": "Regularized OLS Regression\nThe simplest, but highly effective, way to avoid overfit and improve out-sample accuracy is to add penalty parameters for statistical models:\n\nOLS Loss Function :\n\\[\nRSS = \\sum_{i=1}^{N} \\left( y_i - \\beta_0 - \\sum_{j=1}^{J} \\beta_j x_{ij} \\right)^2\n\\]\n\n\nOLS + Penalty:\n\\[\n\\text{RSS} = \\sum_{i=1}^{N} \\left( y_i - \\beta_0 - \\sum_{j=1}^{J} \\beta_j x_{ij} \\right)^2 + \\lambda \\sum_{j=1}^{J} \\beta_j^2 \\rightarrow \\text{ridge regression}\n\\]\n\\[\n\\text{RSS} = \\sum_{i=1}^{N} \\left( y_i - \\beta_0 - \\sum_{j=1}^{J} \\beta_j x_{ij} \\right)^2 + \\lambda \\sum_{j=1}^{J} |\\beta_j| \\rightarrow \\text{lasso regression}\n\\]"
  },
  {
    "objectID": "slides/week_05.html#supervised-learning-pipeline-for-tad-3",
    "href": "slides/week_05.html#supervised-learning-pipeline-for-tad-3",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Supervised Learning Pipeline for TAD",
    "text": "Supervised Learning Pipeline for TAD\n\nStep 1: label some examples of the concept of we want to measure\nStep 2 train a statistical model on these set of label data using the document-feature matrix as input\nStep 3: use the classifier - some f(x) - to predict unseen documents.\nStep 4: use the measure + metadata|exogenous shocks to learn something new about the world."
  },
  {
    "objectID": "slides/week_05.html#evaluating-the-performance",
    "href": "slides/week_05.html#evaluating-the-performance",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Evaluating the Performance",
    "text": "Evaluating the Performance\n\n\n\n\n\nPredicted\n\n\n\n\n\n\n\n\nJ\n¬J\nTotal\n\n\nActual\nJ\na (TP)\nb (FN)\na+b\n\n\n\n¬J\nc (FP)\nd (TN)\nc+d\n\n\n\nTotal\na+c\nb+d\nN\n\n\n\n\n\nAccuracy: number correctly classified/total number of cases = (a+d)/(a+b+c+d)\nPrecision : number of TP / (number of TP+number of FP) = a/(a+c) .\n\nFraction of the documents predicted to be J, that were in fact J.\nThink as a measure for the estimator\n\nRecall: (number of TP) / (number of TP + number of FN) = a /(a+b)\n\nFraction of the documents that were in fact J, that method predicted were J.\nThink as a measure for the data\n\nF : 2 precision*recall / precision+recall\n\nHarmonic mean of precision and recall."
  },
  {
    "objectID": "slides/week_05.html#barbera-et-al-2020-guide-for-supervised-models-with-text",
    "href": "slides/week_05.html#barbera-et-al-2020-guide-for-supervised-models-with-text",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Barbera et al, 2020, Guide for Supervised Models with Text",
    "text": "Barbera et al, 2020, Guide for Supervised Models with Text\n\n\n\n\n\nvia GIPHY\n\n\n Task: Tone of New York Times coverage of the economy. Discusses:\n\nHow to build a corpus\nUnit of analysis\nDocuments or Coders?\nML or Dictionaries?"
  },
  {
    "objectID": "slides/week_06.html#housekeeping",
    "href": "slides/week_06.html#housekeeping",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Housekeeping",
    "text": "Housekeeping\nLet’s quickly review your future assignments:\n\n\nProblem Set 3\n\nNo problem set 2\nAssigned: week 9, March 20\nDue: Following week.\n\nReplication Class II\n\nIn group\nSelect you paper this week\nMore rigorous ~&gt; Contact the author soon to get the raw data!\nClass: April 03\n\nFinal Project\n\nProposal: EOD Friday, Week 9, March 20\n\nyou have to meet with me before submitting your proposal\nSend me a draft of the proposal before the meeting\n\nPresentation: Week 14."
  },
  {
    "objectID": "slides/week_06.html#where-are-we",
    "href": "slides/week_06.html#where-are-we",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Where are we?",
    "text": "Where are we?\nWe started from pre-processing text as data, representing text as numbers, describing features of the tex, and learned how to measure concepts in text:\n\n\nLast Week: Supervised learning ~ Training your models\n\nCrowdsourcing label classification\nFull pipeline for model training\nRegularized regressions\nEvaluating Performance\n\nThis week: unsupervised learning\n\nBegin to take a purely inductive approach\nDiscovery\nLook for things we don’t know about in the text."
  },
  {
    "objectID": "slides/week_06.html#overview-unsupervised-learning",
    "href": "slides/week_06.html#overview-unsupervised-learning",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Overview: Unsupervised Learning",
    "text": "Overview: Unsupervised Learning\n\nData: humans, documents, votes, etc. are not pre-labelled in terms of some underlying concept.\n\nThink about congressional speeches, we know the author, their party, other metadata, but:\n\nwe don’t yet know what that speech ‘represents’ in terms of its latent properties, what ‘kind’ of speech it is, what ‘topics’ it covers, what speeches it is similar to conceptually, etc.\n\n\nGoal is to take the observations and find hidden structure and meaning in them.\n\nsimilarity\ngroups\ntopics\nassociation between word, etc…"
  },
  {
    "objectID": "slides/week_06.html#main-challenges-of-topic-models",
    "href": "slides/week_06.html#main-challenges-of-topic-models",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Main challenges of Topic Models",
    "text": "Main challenges of Topic Models\n\n\nHard to get it right\n\n\nunsupervised learning requires several ad-hoc decisions and these decisions matter for quality of your results\n\nnumber of clusters\nnumber of topics\npre-processing steps\n\nDomain knowledge (and honestly a bit of randomness) guides a lot of these decisions\n\n\n\n\n\nHard to know if you are doing right!\n\n\nin contrast to supervised approaches, we won’t know ‘how correct’ the output is in a simple statistical sense\nuse statistical measures of fit/unfit of different modeling decisions\n\nbut in general, it will involve a hugely amount of qualitative assessment.\n\nNo easy measure of acccuracy, recall and precision."
  },
  {
    "objectID": "slides/week_06.html#k-means-clustering",
    "href": "slides/week_06.html#k-means-clustering",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "K-means Clustering",
    "text": "K-means Clustering\nPurpose: look for ‘groups’ in data explicitly.\n\n\nInput: text + number of clusters\nOutput: documents ~&gt; clusters"
  },
  {
    "objectID": "slides/week_06.html#visually",
    "href": "slides/week_06.html#visually",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Visually",
    "text": "Visually"
  },
  {
    "objectID": "slides/week_06.html#visually-1",
    "href": "slides/week_06.html#visually-1",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Visually",
    "text": "Visually"
  },
  {
    "objectID": "slides/week_06.html#visually-2",
    "href": "slides/week_06.html#visually-2",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Visually",
    "text": "Visually"
  },
  {
    "objectID": "slides/week_06.html#visually-3",
    "href": "slides/week_06.html#visually-3",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Visually",
    "text": "Visually"
  },
  {
    "objectID": "slides/week_06.html#cluster-methods-vs-topic-models",
    "href": "slides/week_06.html#cluster-methods-vs-topic-models",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Cluster Methods vs Topic Models",
    "text": "Cluster Methods vs Topic Models\nTopics models can be thought as a probabilistic generalization of of clustering methods\n\nClustering:\n\nEvery document is assigned to a cluster\n\nTopic Models:\n\nevery document has a probability distribution of topic.\nevery topic has a probability distribution of words."
  },
  {
    "objectID": "slides/week_06.html#topid-models-intuition",
    "href": "slides/week_06.html#topid-models-intuition",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Topid Models: Intuition",
    "text": "Topid Models: Intuition\n\nCapture words that are more likely to occur together across a set of documents.\nAssign these words a probability of being part of a cluster (topic).\nAssign documents a probability of being associated of these clusters.\n\nDocuments: formed from probability distribution of topics\n\na speech can be 40% about trade, 30% about sports, 10% about health, and 20% spread across topics you don’t think make much sense\n\nTopics: fromed from probability distribution over words\n\nthe topic health will have words like hospital, clinic, dr., sick, cancer"
  },
  {
    "objectID": "slides/week_06.html#blei-2012",
    "href": "slides/week_06.html#blei-2012",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Blei, 2012,",
    "text": "Blei, 2012,"
  },
  {
    "objectID": "slides/week_06.html#intuition-language-model",
    "href": "slides/week_06.html#intuition-language-model",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Intuition: Language Model",
    "text": "Intuition: Language Model\n\nStep 1: For each document:\n\nRandomly choose a distribution over topics. That is, choose one of many multinomial distributions, each which mixes the topics in different proportions.\n\nStep 2: Then, for every word in the document\n\nRandomly choose a topic from the distribution over topics from step 1.\nRandomly choose a word from the distribution over the vocabulary that the topic implies."
  },
  {
    "objectID": "slides/week_06.html#step-1-or-what-a-multinomial-distribution-looks-like",
    "href": "slides/week_06.html#step-1-or-what-a-multinomial-distribution-looks-like",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Step 1: or what a multinomial distribution looks like",
    "text": "Step 1: or what a multinomial distribution looks like\nFor each document ~ Randomly choose a distribution over topics from a multinomal distribution"
  },
  {
    "objectID": "slides/week_06.html#step-2-sampling-words",
    "href": "slides/week_06.html#step-2-sampling-words",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Step 2: sampling words",
    "text": "Step 2: sampling words\nFor every word:\n\n\nRandomly choose a topic from the distribution over topics from step 1.\nRandomly choose a word from the distribution over the vocabulary that the topic implies."
  },
  {
    "objectID": "slides/week_06.html#latent-dirichlet-allocation",
    "href": "slides/week_06.html#latent-dirichlet-allocation",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Latent Dirichlet Allocation",
    "text": "Latent Dirichlet Allocation\nTo estimate the model, we need to assume some known mathematical distributions for this data generating process:\n\nFor every topic:\n\n\\(\\beta_k \\sim \\text{Dirichlet}(\\tau)\\)\n\nFor every document:\n\n\\(\\theta_d \\sim \\text{Dirichlet}(\\alpha)\\),\n\nFor every word:\n\na topic \\(z_{dn} \\sim \\text{Multinomial}(\\theta_d)\\).\na word \\(w_{dn} \\sim \\text{Multinomial}(\\beta_{z_{dn}})\\)\n\nwhere:\n\n\\(\\alpha\\) and \\(\\tau\\) are hyperparameter of the Dirichlet priors\n\\(\\beta_k\\) is drawn from a Dirichlet for per-topic word distribution\n\\(\\theta_d\\) is drawn from a Dirichlet for the topic distribution for documents - \\(K\\) topics\n\\(D\\) documents in the corpus\n\\(N_d\\) words in document \\(d\\)"
  },
  {
    "objectID": "slides/week_06.html#aside-dirichlet-distribution",
    "href": "slides/week_06.html#aside-dirichlet-distribution",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Aside: Dirichlet distribution",
    "text": "Aside: Dirichlet distribution\n\nThe Dirichlet distribution is a conjugate prior for the multinomial distribution.\n\nIt makes joint distributions easier to calculate because we know their families.\n\nIt is parameterized by a vector of positive real numbers (\\(alpha\\))\n\nLarger values of \\(\\alpha\\) (assuming we are in symmetric case) mean we think (a priori) that documents are generally an even mix of the topics.\nIf \\(\\alpha\\) is small (less than 1) we think a given document is generally from one or a few topics."
  },
  {
    "objectID": "slides/week_06.html#visually-4",
    "href": "slides/week_06.html#visually-4",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Visually",
    "text": "Visually\n\n\n\n\n\n\n\nSource: Andrew Heiss"
  },
  {
    "objectID": "slides/week_06.html#exercise-plate-notation",
    "href": "slides/week_06.html#exercise-plate-notation",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Exercise: Plate Notation",
    "text": "Exercise: Plate Notation"
  },
  {
    "objectID": "slides/week_06.html#inference-how-to-estimate-all-these-parameters",
    "href": "slides/week_06.html#inference-how-to-estimate-all-these-parameters",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Inference: How to estimate all these parameters?",
    "text": "Inference: How to estimate all these parameters?\nUse the observed data, the words, to make an inference about the latent parameters: the \\(\\beta\\)s, the \\(z\\)s, the \\(\\theta\\)s.\nWe start with the joint distribution implied by our language model (Blei, 2012):\n\\[\np(\\beta_{1:K}, \\theta_{1:D}, z_{1:D}, w_{1:D})= \\prod_{K}^{i=1}p(\\beta_i)\\prod_{D}^{d=1}p(\\theta_d)(\\prod_{N}^{n=1}p(z_{d,n}|\\theta_d)p(w_{d,n}|\\beta_{1:K},z_{d,n})\n\\]\nTo get to the conditional:\n\\[\np(\\beta_{1:K}, \\theta_{1:D}, z_{1:D}|w_{1:D})=\\frac{p(\\beta_{1:K}, \\theta_{1:D}, z_{1:D}, w_{1:D})}{p(w_{1:D})}\n\\]\nThe denominator is hard complicate to be estimate (requires integration for every word for every topic):\n\nSimulate with Gibbs Sampling or Variational Inference.\nTake a Bayesian statistic course to learn more about this type of inference!"
  },
  {
    "objectID": "slides/week_06.html#show-me-results",
    "href": "slides/week_06.html#show-me-results",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Show me results!",
    "text": "Show me results!"
  },
  {
    "objectID": "slides/week_06.html#choosing-the-number-of-topics",
    "href": "slides/week_06.html#choosing-the-number-of-topics",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Choosing the number of topics",
    "text": "Choosing the number of topics\n\n\nChoosing K is “one of the most difficult questions in unsupervised learning” (Grimmer and Stewart, 2013, p.19)\nCommon approach: decide based on cross-validated statistical measures model fit or other measures of topic quality."
  },
  {
    "objectID": "slides/week_06.html#validation-of-topics",
    "href": "slides/week_06.html#validation-of-topics",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Validation of topics",
    "text": "Validation of topics\n\nWorking with topic models require a lot of back-and-forth and humans in the loop.\nHow to measure the quality of the topics?\n\n\n\n\nCrowdsourcing for:\n\nwhether a topic has (human-identifiable) semantic coherence: word intrusion, asking subjects to identify a spurious word inserted into a topic\nwhether the association between a document and a topic makes sense: topic intrusion, asking subjects to identify a topic that was not associated with the document by the model\nSee Ying et al, 2022, Political Analysis."
  },
  {
    "objectID": "slides/week_06.html#barbera-et-al-american-political-science-review-2020.",
    "href": "slides/week_06.html#barbera-et-al-american-political-science-review-2020.",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Barbera et al, American Political Science Review, 2020.",
    "text": "Barbera et al, American Political Science Review, 2020.\n\nData: tweets sent by US legislators, samples of the public, and media outlets.\nLDA with K = 100 topics\nTopic predictions are used to understand agenda-setting dynamics (who leads? who follows?)\nConclusion: Legislators are more likely to follow, than to lead, discussion of public issues,\nDecisions:\nk=100"
  },
  {
    "objectID": "slides/week_06.html#motolinia-american-political-science-review-2021",
    "href": "slides/week_06.html#motolinia-american-political-science-review-2021",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Motolinia, American Political Science Review, 2021",
    "text": "Motolinia, American Political Science Review, 2021\n\nData: transcripts of legislative sessions in Mexican states\nCorrelated Topic model to identify “particularistic” legislation; i.e. laws with clear benefits to voters\nEach topic is then classified into particularistic or not\nValidation: correlation with spending\nUse exogenous electoral reform that allowed legislators to be re-elected"
  },
  {
    "objectID": "slides/week_06.html#exercise",
    "href": "slides/week_06.html#exercise",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Exercise",
    "text": "Exercise\n\nTake 5-min to read the methods sections of this paper\nList to me some of the decisions the authors need to make to get the topic models to work.\nDo you think these make sense? What would you do different"
  },
  {
    "objectID": "slides/week_06.html#extensions-many-more-beyond-lda",
    "href": "slides/week_06.html#extensions-many-more-beyond-lda",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Extensions: Many more beyond LDA",
    "text": "Extensions: Many more beyond LDA\n\nStructural topic model: allow (1) topic prevalence, (2) topic content to vary as a function of document-level covariates (e.g., how do topics vary over time or documents produced in 1990 talk about something differently than documents produced in 2020?); implemented in stm in R (Roberts, Stewart, Tingley, Benoit)\nCorrelated topic model: way to explore between-topic relationships (Blei and Lafferty, 2017); implemented in topicmodels in R; possibly somewhere in Python as well!\nKeyword-assisted topic model: seed topic model with keywords to try to increase the face validity of topics to what you’re trying to measure; implemented in keyATM in R (Eshima, Imai, Sasaki, 2019)\nBertTopic: BERTopic is a topic modeling technique that leverages transformers and TF-IDF to create dense clusters of words."
  },
  {
    "objectID": "slides/week_06.html#stm-adding-structure-to-the-lda",
    "href": "slides/week_06.html#stm-adding-structure-to-the-lda",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "STM: Adding Structure to the LDA",
    "text": "STM: Adding Structure to the LDA\n\n\n\n\n\n\n\n\n  \n\n\nPrevalence: Prior on the mixture over topics is now document-specific, and can be a function of covariates.\nContent: distribution over words is now document-specific and can be a function of covariates.\n\nSee Roberts et al 2014"
  },
  {
    "objectID": "slides/week_06.html#keyword-assisted-topic-model-summary",
    "href": "slides/week_06.html#keyword-assisted-topic-model-summary",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Keyword-assisted topic model: summary",
    "text": "Keyword-assisted topic model: summary"
  },
  {
    "objectID": "slides/week_06.html#keyword-assisted-topic-model-performance",
    "href": "slides/week_06.html#keyword-assisted-topic-model-performance",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Keyword-assisted topic model: performance",
    "text": "Keyword-assisted topic model: performance"
  },
  {
    "objectID": "slides/week_03.html#housekeeping",
    "href": "slides/week_03.html#housekeeping",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Housekeeping",
    "text": "Housekeeping\nThanks for selecting the articles for replication exercise! Next steps:\n\n\nGet access to the data and code ASAP\n\nHarvard Dataverse, see footnotes in the papers, contact the authors\nany issue, please, talk to me!\n\nIf the data is too big for your laptop, use a sample of the data.\nIf the paper has much more than the text analysis, ignore it, just focus on the TAD component.\nAny questions?"
  },
  {
    "objectID": "slides/week_03.html#where-are-we",
    "href": "slides/week_03.html#where-are-we",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Where are we?",
    "text": "Where are we?\nThree primary challenges dealing with text as data:\n\nChallenge I: Text is high dimensional\nChallenge II: Text is unstructured data source\nChallenge III: Outcomes live in the latent space\n\n\nLast week:\n\nPre-processing text + bag of words ~&gt; reduces greatly text complexity (dimensions)\nText representation using vectors of numbers ~&gt; document feature matrix (text to numbers)"
  },
  {
    "objectID": "slides/week_03.html#plans-for-today",
    "href": "slides/week_03.html#plans-for-today",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Plans for today",
    "text": "Plans for today\nWe will start thinking about latent outcomes. Our first approach will focus on descriptive inference about documents:\n\nComparing documents\nUsing similarity to measure text-reuse\nEvaluating complexity in text\nWeighting (TF-iDF)"
  },
  {
    "objectID": "slides/week_03.html#recall-vector-space-model",
    "href": "slides/week_03.html#recall-vector-space-model",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Recall: Vector space model",
    "text": "Recall: Vector space model\nTo represent documents as numbers, we will use the vector space model representation:\n\n\nA document \\(D_i\\) is represented as a collection of features \\(W\\) (words, tokens, n-grams, etc…)\nEach feature \\(w_i\\) can be placed in a real line\nA document \\(D_i\\) is a point in a \\(\\mathbb{R}^W\\)\n\nEach document is now a vector,\nEach entry represents the frequency of a particular token or feature.\nStacking those vectors on top of each other gives the document feature matrix (DFM)."
  },
  {
    "objectID": "slides/week_03.html#document-feature-matrix-fundamental-unit-of-tad",
    "href": "slides/week_03.html#document-feature-matrix-fundamental-unit-of-tad",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Document-Feature Matrix: fundamental unit of TAD",
    "text": "Document-Feature Matrix: fundamental unit of TAD\n\n\n\n\n\n\n\n\n\n\n\nSource: Arthur Spirling TAD Class"
  },
  {
    "objectID": "slides/week_03.html#in-a-two-dimensional-space",
    "href": "slides/week_03.html#in-a-two-dimensional-space",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "In a two dimensional space",
    "text": "In a two dimensional space\n\n\n\nDocuments, W=2\n\n\nDocument 1 = “yes yes yes no no no”\nDocument 2 = “yes yes yes yes yes yes”"
  },
  {
    "objectID": "slides/week_03.html#how-far-is-document-a-from-document-b",
    "href": "slides/week_03.html#how-far-is-document-a-from-document-b",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "How `far’ is document a from document b?",
    "text": "How `far’ is document a from document b?\nUsing the vector space, we can use notions of geometry to build well-defined comparison/similarity measures between the documents.\n\n\nin multiple dimensions!!"
  },
  {
    "objectID": "slides/week_03.html#euclidean-distance",
    "href": "slides/week_03.html#euclidean-distance",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Euclidean Distance",
    "text": "Euclidean Distance\nThe ordinary, straight line distance between two points in space. Using document vectors \\(y_a\\) and \\(y_b\\) with \\(j\\) dimensions\n\n\n\nEuclidean Distance\n\n\n\\[\n||y_a - y_b|| = \\sqrt{\\sum^{j}(y_{aj} - y_{bj})^2}\n\\]\n\n\n\nCan be performed for any number of features J ~ has nice mathematical properties\n\nno negative distances: sij   0 2 distance between documents is zero () documents are identical 3 distance between documents is symmetric: sij = sji 4 measures satisfy triangle inequality. sik   sij + sjk"
  },
  {
    "objectID": "slides/week_03.html#euclidean-distance-w2",
    "href": "slides/week_03.html#euclidean-distance-w2",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Euclidean Distance, w=2",
    "text": "Euclidean Distance, w=2\n\n\n\nEuclidean Distance\n\n\n\\[\n||y_a - y_b|| = \\sqrt{\\sum^{j}(y_{aj} - y_{bj})^2}\n\\]\n\n\n\n\n\\(y_a\\) = [0, 2.51, 3.6, 0] and \\(y_b\\) = [0, 2.3, 3.1, 9.2]\n\\(\\sum_{j=1}^j (y_a - y_b)^2\\) = \\((0-0)^2 + (2.51-2.3)^2 + (3.6-3.1)^2 + (9-0)^2\\) = \\(84.9341\\)\n\\(\\sqrt{\\sum_{j=1}^j (y_a - y_b)^2}\\) = 9.21"
  },
  {
    "objectID": "slides/week_03.html#exercise",
    "href": "slides/week_03.html#exercise",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Exercise",
    "text": "Exercise\n\n\n\nDocuments, W=3 {yes, no}\n\n\nDocument 1 = “yes yes yes no no no” (3, 3)\nDocument 2 = “yes yes yes yes yes yes” (6,0)\nDocument 3= “yes yes yes no no no yes yes yes no no no yes yes yes no no no yes yes yes no no no” (12, 12)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhich documents will the euclidean distance place closer together?\nDoes it look like a good measure for similarity?\n\nDoc C = Doc A * 3"
  },
  {
    "objectID": "slides/week_03.html#cosine-similarity",
    "href": "slides/week_03.html#cosine-similarity",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Cosine Similarity",
    "text": "Cosine Similarity\nEuclidean distance rewards magnitude, rather than direction\n\\[\n\\text{cosine similarity}(\\mathbf{y_a}, \\mathbf{y_b}) = \\frac{\\mathbf{y_a} \\cdot \\mathbf{y_b}}{\\|\\mathbf{y_a}\\| \\|\\mathbf{y_b}\\|}\n\\]\nUnpacking the formula:\n\n\\(\\mathbf{y_a} \\cdot \\mathbf{y_b}\\) ~ dot product between vectors\n\nprojecting common magnitudes\nmeasure of similarity (see textbook)\n\\(\\sum_j{y_{aj}*y_{bj}}\\)\n\n\\(||\\mathbf{y_a}||\\) ~ vector magnitude, length ~ \\(\\sqrt{\\sum{y_{aj}^2}}\\)\nnormalizes similarity by documents’ length ~ independent of document length be because it deals only with the angle of the vectors\ncosine similarity captures some notion of relative direction (e.g. style or topics in the document)"
  },
  {
    "objectID": "slides/week_03.html#cosine-similarity-1",
    "href": "slides/week_03.html#cosine-similarity-1",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Cosine Similarity",
    "text": "Cosine Similarity\nCosine function has a range between -1 and 1.\n\nConsider: cos (0) = 1, cos (90) = 0, cos (180) = -1"
  },
  {
    "objectID": "slides/week_03.html#exercise-1",
    "href": "slides/week_03.html#exercise-1",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Exercise",
    "text": "Exercise\nThe cosine function can range from [-1, 1]. When thinking about document vectors, cosine similarity is actually constrained to vary only from 0 - 1.\n\nWhy does cosine similarity for document vectors can never be lower than zero? Think about the vector representation and the document feature matrix."
  },
  {
    "objectID": "slides/week_03.html#more-metrics",
    "href": "slides/week_03.html#more-metrics",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "More metrics",
    "text": "More metrics\nThere are a large number of distance/similarity metrics out there, just to name a few:\n\nJaccard Similarity: overlap between documents\nManhattan Distance: absolute distance between documents\nCanberra Distance: Weighted version of Manhattan Distance\nMinowski: generalized version of Euclidean\n\nNo single best measure, depends on your research question."
  },
  {
    "objectID": "slides/week_03.html#mozer-et-al-2020-matching-with-text-data",
    "href": "slides/week_03.html#mozer-et-al-2020-matching-with-text-data",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Mozer et al, 2020 ‘Matching with Text Data’",
    "text": "Mozer et al, 2020 ‘Matching with Text Data’\nBut some recent research show Document Feature Matrix (DTM) + Cosine similarity works well to perceived similarity on documents"
  },
  {
    "objectID": "slides/week_03.html#linder-et.-al-2020---text-as-policy",
    "href": "slides/week_03.html#linder-et.-al-2020---text-as-policy",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Linder et. al, 2020 - Text as Policy",
    "text": "Linder et. al, 2020 - Text as Policy\n\n\n\n\n\n\n\n\n\n\n\n\nHow is cosine similarity used in the application?\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nMain assumption: text reuse serves as a summary measure of the greatest overlap observed across all relevant policy dimensions represented in legislative text.\nHow? Identifying large segments of equivalent or highly similar text.\nSmith-Waterman local alignment algorithm\n\nlongest sequence of overlap allowing for gaps and mismatches\n\nThe SW algorithm amounts to a systematic procedure for scoring similar sequences of text, and efficiently finding the highest scoring sequences in two documents.\nAnalysis:\n\nSelection: Elastic search for 500 candidates\nSmith Waterman alignment algorithm\nDowneight give average cosine dissimilarity between allignment and a random sample of 1000 other alignments. If the alignment is everywhere, downweight."
  },
  {
    "objectID": "slides/week_03.html#lexical-diversity",
    "href": "slides/week_03.html#lexical-diversity",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Lexical Diversity",
    "text": "Lexical Diversity\n\nLength refers to the size in terms of: characters, words, lines, sentences, paragraphs, pages, sections, chapters, etc.\nTokens are generally words ~ useful semantic unit for processing\nTypes are unique tokens.\nTypically \\(N_{tokens}\\) &gt;&gt;&gt;&gt; \\(N_{types}\\)\n\n\nType-to-Token ratio\n\\[ TTR: \\frac{\\text{total type}}{\\text{total tokens}} \\]\nSo… authors with limited vocabularies will have a low lexical diversity"
  },
  {
    "objectID": "slides/week_03.html#issues-with-ttr-and-extensions",
    "href": "slides/week_03.html#issues-with-ttr-and-extensions",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Issues with TTR and Extensions",
    "text": "Issues with TTR and Extensions\n\nTTR is very sensitive to overall document length,\n\nshorter texts may exhibit fewer word repetitions\n\nLength also correlates with topic variation ~ more types being added to the document\n\n\n\nOther Measures\n\nGuiraud: \\(\\frac{\\text{total type}}{\\sqrt{\\text{total tokens}}}\\)\nS Summer’s Index: \\(\\frac{\\text{log(total type)}}{\\text{log(total tokens)}}}\\)\nMTTR: the Moving-Average Type-Token Ratio (Covington and McFall, 2010)"
  },
  {
    "objectID": "slides/week_03.html#readability",
    "href": "slides/week_03.html#readability",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Readability",
    "text": "Readability\nAnother way to think about textual complexity is to consider readability.\nReadability: ease with which reader (especially of given education) can comprehend a text\n\n\nCombines both difficulty (text) and sophistication (reader)\n\nUse a combination of syllables and sentence length to indicate difficulty\nHuman inputs to built parameters\nFlesch-Kincaid readability index\n\nMeasurement problems from education research\naverage grade of students who could correctly answer at least 75% of some multiple-choice questions"
  },
  {
    "objectID": "slides/week_03.html#flesch-kincaid-readability-index",
    "href": "slides/week_03.html#flesch-kincaid-readability-index",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Flesch-Kincaid readability index",
    "text": "Flesch-Kincaid readability index\n\n\n\nFlesch Reading Ease (FRE)\n\n\n\\[\nFRE = 206.835 - 1.015\\left(\\frac{\\mbox{total words}}{\\mbox{total sentences}}\\right)-84.6\\left(\\frac{\\mbox{total syllables}}{\\mbox{total words}}\\right)\n\\]\n\n\n\n\n\n\nFlesch-Kincaid (Rescaled to US Educational Grade Levels)\n\n\n\\[\nFRE = 15.59 - 0.39\\left(\\frac{\\mbox{total words}}{\\mbox{total sentences}}\\right)- 11.8\\left(\\frac{\\mbox{total syllables}}{\\mbox{total words}}\\right)\n\\]\n\n\n\nInterpretation: 0-30: university level; 60-70: understandable by 13-15 year olds; and 90-100 easily understood by an 11-year old student."
  },
  {
    "objectID": "slides/week_03.html#spirling-2016.-the-effects-of-the-second-reform-act",
    "href": "slides/week_03.html#spirling-2016.-the-effects-of-the-second-reform-act",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Spirling, 2016. The effects of the Second Reform Act",
    "text": "Spirling, 2016. The effects of the Second Reform Act"
  },
  {
    "objectID": "slides/week_03.html#benoit-et-al.-2019-political-sophistication",
    "href": "slides/week_03.html#benoit-et-al.-2019-political-sophistication",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Benoit et al., 2019, Political Sophistication",
    "text": "Benoit et al., 2019, Political Sophistication\nApproach\n\nGet human judgments of relative textual easiness for specifically political texts.\nUse a logit model to estimate latent “easiness” as equivalent to the “ability” parameter in the Bradley-Terry framework.\nUse these as training data for a tree-based model. Pick most important parameters\nRe-estimate the models using these covariates (Logit + covariates)\nUsing these parameters, one can “predict” the easiness parameter for a given new text\n - Nice plus ~ add uncertainty to model-based estimates via bootstrapping"
  },
  {
    "objectID": "slides/week_03.html#benoit-et-al.-2019-political-sophistication-1",
    "href": "slides/week_03.html#benoit-et-al.-2019-political-sophistication-1",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Benoit et al., 2019, Political Sophistication",
    "text": "Benoit et al., 2019, Political Sophistication"
  },
  {
    "objectID": "slides/week_03.html#can-we-do-better-than-just-using-frequencies",
    "href": "slides/week_03.html#can-we-do-better-than-just-using-frequencies",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Can we do better than just using frequencies?",
    "text": "Can we do better than just using frequencies?\nSo far our inputs for the vector representation of documents have relied simply the word frequencies.\nCan we do better?\n\nOne option: weighting\nWeights:\n\nReward words more unique;\nPunish words that appear in most documents\n\n\n\n\n\nTF-IDF = Term Frequency - Inverse Document Frequency\n\n\n\\(\\text{TF-IDF}(t, d) = \\text{TF}(t, d) \\times \\text{IDF}(t)\\) - \\(\\text{TF}(t, d) = \\frac{\\text{Number of times term } t \\text{ appears in document } d}{\\text{Total number of terms in document } d}\\) - \\(\\text{IDF}(t) = \\log \\left( \\frac{\\text{Total number of documents}}{\\text{Number of documents with term } t \\text{ in them}} \\right)\\)"
  },
  {
    "objectID": "slides/week_03.html#federalist-papers",
    "href": "slides/week_03.html#federalist-papers",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Federalist Papers",
    "text": "Federalist Papers\n\n\n\n\n\n     \n\n\nSource: Grimmer, Roberts, and Stewart, Text as Data, 2022"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PPOL 6801: Text as Data - Computational Linguistics",
    "section": "",
    "text": "In recent years, the surge in the availability of textual data, ranging from the digitalization of archival documents, political speeches, social media posts, and online news articles, has led to a growing demand for statistical analysis using large volumes of text data. In this course, we will teach students how to analyze and somewhat collect this data from a social science viewpoint. The focus is on understanding the real-world use of text as data rather than just the theory behind it. Students will learn how to acquire text, turn text into data, and analyze it to answer important policy-relevant questions. Each week, we’ll go over different methods, like building and using dictionaries, understanding sentiment in text, scaling texts on ideological and policy dimensions, and using machine learning to classify text. Lectures will include hands-on activities, letting students work directly with actual texts, and I strongly encourage students to bring their own data to class. The course aims to equip students with a variety of text analysis techniques that will be valuable for their future work as policy experts and computational social scientists.\nWhile the course covers an interdisciplinary topic, and many of the techniques we discuss have their origins in computer science or statistics, we will spend relatively little time on traditional Natural Language Processing techniques, such as machine translation, optical character recognition, and parts of speech tagging, etc. Although we will touch on Large Language Models (ChatGPT) at the end of the course, we will focus mostly on the practical use of these models through their APIs instead of building up and providing an in-depth understanding of their architecture.\nI assume students taking this class have taken, at minimum, an introductory class in statistics and have basic knowledge of probability, distributions, hypothesis testing, and linear models. The core language and software environment of this course is R. If you are not familiar with R, you will struggle with the assigned exercises. We will also provide some code in Python, but no prior knowledge here is assumed since this will be additional material."
  },
  {
    "objectID": "index.html#course-description",
    "href": "index.html#course-description",
    "title": "PPOL 6801: Text as Data - Computational Linguistics",
    "section": "",
    "text": "In recent years, the surge in the availability of textual data, ranging from the digitalization of archival documents, political speeches, social media posts, and online news articles, has led to a growing demand for statistical analysis using large volumes of text data. In this course, we will teach students how to analyze and somewhat collect this data from a social science viewpoint. The focus is on understanding the real-world use of text as data rather than just the theory behind it. Students will learn how to acquire text, turn text into data, and analyze it to answer important policy-relevant questions. Each week, we’ll go over different methods, like building and using dictionaries, understanding sentiment in text, scaling texts on ideological and policy dimensions, and using machine learning to classify text. Lectures will include hands-on activities, letting students work directly with actual texts, and I strongly encourage students to bring their own data to class. The course aims to equip students with a variety of text analysis techniques that will be valuable for their future work as policy experts and computational social scientists.\nWhile the course covers an interdisciplinary topic, and many of the techniques we discuss have their origins in computer science or statistics, we will spend relatively little time on traditional Natural Language Processing techniques, such as machine translation, optical character recognition, and parts of speech tagging, etc. Although we will touch on Large Language Models (ChatGPT) at the end of the course, we will focus mostly on the practical use of these models through their APIs instead of building up and providing an in-depth understanding of their architecture.\nI assume students taking this class have taken, at minimum, an introductory class in statistics and have basic knowledge of probability, distributions, hypothesis testing, and linear models. The core language and software environment of this course is R. If you are not familiar with R, you will struggle with the assigned exercises. We will also provide some code in Python, but no prior knowledge here is assumed since this will be additional material."
  },
  {
    "objectID": "index.html#class-website",
    "href": "index.html#class-website",
    "title": "PPOL 6801: Text as Data - Computational Linguistics",
    "section": "Class Website",
    "text": "Class Website\nhttps://tiagoventura.github.io/PPOL_6801_2024/"
  },
  {
    "objectID": "index.html#instructor",
    "href": "index.html#instructor",
    "title": "PPOL 6801: Text as Data - Computational Linguistics",
    "section": "Instructor",
    "text": "Instructor\nProfessor Tiago Ventura\n\nPronouns: He/Him\nEmail: tv186@georgetown.edu\nOffice hours: Every Tuesday, 4pm - 6pm\nLocation: Old north, 312\n\n\n\n\n\n\n\nWhen should I go to your office hours?\n\n\n\n\n\n\nYou are all welcome to the office hours. You can come to the office hours to:\n\ndrink some coffee;\ntalk about soccer;\nAsk what I am doing research at;\nAsk any question about our class.\n\nAll are valid options! And no need to schedule time with me!"
  },
  {
    "objectID": "index.html#course-infra-structure",
    "href": "index.html#course-infra-structure",
    "title": "PPOL 6801: Text as Data - Computational Linguistics",
    "section": "Course Infra-structure",
    "text": "Course Infra-structure\nClass Website: This class website will be used throughout the course and should be checked on a regular basis for lecture materials and required readings.\nClass Slack Channel: The class also has a dedicated slack channel. The channel serves as an open forum to discuss, collaborate, pose problems/questions, and offer solutions. Students are encouraged to pose any questions they have there as this will provide the professor the means of answering the question so that all can see the response. If you’re unfamiliar with, please consult the following start-up tutorial https://get.slack.help/hc/en-us/articles/218080037-Getting-started-for-new-members. Please follow the invite link to be added to the Slack channel.\nCanvas: A Canvas site http://canvas.georgetown.edu will be used throughout the course and should be checked on a regular basis for announcements. Materials will be posted here, and not on canvas, or distributed in class or by e-mail. Support for Canvas is available at (202) 687-4949"
  },
  {
    "objectID": "index.html#credits",
    "href": "index.html#credits",
    "title": "PPOL 6801: Text as Data - Computational Linguistics",
    "section": "Credits",
    "text": "Credits\nTo build this course, I used materials from Arthur Spirling Text-as-Data Class at NYU, and lab materials from various TA’s for his course (Elisa Wirsching, Lucia Motolinia, Pedro L. Rodriguez, Kevin Munger, Patrick Chester, Leslie Huang), Pablo Barbera’s Computational Social Science Seminar seminar, Brandon Stewart, Alex Siegel, Chris Bail, Sebastian Vallejo, among others. Their lessons and inspiration are spread throughout all the materials of the course. Thanks!"
  },
  {
    "objectID": "weeks/week-01.html",
    "href": "weeks/week-01.html",
    "title": "Week 01",
    "section": "",
    "text": "Topics\n\nReview of syllabus\nClass organization\nGetting to know you\nIntroduction to computational text analysis.\n\n\n\nReadings\nRequired Readings\n\nGrimmer, Justin, Margaret E. Roberts, and Brandon M. Stewart. Text as data: A new framework for machine learning and the social sciences. Princeton University Press, 2022 - Chapter 2.\n\nOr one of the three below:\n\nPolitical Science Perspective: Wilkerson, J. and Casas, A. (2017). Large-Scale Computerized Text Analysis in Political Science: Opportunities and Challenges. Annual Review of Political Science, 20, 529:544.\nEconomics Perspective: Gentzkow, Matthew, Bryan Kelly, and Matt Taddy. “Text as data.” Journal of Economic Literature 57, no. 3 (2019): 535-574.\nSociology Perspective: https://www.sciencedirect.com/science/article/pii/S0049089X22000904\n\nCoding Materials\n\nNone\n\n\n\nSlides\n\nWeek 1 - Introduction"
  },
  {
    "objectID": "weeks/week-02.html",
    "href": "weeks/week-02.html",
    "title": "Week 02",
    "section": "",
    "text": "Topics\n\nHow to represent text as data?\nWhat is a Bag of Words?\nWhat are tokens?\nWhy should we care about tokens?\n\n\n\nReadings\nRequired Readings\n\nGrimmer, Justin, Margaret E. Roberts, and Brandon M. Stewart. Text as data: A new framework for machine learning and the social sciences. Princeton University Press, 2022 - Chapters 3-5\nApplied Papers:\nDenny, M. J., & Spirling, A. (2018). Text preprocessing for unsupervised learning: why it matters, when it misleads, and what to do about it. Political Analysis, 26(2): 168-189.\nBan, Pamela, Alexander Fouirnaies, Andrew B. Hall, and James M. Snyder. “How newspapers reveal political power.” Political Science Research and Methods 7, no. 4 (2019): 661-678.\nMichel, J.B., et al. 2011. “Quantitative Analysis of Culture Using Millions of Digitized Books.” Science 331 (6014): 176–82. https://doi.org/10.1126/science.1199644\n\nCoding Materials\n\n Week 2 - Text Data Processing \n\n\n\nSlides\n\nWeek 2 - From Text to Matrices: Representing Text as Data"
  },
  {
    "objectID": "weeks/week-13.html",
    "href": "weeks/week-13.html",
    "title": "Week 13",
    "section": "",
    "text": "Topics\n\n\nReadings\nRequired Readings\n\nWu, Patrick Y., Joshua A. Tucker, Jonathan Nagler, and Solomon Messing. “Large language models can be used to estimate the ideologies of politicians in a zero-shot learning setting.” arXiv preprint arXiv:2303.12057 (2023).\n\n- [Rathje, Steve, Dan-Mircea Mirea, Ilia Sucholutsky, Raja Marjieh, Claire Robertson, and Jay J. Van Bavel. GPT is an effective tool for multilingual psychological text analysis. (2023)](https://osf.io/preprints/psyarxiv/sekf5).\n\n- [Davidson, Thomas: Start Generating: Harnessing Generative Artificial Intelligence for Sociological Research, PrePrint](https://osf.io/preprints/socarxiv/u9nft)\n\n- [Bisbee, James, Joshua Clinton, Cassy Dorff, Brenton Kenkel, and Jennifer Larson. 2023. “Synthetic Replacements for Human Survey Data? the Perils of Large Language Models.” SocArXiv. May 4.](https://osf.io/preprints/socarxiv/5ecfa) \nCoding Materials\n\nTBD\n\n\n\nSlides\n\nTBD"
  },
  {
    "objectID": "weeks/week-04.html",
    "href": "weeks/week-04.html",
    "title": "Week 04",
    "section": "",
    "text": "Topics\n\nWhat are dictionaries?\nWhy/when are they useful?\nWhat are their limitations?\nCan we use models trained by others and for other purposes in our classification tasks?\n\n\n\nReadings\nRequired Readings\n\n[GMB] - Chapters 15-16\nApplied Papers:\n\nLori Young and Stuart Soroka 2012 “Affective News: The Automated Coding of Sentiment in Political Texts.” Political Communication, 29:2, 205-231.\nRathje, Steve, Jay J. Van Bavel, and Sander Van Der Linden. “Out-group animosity drives engagement on social media.” Proceedings of the National Academy of Sciences 118, no. 26 (2021): e2024292118.\nVentura, Tiago, Kevin Munger, Katherine McCabe, and Keng-Chi Chang. “Connective effervescence and streaming chat during political debates.” Journal of Quantitative Description: Digital Media 1 (2021).\n\n\nCoding Materials\n\n Week 4 - Dictionaries and Off-the-Shelf Classifiers\n Week 4 - Transformers Based Classifiers \n\n\n\nProblem set 1\n\nProblem Set 1\n\n\n\nSlides\n\nWeek 4 - Dictionaries and Off-the-Shelf Classifiers"
  },
  {
    "objectID": "weeks/week-08.html",
    "href": "weeks/week-08.html",
    "title": "Week 08",
    "section": "",
    "text": "Topics\n\nWhat are word-embeddings?\nWhen and how can we use them?\nWhat? Topic models again?\nIs this still a bag of words?\n\n\n\nReadings\nRequired Readings\n\n[GMB] - Chapter 8.\n[SLP] Chapter 6, “Vector Semantics and Embeddings.”\nJay Alanmar, The Illustrated Word2vec\nSpirling and Rodriguez, Word embedding: What works, what doesn’t, and how to tell the difference for applied research.\n\nCoding Materials\n\nTBD\n\n\n\nSlides\n\nTBD"
  },
  {
    "objectID": "weeks/week-09.html",
    "href": "weeks/week-09.html",
    "title": "Week 09",
    "section": "",
    "text": "Topics\n\nApplications of Word Embeddings to social science problems\n\n\n\nReadings\nRequired Readings\n-  Rodman, E., 2020. A Timely Intervention: Tracking the Changing Meanings of Political Concepts with Word Vectors. Political Analysis, 28(1), pp.87-111.\n\n- Gennaro, Gloria, and Elliott Ash. \"Emotion and reason in political language.\" The Economic Journal 132, no. 643 (2022): 1037-1059.\n\n- Rheault, Ludovic, and Christopher Cochrane. \"Word embeddings for the analysis of ideological placement in parliamentary corpora.\" Political Analysis 28, no. 1 (2020): 112-133.\n\n- Austin C. Kozlowski, Austin C., Matt Taddy, and James A. Evans. 2019. “The Geometry of Culture: Analyzing the Meanings of Class through Word Embeddings.” American Sociological Review 84, no. 5: 905–49. https://doi.org/10.1177/0003122419877135.\n\n- Garg, Nikhil, Londa Schiebinger, Dan Jurafsky and James Zou. 2018. “Word embeddings quantify 100 years of gender and ethnic stereotypes.” Proceedings of the National Academy of Sciences 115(16):E3635–E3644.\nCoding Materials\n\nTBD\n\n\n\nProblem Set 3\n\nTBD\n\n\n\nSlides\n\nTBD"
  },
  {
    "objectID": "weeks/week-10.html",
    "href": "weeks/week-10.html",
    "title": "Week 10",
    "section": "",
    "text": "Topics\n\nWhat are scaling models and what can they tell us?\nCan we represent politicians/users ideology using text?\n\n\n\nReadings\nRequired Readings\n\nLaver, Michael, Kenneth Benoit, and John Garry. 2003. “Extracting Policy Positions from Political Texts Using Words as Data”. American Political Science Review. 97, 2, 311-331\nSlapin, Jonathan and Sven-Oliver Prokschk. 2008. “A Scaling Model for Estimating Time-Series Party Positions from Texts.” American Journal of Political Science. 52, 3 705-722\n\n- Rheault, Ludovic, and Christopher Cochrane. \"Word embeddings for the analysis of ideological placement in parliamentary corpora.\" Political Analysis 28, no. 1 (2020): 112-133.\n\n- Aruguete, Natalia, Ernesto Calvo, and Tiago Ventura. \"News by popular demand: Ideological congruence, issue salience, and media reputation in news sharing.\" The International Journal of Press/Politics 28, no. 3 (2023): 558-579.\n\n- Izumi, Mauricio Y., and Danilo B. Medeiros. \"Government and opposition in legislative speechmaking: using text-as-data to estimate Brazilian political parties’ policy positions.\" Latin American Politics and Society 63, no. 1 (2021): 145-164.\nCoding Materials\n\nTBD\n\n\n\nSlides\n\nTBD"
  },
  {
    "objectID": "weeks/week-05.html",
    "href": "weeks/week-05.html",
    "title": "Week 05",
    "section": "",
    "text": "Topics\nWe will study the framework to train our own supervised models, and when to use them.\n\n\nReadings\nRequired Readings\n\n[GMB] - Chapters 17, 18, 19, and 20.\nApplied Papers:\n\nBarberá, Pablo, Amber E. Boydstun, Suzanna Linn, Ryan McMahon, and Jonathan Nagler. Automated text classification of news articles: A practical guide. Political Analysis 29, no. 1 (2021): 19-42.\nSiegel, Alexandra A., Evgenii Nikitin, Pablo Barberá, Joanna Sterling, Bethany Pullen, Richard Bonneau, Jonathan Nagler, and Joshua A. Tucker. Trumping hate on Twitter? Online hate speech in the 2016 US election campaign and its aftermath. Quarterly Journal of Political Science 16, no. 1 (2021): 71-104.\nTheocharis, Y., Barberá, P., Fazekas, Z., & Popa, S. A. (2020). The dynamics of political incivility on Twitter. Sage Open, 10(2), 2158244020919447.\nMitts, T., Phillips, G., & Walter, B. (2021). Studying the Impact of ISIS Propaganda Campaigns. Journal of Politics\n\n\nCoding Materials\n\n Week 5 - Supervised Learning: Training your own classifiers\n\n\n\nSlides\n\nWeek 5 - Supervised Learning"
  },
  {
    "objectID": "weeks/week-07.html",
    "href": "weeks/week-07.html",
    "title": "Week 07",
    "section": "",
    "text": "Topics\n-what if we do not have an outcome to predict? - can we cluster the text in groups? - what are topics? - how to decide between different topics?\n\n\nReadings\nRequired Readings\n\n[GMB] - Chapters 12-14.\nDavid M. Blei . 2012. “Probabilistic Topic Models.” http://www.cs.columbia.edu/~blei/papers/ Blei2012.pdf\nApplied Papers:\n\nMotolinia, Lucia. Electoral accountability and particularistic legislation: evidence from an electoral reform in Mexico. American Political Science Review 115, no. 1 (2021): 97-113.\nBarberá, P., Casas, A., Nagler, J., Egan, P. J., Bonneau, R., Jost, J. T., & Tucker, J. A. (2019). Who leads? Who follows? Measuring issue attention and agenda setting by legislators and the mass public using social media data. American Political Science Review, 113(4), 883-901.\nEshima, Shusei, Kosuke Imai, and Tomoya Sasaki. “Keyword‐Assisted Topic Models.” American Journal of Political Science (2020).\n\n\nCoding Materials\n\n Week 7 - Unsupervised Learning: Topic Models\n\n\n\nSlides\n\nWeek 7 - Unsupervised Learning"
  },
  {
    "objectID": "weeks/week-12.html",
    "href": "weeks/week-12.html",
    "title": "Week 12",
    "section": "",
    "text": "Topics\n\nWe will learn about the Transformers architecture, attention, and the encoder-coder infrastructure.\n\n\n\nReadings\nRequired Readings\n\n[SLP] - Chapter 10.\nJay Alammar. 2018. “The Illustrated Transformer.” https://jalammar.github.io/illustratedtransformer/\nVaswani, A., et al. (2017). Attention is all you need. Advances in neural information processing systems, 30;\nTimoneda and Vera, BERT, RoBERTa or DeBERTa? Comparing Performance Across Transformer Models in Political Science Text, Forthcoming Journal of Politics.\n\nCoding Materials\n\nTBD\n\n\n\nSlides\n\nTBD"
  },
  {
    "objectID": "weeks/week-03.html",
    "href": "weeks/week-03.html",
    "title": "Week 03",
    "section": "",
    "text": "Topics\n\nMore about the vector space model\nHow do we compare texts?\nHow do we evaluate similarity in text?\nHow do we evaluate complexity in text?\nWhy should we care about complexity in text?\n\n\n\nReadings\nRequired Readings\n\n[GMB] - Chapter 7\nApplied Papers:\n\nSpirling, Arthur. 2016. “Democratization and Linguistic Complexity”, Journal of Politics.\nBenoit, K., Munger, K. and Spirling, A. 2017. Measuring and Explaining Political Sophistication Through Textual Complexity\nLinder, Fridolin, Bruce Desmarais, Matthew Burgess, and Eugenia Giraudy. “Text as policy: Measuring policy similarity through bill text reuse.” Policy Studies Journal 48, no. 2 (2020): 546-574.\n\n\nCoding Materials\n\n Week 3 - Descriptive Inference \n\n\n\nSlides\n\nWeek 3 - Descriptive Inference"
  },
  {
    "objectID": "slides/week_02.html#outline",
    "href": "slides/week_02.html#outline",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Outline",
    "text": "Outline\n\n\nChallenges of working with text\nDefining a corpus and selecting documents\nUnit of analysis\nReducing complexity (Denny & Spirling’s article)\nBag-of-Word, Vector model representation and Document-Feature Matrix\nApplication (Ban et. al.’s paper)"
  },
  {
    "objectID": "slides/week_02.html#challenge-i-text-is-high-dimensional",
    "href": "slides/week_02.html#challenge-i-text-is-high-dimensional",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Challenge I: Text is High-Dimensional",
    "text": "Challenge I: Text is High-Dimensional\nFrom Gentzkow et al 2017:\n\nsample of documents, each \\(n_L\\) words long, drawn from vocabulary of \\(n_V\\) words.\nThe unique representation of each document has dimension \\(n_{V}^{n_L}\\) .\n\ne.g., a sample of 30-word (\\(n_L\\)) Twitter messages using only the one thousand most common words in the English language\nDimensionality = \\(1000^{30}\\)\nAs a matrix: \\(M^{1000}_{n_tweets}\\)"
  },
  {
    "objectID": "slides/week_02.html#challenge-ii-text-is-an-unstructure-data-source",
    "href": "slides/week_02.html#challenge-ii-text-is-an-unstructure-data-source",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Challenge II: Text is an unstructure data source",
    "text": "Challenge II: Text is an unstructure data source"
  },
  {
    "objectID": "slides/week_02.html#challenges-iii-outcomes-live-in-the-latent-space",
    "href": "slides/week_02.html#challenges-iii-outcomes-live-in-the-latent-space",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Challenges III: Outcomes live in the Latent Space",
    "text": "Challenges III: Outcomes live in the Latent Space\nIn most social science applications of text as data, we are trying to make an inference about a latent variable\n\nLatent variable: we cannot observe directly but try to identify with statistical and theoretical assumptions.\nExamples: ideology, sentiment, political stance, propensity of someone to turnout\n\n\nTraditional social science: mapping between observed and latent/theoretical concepts is easier.\n\nWe observe/measure country macroeconomic variables, collect survey responses, see how politicians vote.\nIn text, we only observe the words. Much harder to identify the latent concepts."
  },
  {
    "objectID": "slides/week_02.html#learning-goals",
    "href": "slides/week_02.html#learning-goals",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Learning goals",
    "text": "Learning goals\nToday:\n\nCover techniques to reduce complexity from text data using a set of pre-processing steps ~ Challenge I\nHow to represent text as numbers using the vector space model ~ Challenge II\nStarting next week we will deal more with inference and modeling latent parameters using text ~ Challenge III"
  },
  {
    "objectID": "slides/week_02.html#corpus-and-selecting-documents",
    "href": "slides/week_02.html#corpus-and-selecting-documents",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "1. Corpus and selecting documents",
    "text": "1. Corpus and selecting documents\n\nA corpus is (typically) a large set of texts or documents which we wish to analyze.\n\nif you can read them in an small amount of time, you should just do it, not TAD\n\nWhen selecting a corpus, we should consider how the corpus relates to our research question in two aspects:\n\nPopulation of interest: does the corpus allows us to make inferences about them?\nQuantity of interest: can we measure what we plan to?\nSampling Bias: documents are often sampled from a larger population. Are there concerns about sample selection bias?\n\nMost often we use these documents because they were available to us (custom made data). In these cases, considering the three questions above is even more important."
  },
  {
    "objectID": "slides/week_02.html#ventura-et.-al.-streaming-chats-2021",
    "href": "slides/week_02.html#ventura-et.-al.-streaming-chats-2021",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Ventura et. al., Streaming Chats, 2021",
    "text": "Ventura et. al., Streaming Chats, 2021\n\n\n\n\n\n\n\n\nKey components\n\n\nRQ: Measure quality of comments on streaming chat platforms during political debates\nPopulation of interest?\nQuantity of interest?\nSource of bias?"
  },
  {
    "objectID": "slides/week_02.html#unit-of-analysis",
    "href": "slides/week_02.html#unit-of-analysis",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "2. Unit of Analysis",
    "text": "2. Unit of Analysis\nAfter selecting your documents and converting them to a computer-friendly format, we must decide our unit of analysis\n\nentire document? sentence? paragraph? a larger group of documents?\n\n\nThree things to consider in making this decision:\n\nFeatures of your data and model fit\nYour research question\nIterative model\n\nswitching through different units of analysis has a low cost\nallows you to look at the data from a different angle\nprovide new insights to your research"
  },
  {
    "objectID": "slides/week_02.html#reducing-complexity",
    "href": "slides/week_02.html#reducing-complexity",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "3. Reducing complexity",
    "text": "3. Reducing complexity\nLanguage is extraordinarily complex, and involves great subtlety and nuanced interpretation.\n\nWe simplify documents so that we can analyze/compared them:\n\nmakes the modeling problem much more tractable.\ncomplexity makes not much difference in topic identification or simple prediction tasks (sentiment analysis, for example)\n\nthe degree to which one simplifees is dependent on the particular task at hand.\n\nDenny and Spirling (2019) ~ check sensitivity."
  },
  {
    "objectID": "slides/week_02.html#reducing-complexity-steps",
    "href": "slides/week_02.html#reducing-complexity-steps",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Reducing complexity: steps",
    "text": "Reducing complexity: steps\n\nTokenization: What does constitute a feature?\nRemove `superfulous’ material: HTML tags, punctuation, numbers, lower case and stop words\nMap words to equivalence forms: stemming and lemmatization\nDiscard less useful features for your task at hand: functional words, highly frequent or rare words\nDiscard word order: Bag-of-Words Assumption"
  },
  {
    "objectID": "slides/week_02.html#tokenization",
    "href": "slides/week_02.html#tokenization",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Tokenization",
    "text": "Tokenization\n\nA first step in any text analysis task is to break documents in meaningful units of analysis (tokens)\nTokens are often words for most tasks. A simple tokenizer uses white space marks to split documents in tokens.\nTokenizer may vary [across tasks]{.red}:\n\nTwiter specific tokenizer ~ keep hashtags, for example.\n\nMay also vary across languages, in which white space is not a good marker to split text into tokens\n\nchinese and japanese\n\nCertain tokens, even in english, make more sense together than separate (“White House”, “United States”). These are collocations\n\nstatistical testing for collocations ~ PMI(a, b) = log(p(a,b)/p(a)*p(b))"
  },
  {
    "objectID": "slides/week_02.html#stop-words",
    "href": "slides/week_02.html#stop-words",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Stop Words",
    "text": "Stop Words\n\nThere are certain words that serve as linguistic connectors (`function words’) which we can remove.\n\n( the, it, if, a, for, from, at, on, in, be )\n\nAdd noise to the document. Discard them, focus on signal, meaningful words.\nMost TAD packages have a pre-selected list of stopwords. You can add more given you substantive knowledge (more about this later)\nUsually not important for unsupervised and mostly supervised tasks, but might matter for authorship detection.\n\nFederalist Papers, example. Stop words give away writing styles."
  },
  {
    "objectID": "slides/week_02.html#equivalence-mapping",
    "href": "slides/week_02.html#equivalence-mapping",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Equivalence mapping",
    "text": "Equivalence mapping\nReduce dimensionality even further!\n\nDifferent forms of words (family, families, familial), or words which are similar in concept (bureaucratic, bureaucrat, bureaucratization) that refer to same basic token/concept.\nuse algorithms to map these variation to a equivalent form:\n\nstemming: chop the end of the words: family, families, familiar ~ famili\nlemmatization: condition on part of speech\n\nbetter (adj) ~ good\nleaves (noun) ~ leaf\nleaves (verb) ~ leave\n\n\nAll [TAD/NLP packages[{.red}] offer easy applications for these algorithms."
  },
  {
    "objectID": "slides/week_02.html#other-steps-functional-words-highly-frequent-or-rare-words",
    "href": "slides/week_02.html#other-steps-functional-words-highly-frequent-or-rare-words",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Other steps: functional words, highly frequent or rare words",
    "text": "Other steps: functional words, highly frequent or rare words\nSome other commons steps, which are highly dependent on your contextual knowledge, are:\n\ndiscard functional words: for example, when working with congressional speeches, remove representative, congress, session, etc...\nremove highly frequent words: words that appear in all documents carry very little meaning for most supervised and unsupervised tasks ~ no clustering and not discrimination.\nremove rare frequent words: same logic as above, no signal. Commong practice, words appear less 5% fo documents."
  },
  {
    "objectID": "slides/week_02.html#bag-of-words-assumption",
    "href": "slides/week_02.html#bag-of-words-assumption",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "4. Bag-of-Words Assumption",
    "text": "4. Bag-of-Words Assumption\nNow we have pre-processed our data. So we simplify it even further:\n\nBag-of-Words Assumption: the order in which words appear does not matter.\n\nIgnore order\nBut keep multiplicity, we still consider frequency of words\n\n\n\nHow could this possible work:\n\nit might note: you need validation\ncentral tendency in text: some words are enough to topic detection, classificaiton, measures of similarity, and distance, for example.\nhumans in the loop: expertise knowledge help you figure it out subtle relationships between words and outcomes"
  },
  {
    "objectID": "slides/week_02.html#can-we-preserve-the-word-order-another-pre-processing-decision",
    "href": "slides/week_02.html#can-we-preserve-the-word-order-another-pre-processing-decision",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Can we preserve the word order? (another pre-processing decision)",
    "text": "Can we preserve the word order? (another pre-processing decision)\nYes\n\nwe might retaining word order using n-grams.\n\nWhite House, Bill Gates, State Department, Middle East\nwe think some important subtlety of expression is lost: negation perhaps\n\nI want coffee, not tea might be interpreted very diferently without word order.\n\n\ncan use [n-grams], which are (sometimes contiguous) sequences of two (bigrams) or three (trigrams) tokens.\nThis makes computations considerably more complex. We can pick some n-grams to keep but not all:\n\n\\(PMI_{a,b} = log \\frac{p_{a,b}}{p_a \\cdot p_b}\\)\n\nif p(a,b)=0 ~ log (0) = -inf\nif p(a,b)=p(a)p(b) ~ log(1) = 0\nif p(a,b)&lt;p(a)p(b) ~ log(0&lt;x&lt;1) &lt; 0\nif p(a,b)&gt;p(a)p(b) ~ log(x&gt;1) &gt; 0"
  },
  {
    "objectID": "slides/week_02.html#complete-example",
    "href": "slides/week_02.html#complete-example",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Complete Example",
    "text": "Complete Example\n\n\n\nText\n\n\nWe use a new dataset containing nearly 50 million historical newspaper pages from 2,700 local US newspapers over the years 1877–1977. We define and discuss a measure of power we develop based on observed word frequencies, and we validate it through a series of analyses. Overall, we find that the relative coverage of political actors and of political offices is a strong indicator of political power for the cases we study\n\n\n\n\n\n\n\nAfter pre-processing\n\n\nuse new dataset contain near 50 million historical newspaper pag 2700 local u newspaper year 18771977 define discus measure power develop bas observ word frequenc validate ser analys overall find relat coverage political actor political offic strong indicator political power cas study"
  },
  {
    "objectID": "slides/week_02.html#denny-spirling-2018",
    "href": "slides/week_02.html#denny-spirling-2018",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Denny & Spirling, 2018",
    "text": "Denny & Spirling, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nStarting point: No rigorous way to compare results across different pre-processing steps. Adapting recommendations from supervised learning tasks.\n\nUnsupervised vs Supervised Learning?\nWhat is their solution? (no math needed!)\n\n\n\ncalculate the distance for every pair of documents, and rank the distances\ncompared to no pre processing, do the pair wise distances again, and get which document pair changed kth position, where k=1 for the pair that changed the most.\nBuild vector Vm_i^ with the position of the pairwise distance k affected in every other m combination. So Vm_1_1 contains the position of the changes in parwise distance on every other combination other than m=1 for the most changed document in m_1.\ntheir words: vmk = the rank difference for pair k between specification i and all others.\nAnother example with 3 documents, Vm1_1 = (1m2, 1m3), indicates that the document the changed the most in m1 is also the same in m2 and m3.\npretext score: mean over k (mean of V_m_k)\n\n\n\nToo much work? Substantive knowledge out of the table?"
  },
  {
    "objectID": "slides/week_02.html#vector-space-model",
    "href": "slides/week_02.html#vector-space-model",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "5. Vector Space Model",
    "text": "5. Vector Space Model\nTo represent documents as numbers, we will use the vector space model representation:\n\nA document \\(D_i\\) is represented as a collection of features \\(W\\) (words, tokens, n-grams..)\nEach feature \\(w_i\\) can be place in a real line, then a document \\(D_i\\) is a point in a \\(W\\) dimensional space\n\n\nImagine the sentence below: “If that is a joke, I love it. If not, can’t wait to unpack that with you later.”\n\nSorted Vocabulary =(a, can’t, i, if, is, it, joke, later, love, not, that, to, unpack, wait, with, you”)\nFeature Representation = (1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1)\nFeatures will typically be the n-gram (mostly unigram) frequencies of the tokens in the document, or some function of those frequencies\n\n\n\nNow each document is now a vector (vector space model)\n\nstacking these vectors will give you our workhose representation for text: Document Feature Matrix"
  },
  {
    "objectID": "slides/week_02.html#visualizing-vector-space-model",
    "href": "slides/week_02.html#visualizing-vector-space-model",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Visualizing Vector Space Model",
    "text": "Visualizing Vector Space Model\n\n\n\nDocuments\n\n\nDocument 1 = “yes yes yes no no no”\nDocument 2 = “yes yes yes yes yes yes”"
  },
  {
    "objectID": "slides/week_02.html#visualizing-vector-space-model-1",
    "href": "slides/week_02.html#visualizing-vector-space-model-1",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Visualizing Vector Space Model",
    "text": "Visualizing Vector Space Model\nIn the vector space, we can use geometry to build well-defined comparison measures between the documents (more about this next week)"
  },
  {
    "objectID": "slides/week_02.html#document-feature-matrix",
    "href": "slides/week_02.html#document-feature-matrix",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "6. Document-Feature Matrix",
    "text": "6. Document-Feature Matrix\n\n\n\n\n\n\n\n\n\n\n\n\nSource: Arthur Spirling TAD Class"
  },
  {
    "objectID": "slides/week_02.html#ban-et.-al.-2019-how-newspapers-reveal-political-power.",
    "href": "slides/week_02.html#ban-et.-al.-2019-how-newspapers-reveal-political-power.",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Ban et. al. 2019, How Newspapers Reveal Political Power.",
    "text": "Ban et. al. 2019, How Newspapers Reveal Political Power.\n\n\n\n\n\n\n\n\n \n\nPurely descriptive\nSimple measure just by counting words.\nTheorethically-driven: measure that capture a theorethically relevant concept.\n\n\n\\[\n\\small \\text{Coverage of Mayor}_{it} = \\frac{\\text{Mayor}_{it}}{\\text{Mayor}_{it} + \\text{City Manager}_{it} + \\text{City Council}_{it}}\n\\]\n\n\n\n\n\nText-as-Data"
  },
  {
    "objectID": "slides/week_07.html#survey-responses",
    "href": "slides/week_07.html#survey-responses",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Survey Responses",
    "text": "Survey Responses\nThank you so much for responding to the survey! Here are the most important feedback I receive:\n\nmore structured walk throughs of the code\nIf you could a few resources we can look at for mathematical depth for class topic. Really enjoyed the in-depth discussion on LDA\nI hope we can have a few discussion questions along with weekly readings\n[Including discussion of papers that are from non-polisci applications? For example, econ, sociology, psychology, etc.?{.midgray}\n\nStop Doing:\n\nlong alone time with code\nSometimes we spend a bit too long on the recap part"
  },
  {
    "objectID": "slides/week_07.html#plans-for-today",
    "href": "slides/week_07.html#plans-for-today",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Plans for Today:",
    "text": "Plans for Today:\n\nLive coding from last class on topic models\nWord Embeddings\n\nSemantics, Distributional Hypothesis, From Sparse to Dense Vectors\nWord2Vec Algorithm\n\nMathematical Model\nEstimate with Neural Networks\n\n\nNext week:\n\nStart with coding to estimate Word2Vec from Co-Occurence Matrices\nDiscuss applications to social science"
  },
  {
    "objectID": "slides/week_07.html#back-to-class-ii-vector-space-model",
    "href": "slides/week_07.html#back-to-class-ii-vector-space-model",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Back to class II: Vector Space Model",
    "text": "Back to class II: Vector Space Model\nIn the vector space model, we learned:\n\nA document \\(D_i\\) is represented as a collection of features \\(W\\) (words, tokens, n-grams..)\nEach feature \\(w_i\\) can be place in a real line, then a document \\(D_i\\) is a point in a \\(W\\) dimensional space.\n\nEmbedded in this model, there is the idea we represent words as a one-hot encoding.\n\n“cat”: [0,0, 0, 0, 0, 0, 1, 0, ….., V] , on a V dimensional vector\n“dog”: [0,0, 0, 0, 0, 0, 0, 1, …., V]\nreally sparse\nvectors are orthogonal\nno natural notion of similarity"
  },
  {
    "objectID": "slides/week_07.html#distributional-semantics",
    "href": "slides/week_07.html#distributional-semantics",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Distributional Semantics",
    "text": "Distributional Semantics\n\n“you shall know a word by the company it keeps.” J. R. Firth 1957\n\nDistributional semantics: words that are used in the same contexts tend to be similar in their meaning.\n\n\nHow can we use this insight to understand what words mean?\n\nMove from sparse representation to dense representation\nRepresent words as vectors of numbers with high number of dimensions\nEach feature on this vectors embeds some information from the word (gender? noun? sentiment? stance?)\nLearn this representation from the unlabeled data."
  },
  {
    "objectID": "slides/week_07.html#visually",
    "href": "slides/week_07.html#visually",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Visually",
    "text": "Visually\nOne-hot encoding:\n\ncat = \\(\\begin{bmatrix} 0,0, 0, 0, 0, 0, 1, 0, 0 \\end{bmatrix}\\)\ndog = \\(\\begin{bmatrix} 0,0, 0, 0, 0, 1, 0, 0, 0 \\end{bmatrix}\\)\n\nWord Embedding / Dense Representation:\n\ncat = \\(\\begin{bmatrix} 0.25, -0.75, 0.90, 0.12, -0.50, 0.33, 0.66, -0.88, 0.10, -0.45 \\end{bmatrix}\\)\ndog = \\(\\begin{bmatrix} 0.25, 1.75, 0.90, 0.12, -0.50, 0.33, 0.66, -0.88, 0.10, -0.45 \\end{bmatrix}\\)"
  },
  {
    "objectID": "slides/week_07.html#with-colors-and-real-word-vectors",
    "href": "slides/week_07.html#with-colors-and-real-word-vectors",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "With colors and real word vectors",
    "text": "With colors and real word vectors\n\nSource: Illustrated Word2Vec"
  },
  {
    "objectID": "slides/week_07.html#why-word-embeddings",
    "href": "slides/week_07.html#why-word-embeddings",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Why Word Embeddings?",
    "text": "Why Word Embeddings?\nEncoding similarity: vectors are not ortogonal anymore!\nAutomatic Generalization: learn about one word allow us to automatically learn about related words\nEncodes Meaning: by learning the context, I can learn what a word means.\nAs a consequence:\n\nWord Embeddings improves several NLP/Text-as-Data Tasks.\nForm the core idea of state-of-the-art models, such as LLMs."
  },
  {
    "objectID": "slides/week_07.html#estimating-word-embeddings",
    "href": "slides/week_07.html#estimating-word-embeddings",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Estimating Word Embeddings",
    "text": "Estimating Word Embeddings\nApproches:\n\n[Count-based methods]{.red: look at how often words co-occur with neighbors.\n\nuse this matrix, and some some factorization to retrieve vectors for the words\nGloVE\nfast, not computationally intensive, but not the best representation\nwe will see code doing this next week\n\nPredictive Methods: rely on the idea of self-supervision\n\nuse unlabeled data and use words to predict sequence\nthe famous word2vec.\n\nSkipgram: predicts context words\nContinuous Bag of Words: predict center word"
  },
  {
    "objectID": "slides/week_07.html#word2vec",
    "href": "slides/week_07.html#word2vec",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Word2Vec",
    "text": "Word2Vec\nWord2vec is a framework for learning word vectors (Mikolov et al. 2013)\nIdea:\n\nWe have a large corpus (“body”) of text: a long list of words\nEvery word in a fixed vocabulary is represented by a vector\nGo through each position t in the text, which has a center word c and context (“outside”) words o\nUse the similarity of the word vectors for c and o to calculate the probability of o given c (or vice versa)\nKeep adjusting the word vectors to maximize this probability\n\nNeural Network + Gradient Descent"
  },
  {
    "objectID": "slides/week_07.html#skigram-example-self-supervision",
    "href": "slides/week_07.html#skigram-example-self-supervision",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Skigram Example: Self-Supervision",
    "text": "Skigram Example: Self-Supervision\n\nSource: CS224N"
  },
  {
    "objectID": "slides/week_07.html#skigram-example-self-supervision-1",
    "href": "slides/week_07.html#skigram-example-self-supervision-1",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Skigram Example: Self-Supervision",
    "text": "Skigram Example: Self-Supervision\n\nSource: CS224N"
  },
  {
    "objectID": "slides/week_07.html#encoding-similarity",
    "href": "slides/week_07.html#encoding-similarity",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Encoding Similarity",
    "text": "Encoding Similarity\nTo estimate the model, we first need to formalize the probability function we want to estimate.\nThis is as simple as a logistic regression:\n\nIn logistic regression: probability of a event occur given data X and parameters:\n\n\\(\\beta\\). \\(P(y=1|X, \\beta ) = X*\\beta\\)\n\\(X*\\beta\\) is not a proper probability function, so we make it to be by using a logit transformation\n\\(P(y=1|X, \\beta ) = \\frac{exp(XB)}{1 + exp(XB)}\\)\n\n\\(P(w_t|w_{t-1})\\) can be parametrized as how similar these words are:\n\n\\(P(w_t|w_{t-1}) = u_c \\cdot u_t\\)\n\n\\(u_c \\cdot u_t\\): dot product between vectors, measures similarity\n\\(u_c\\): center vector\n\\(u_t\\): target vectors\n\n\\(u_c \\cdot u_t\\) is also not a proper probability distribution ~ use logit/sigmoid/softmax function\n\n\\(P(w_t|w_{t-1}) = \\frac{exp(u_c \\cdot u_t)}{{\\sum_{w}^V exp(u_c*u_w)}}\\)"
  },
  {
    "objectID": "slides/week_07.html#letsee-this-again",
    "href": "slides/week_07.html#letsee-this-again",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Let’see this again:",
    "text": "Let’see this again:\n\\[P(w_t|w_{t-1}) = \\frac{exp(u_c \\cdot u_t)}{{\\sum_{w}^V exp(u_c*u_w)}}\\]\n\nDotproduct compares similarity between vectors\nnumerator: center vs target\nexponentiation makes everything positive\nNormalize over entire vocabulary to give probability distribution\nThis is an example of a softmax function\n\nmax: assign high values to be 1\nsoft: still assigns some probability to smaller values\ngeneralization of the logit ~ multinomial logistic function."
  },
  {
    "objectID": "slides/week_07.html#word2vec-objective-function",
    "href": "slides/week_07.html#word2vec-objective-function",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Word2Vec: Objective Function",
    "text": "Word2Vec: Objective Function\nFor each position \\(t\\), predict context words within a window of fixed size \\(m\\), given center word \\(w\\).\nLikelihood Function\n\\[ L(\\theta) = \\prod_{t=1}^{T} \\prod_{\\substack{-m&lt;= j&lt;=m \\\\ j \\neq 0}}^{m} P(w_{t+j} | w_t; \\theta) \\]\n\nAssuming independence, this means you multiplying the probability of every target for every center word in your dictionary.\nThis likelihood function will change if you do skipgram with negative sampling (See SLP chapter 6)\n\nObjective Function: Negative log likelihood\n\\[J(\\theta) = - \\frac{1}{T}log(L(\\theta))\\]\n\nbetter to take the gradient with log and sums\nthe average increases the numerical stability of the gradient."
  },
  {
    "objectID": "slides/week_07.html#neural-networks-brief-overview",
    "href": "slides/week_07.html#neural-networks-brief-overview",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Neural Networks: Brief overview",
    "text": "Neural Networks: Brief overview"
  },
  {
    "objectID": "slides/week_07.html#skipgram-architecture",
    "href": "slides/week_07.html#skipgram-architecture",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Skipgram Architecture",
    "text": "Skipgram Architecture"
  },
  {
    "objectID": "slides/week_07.html#check-your-matrices",
    "href": "slides/week_07.html#check-your-matrices",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Check your matrices",
    "text": "Check your matrices\n\n\n\n\n\n\n\n\n  \nPractice with a vocabulary of size 5, a embedding with 3 dimensions, and I want to predict the next word.\n\nStep 1: v_1^5 * W_5^3\nStep 2: w_1^3 * C_3^5\nStep 3: Softmax entire vector"
  },
  {
    "objectID": "slides/week_07.html#word-embeddings-matrices",
    "href": "slides/week_07.html#word-embeddings-matrices",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Word Embeddings Matrices",
    "text": "Word Embeddings Matrices"
  },
  {
    "objectID": "slides/week_07.html#applications",
    "href": "slides/week_07.html#applications",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Applications:",
    "text": "Applications:\nOnce we’ve optimized, we can extract the word specific vectors from W as embedding vectors. These real valued vectors can be used for analogies and related tasks\n\nWe will see several applications next week. Most important:\n\nAlternative to bag-of-words feature representation in supervised learning tasks\nSupport for other automated text analysis tasks: expand dictionaries\nUnderstanding word meaning: variation over time, bias, variation by groups\nas a scaling method (in two weeks)"
  },
  {
    "objectID": "slides/week_07.html#training-embeddings",
    "href": "slides/week_07.html#training-embeddings",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Training Embeddings",
    "text": "Training Embeddings\nEmbeddings need quite a lot of text to train: e.g. want to disambiguate meanings from contexts. You can download pre-trained, or get the code and train locally\n\nWord2Vec is trained on the Google News dataset (∼ 100B words, 2013)\nGloVe are trained on different things: Wikipedia\n\n\n\nGigaword (6B words), Common Crawl, Twitter\n\n\n\nfastext from facebook"
  },
  {
    "objectID": "slides/week_07.html#decisions-on-embeddings-rodriguez-and-spirling-2022",
    "href": "slides/week_07.html#decisions-on-embeddings-rodriguez-and-spirling-2022",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Decisions on embeddings, Rodriguez and Spirling, 2022",
    "text": "Decisions on embeddings, Rodriguez and Spirling, 2022\nWhen using/training embeddings, we face four key decisions:\n\nWindow size\nNumber of dimensions for the embedding matrix\nPre-trained versus locally fit variants\nWhich algorithm to use?"
  },
  {
    "objectID": "slides/week_07.html#findings",
    "href": "slides/week_07.html#findings",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Findings",
    "text": "Findings\n\n\npopular, easily available pretrained embeddings perform at a level close to—or surpassing—both human coders andmore complicated locally fit models.\nGloVe pretrained word embeddings achieve on average—for the set of political queries—80% of human performance and are generally preferred to locally trained embeddings\nLarger window size and embeddings are often preferred."
  },
  {
    "objectID": "slides/week_04.html#housekeeping",
    "href": "slides/week_04.html#housekeeping",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Housekeeping",
    "text": "Housekeeping\nYour first problem set will be assigned today! Some important information:\n\n\nYou will receive and submit your assignment using Github!\n\nGithub Clasroom: Creates automatically a repo for the assignment. You and I are owner of the repo.\n\nDeadline: EOD next Wednesday, February 14th.\nPlease use an .RMD/.QMD file to submit your assignment. If you prefer to solve using jupyter, let me know!\nAny questions?"
  },
  {
    "objectID": "slides/week_04.html#where-are-we",
    "href": "slides/week_04.html#where-are-we",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Where are we?",
    "text": "Where are we?\nAfter learning how to process and represent text as numbers, we started digging in on how to use text on a research pipeline.\n\n\nDescriptive inference:\n\n\nCounting words (Ban’s Paper)\nComparing document similarity using vector space model (text re-use)\nMeasures of lexical diversity and readability"
  },
  {
    "objectID": "slides/week_04.html#plans-for-today",
    "href": "slides/week_04.html#plans-for-today",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Plans for today",
    "text": "Plans for today\nFor the next two weeks, we will talk about Measurement\n\nMeasurement: Map/Measure concepts from theory to data.\n\n\n\nDocuments pertaining to certain classes and how we can use statistical assumptions to measure these classes\n\n\n\n\n\nDictionary Methods\n\nDiscuss some well-known dictionaries\n\nOff-the-Shelf Classifiers\n\nPerspective API\nHugging Face (only see as a off-the-shelf machines, LMMs later in this course)\n\nNext week: training our own machine learning models"
  },
  {
    "objectID": "slides/week_04.html#connecting-machine-learning-with-tad",
    "href": "slides/week_04.html#connecting-machine-learning-with-tad",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Connecting Machine Learning with TAD",
    "text": "Connecting Machine Learning with TAD\nIn the Machine Learning tradition, we are introduced to two core family of models:\n\n\nUnsupervised Models: learning (hidden or latent) structure in unlabeled data.\n\nTopic Models to cluster documents and words\n\n\n\n\n\nSupervised Models: learning relationship between inputs and a labeled set of outputs.\n\nSentiment Analysis, classify if a tweet contains misinformation, etc..\n\n\n\n\n\nIn TAD, we mostly use unsupervised techniques for discovery and supervised for measurement of concepts."
  },
  {
    "objectID": "slides/week_04.html#supervised-learning-pipeline-for-tad",
    "href": "slides/week_04.html#supervised-learning-pipeline-for-tad",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Supervised Learning Pipeline for TAD",
    "text": "Supervised Learning Pipeline for TAD\n\n\nStep 1: label some examples of the concept of we want to measure\n\nsome tweets are positive, some are neutral and some are negative\n\nStep 2: train a statistical model on these set of label data using the document-feature matrix as input\n\nchoose a model (transformation function) that gives higher out-of-sample accuracy\n\nStep 3: use the classifier - some f(x) - to predict unseen documents.\nStep 4: use the measure + metadata|exogenous shocks to learn something new about the world.\n\nThis is where social science happens!"
  },
  {
    "objectID": "slides/week_04.html#back-to-the-future-exercise",
    "href": "slides/week_04.html#back-to-the-future-exercise",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Back to the Future Exercise",
    "text": "Back to the Future Exercise\n\n\n\n\n\nvia GIPHY\n\n\nAssume you got the delorean to travel back twenty years ago, you want to run a simple sentiment analysis in a corpus of news articles.\n\nWhich challenges would you face?\nHow could you solve it?\nPlease consider all four steps described before"
  },
  {
    "objectID": "slides/week_04.html#overview-of-dictionaries",
    "href": "slides/week_04.html#overview-of-dictionaries",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Overview of Dictionaries",
    "text": "Overview of Dictionaries\n\n\nUse a set of pre-defined words that allow us to classify documents automatically, quickly and accurately.\nInstead of optimizing a transformation function using statistical assumption and seen data, in dictionaries we have a pre-assumed recipe for the transformation function.\nA dictionary contains:\n\na list of words that corresponds to each category\n\npositive and negative for sentiment\nSexism, homophobia, xenophobia, racism for hate speech\n\n\nWeights given to each word ~ same for all words or some continuous variation."
  },
  {
    "objectID": "slides/week_04.html#more-specifically",
    "href": "slides/week_04.html#more-specifically",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "More specifically…",
    "text": "More specifically…\nWe have a set of key words with weights,\n\ne.g. for sentiment analysis: horrible is scored as \\(-1\\) and beautiful as \\(+1\\)\nthe relative rate of occurrence of these terms tells us about the overall tone or category that the document should be placed in.\n\n\nFor document \\(i\\) and words \\(m=1,\\ldots, M\\) in the dictionary,\n\\[\\text{tone of document $i$}= \\sum^M_{m=1} \\frac{s_m w_{im}}{N_i}\\]\nWhere:\n\n\\(s_m\\) is the score of word \\(m\\)\n\\(w_{im}\\) is the number of occurrences of the \\(m_{th}\\) dictionary word in the document \\(i\\)\n\\(N_i\\) is the total number of all dictionary words in the document"
  },
  {
    "objectID": "slides/week_04.html#why-dictionaries",
    "href": "slides/week_04.html#why-dictionaries",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Why Dictionaries?",
    "text": "Why Dictionaries?\n\n\nLow cost and computationally efficient ~ if using a dictionary developed and validated by others\nA hybrid procedure between qualitative and quantitative classification at the fully automated end of the text analysis spectrum\nDictionary construction involves a lot of contextual interpretation and qualitative judgment\nTransparency: no black-box model behind the classification task"
  },
  {
    "objectID": "slides/week_04.html#general-inquirer-stone-et-al-1966",
    "href": "slides/week_04.html#general-inquirer-stone-et-al-1966",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "General Inquirer (Stone et al 1966)",
    "text": "General Inquirer (Stone et al 1966)\n\n\nIt combines several dictionaries to make total of 182 categories:\n\nthe “Harvard IV-4” dictionary: psychology, themes, topics\nthe “Lasswell” dictionary, five categories based on the social cognition work of Semin and Fiedler\n\n“self references”, containing mostly pronouns;\n“negatives”, the largest category with 2291 entries"
  },
  {
    "objectID": "slides/week_04.html#linquistic-inquiry-and-word-count",
    "href": "slides/week_04.html#linquistic-inquiry-and-word-count",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Linquistic Inquiry and Word Count",
    "text": "Linquistic Inquiry and Word Count\nCreated by Pennebaker et al — see http://www.liwc.net\n\nLarge dictionary with around 4,500 words and words steams\n90 categories\nCategories are organized hierarchically\n\nAll anger words, by definition, will be categorized as negative emotion and overall emotion words.\n\nWords are in one or more categories\n\nthe word cried is part of five word categories: sadness, negative emotion, overall affect, verb, and past tense verb.\n\nYou can buy it here: http://www.liwc.net/descriptiontable1.php"
  },
  {
    "objectID": "slides/week_04.html#heavily-used-in-academia",
    "href": "slides/week_04.html#heavily-used-in-academia",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Heavily used in academia!",
    "text": "Heavily used in academia!\n\n\n\n\n\n\n\nPennebaker et al, 2009"
  },
  {
    "objectID": "slides/week_04.html#vader-an-open-source-alternative-to-liwc",
    "href": "slides/week_04.html#vader-an-open-source-alternative-to-liwc",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "VADER: an open-source alternative to LIWC",
    "text": "VADER: an open-source alternative to LIWC\nValence Aware Dictionary and sEntiment Reasoner:\n\nTuned for social media text\nCapture polarity and intensity\n\nSentiment Lexicon: This is a list of known words and their associated sentiment scores.\nSentiment Intensity Scores: Each word in the lexicon is assigned a score that ranges from -4 (extremely negative) to +4 (extremely positive).\nFive Heuristic-based rules: exclamation points, caps lock, intensifiers, negation, tri-grams\n\nPython and R libraries: https://github.com/cjhutto/vaderSentiment\nArticle: https://ojs.aaai.org/index.php/ICWSM/article/view/14550/14399"
  },
  {
    "objectID": "slides/week_04.html#young-sarokas-lexicoder-sentiment-dictionary",
    "href": "slides/week_04.html#young-sarokas-lexicoder-sentiment-dictionary",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Young & Saroka’s Lexicoder Sentiment Dictionary",
    "text": "Young & Saroka’s Lexicoder Sentiment Dictionary\n\nCreate dictionary specifically for political communication\nCombines:\n\nGeneral Inquirer;\nRoget’s Thesaurus and\nRegressive Imagery Dictionary\n\nEach words pertains to a single class\nPlus\n\nHand coding\nKeyword in context dos disambiguation"
  },
  {
    "objectID": "slides/week_04.html#performance",
    "href": "slides/week_04.html#performance",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Performance",
    "text": "Performance\n\n\n\n\n\n\n\nLSD results assign 74% to the positive category and just 12% to the negative category. Of the 495 articles that are categorized as negative by at least two coders, LSD results assign 53% to the negative category and 32% to the positive category ~ 69% of accuracy"
  },
  {
    "objectID": "slides/week_04.html#laver-and-garry-2000",
    "href": "slides/week_04.html#laver-and-garry-2000",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Laver and Garry, 2000",
    "text": "Laver and Garry, 2000\nA hierarchical set of categories to distinguish policy domains and policy positions on party manifestos\n\nFive Domains:\n\neconomy\npolitical system\nsocial system\nexternal relations\n\nLookes for word occurrences within “word strings with an average length of ten words”\nArticle: Estimating Policy Positions from Political Texts"
  },
  {
    "objectID": "slides/week_04.html#laver-and-garry-2000-1",
    "href": "slides/week_04.html#laver-and-garry-2000-1",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Laver and Garry, 2000",
    "text": "Laver and Garry, 2000"
  },
  {
    "objectID": "slides/week_04.html#rathje-et.-al-2020-pnas-out-group-animosity",
    "href": "slides/week_04.html#rathje-et.-al-2020-pnas-out-group-animosity",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Rathje et. al 2020, PNAS, Out-group animosity",
    "text": "Rathje et. al 2020, PNAS, Out-group animosity"
  },
  {
    "objectID": "slides/week_04.html#rathje-et.-al-2020-pnas-out-group-animosity-1",
    "href": "slides/week_04.html#rathje-et.-al-2020-pnas-out-group-animosity-1",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Rathje et. al 2020, PNAS, Out-group animosity",
    "text": "Rathje et. al 2020, PNAS, Out-group animosity\n\nWe used the R package quanteda to analyze Twitter and Facebook text. During text preprocessing, we removed punctuation, URLs, and numbers. To classify whether a specific post was referring to a liberal or a conservative, we adapted previously used dictionaries that referred to words associated with liberals or conservatives. Specifically, these dictionaries included 1) a list of the top 100 most famous Democratic and Republican politicians according to YouGov, along with their Twitter handles (or Facebook page names for the Facebook datasets) (e.g., “Trump,” “Pete Buttigieg,” “@realDonaldTrump”); 2) a list of the current Democratic and Republican (but not independent) US Congressional members (532 total) along with their Twitter and Facebook names (e.g., “Amy Klobuchar,” “Tom Cotton”); and 3) a list of about 10 terms associated with Democratic (e.g., “liberal,” “democrat,” or “leftist”) or Republican identity (e.g., “conservative,” “republican,” or “ring-wing”)."
  },
  {
    "objectID": "slides/week_04.html#rathje-et.-al-2020-pnas-out-group-animosity-2",
    "href": "slides/week_04.html#rathje-et.-al-2020-pnas-out-group-animosity-2",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Rathje et. al 2020, PNAS, Out-group animosity",
    "text": "Rathje et. al 2020, PNAS, Out-group animosity\n\nWe then assigned each tweet a count for words that matched our Republican and Democrat dictionaries (for instance, if a tweet mentioned two words in the “Republican” dictionary, it would receive a score of “2” in that category). We also used previously validated dictionaries that counted the number of positive and negative affect words per post and the number of moral-emotional words per post (LIWC)."
  },
  {
    "objectID": "slides/week_04.html#advantages",
    "href": "slides/week_04.html#advantages",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Advantages",
    "text": "Advantages\nWe already discussed some of the advantages:\n\nlow-cost when working with open sourced dictionaries\n\nrelatively easy to build/expand on new dictionaries\n\nbridge qualitative and quantitative\neasy to validate\n\ndictionaries are transparent and reliable.\n\ntransfer well across languages."
  },
  {
    "objectID": "slides/week_04.html#disadvantage-context-specific",
    "href": "slides/week_04.html#disadvantage-context-specific",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Disadvantage: Context specific",
    "text": "Disadvantage: Context specific\n\n\n\n\n\n\n\nSource: Gonzalez-Bailon et al"
  },
  {
    "objectID": "slides/week_04.html#disadvantage-performance",
    "href": "slides/week_04.html#disadvantage-performance",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Disadvantage: Performance",
    "text": "Disadvantage: Performance\n\n\n\n\n\n\n\nMuddiman et al, 2019 - Reclaiming our Expertise"
  },
  {
    "objectID": "slides/week_04.html#off-the-shelf-models-ventura-et.-al.-2021.",
    "href": "slides/week_04.html#off-the-shelf-models-ventura-et.-al.-2021.",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Off-the-shelf models: Ventura et. al. 2021.",
    "text": "Off-the-shelf models: Ventura et. al. 2021.\n\n\n\n\n\n\n\nSee article: Ventura et al. 2021, Connective Effervescence and Streaming Chat"
  },
  {
    "objectID": "slides/week_04.html#off-the-shelf-deep-learning-models",
    "href": "slides/week_04.html#off-the-shelf-deep-learning-models",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Off-the-shelf Deep Learning Models",
    "text": "Off-the-shelf Deep Learning Models\n\nDefinition: Pre-trained models designed for general-purpose classification tasks\n\nIn general those are models built on TONS of data and optimized for a particular task\n\nKey Features:\n\nReady to use\nLow to zero cost\nDeep ML architectures ~ High accuracy\nCan be re-trained for your specific task"
  },
  {
    "objectID": "slides/week_04.html#transformers",
    "href": "slides/week_04.html#transformers",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Transformers",
    "text": "Transformers"
  },
  {
    "objectID": "slides/week_01_intro.html#outline",
    "href": "slides/week_01_intro.html#outline",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Outline",
    "text": "Outline\n\n\nIntroduction (me)\nMotivation for Computational Linguistics\n\nDigital information age\nPrinciples of Computational Linguistics.\nWhat this course is not.\nExamples of models and applications for this course\n\nIntroductions (you)\nClass Logistics ( + 10 min for you to read through the syllabus)\nQ&A\nAcquiring text in the web (Jupyter notebooks for scrapping)"
  },
  {
    "objectID": "slides/week_01_intro.html#introduction",
    "href": "slides/week_01_intro.html#introduction",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Introduction",
    "text": "Introduction\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProfessor Tiago Ventura (he/him)\n\nAssistant Professor at McCourt School.\nPolitical Science Ph.D.\nPostdoc at Center for Social Media and Politics - NYU.\nResearcher at Twitter.\n\nSome Projects I am involved\n\nGlobal Social Media Deactivation.\nEffects of WhatsApp on Elections in the Global South.\nAI and Misinformation in 2024 elections.\nPanels of voter files and twitter users.\n\nOutside of work, I enjoy watching soccer, reading sci-fi and running"
  },
  {
    "objectID": "slides/week_01_intro.html#rise-of-the-digital-information-age",
    "href": "slides/week_01_intro.html#rise-of-the-digital-information-age",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Rise of the digital information age",
    "text": "Rise of the digital information age"
  },
  {
    "objectID": "slides/week_01_intro.html#official-documents-congressional-speeches-bills-press-releases-transcripts-from-all-over-the-world",
    "href": "slides/week_01_intro.html#official-documents-congressional-speeches-bills-press-releases-transcripts-from-all-over-the-world",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Official Documents: Congressional Speeches, Bills, Press Releases, Transcripts, from all over the world!!",
    "text": "Official Documents: Congressional Speeches, Bills, Press Releases, Transcripts, from all over the world!!"
  },
  {
    "objectID": "slides/week_01_intro.html#social-media",
    "href": "slides/week_01_intro.html#social-media",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Social Media",
    "text": "Social Media"
  },
  {
    "objectID": "slides/week_01_intro.html#the-internet-news-comments-blogs-etc",
    "href": "slides/week_01_intro.html#the-internet-news-comments-blogs-etc",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "The internet: News, Comments, Blogs, etc…",
    "text": "The internet: News, Comments, Blogs, etc…"
  },
  {
    "objectID": "slides/week_01_intro.html#what-is-this-class-about",
    "href": "slides/week_01_intro.html#what-is-this-class-about",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "What is this class about?",
    "text": "What is this class about?\n\nFor many years, social scientists use text in their analysis\nMostly through in-depth reading of documents.\nClose Reading. Humans are great at this!\nDigital Revolution:\n\nProduction of text increased\nThe capacity to analyze them at scale as well.\n\nThis class covers methods (and many applications) of using text as data to answer social science problems and test social science theories\nComputational Linguistics ~ Distant Reading. Computers are better at understanding patterns, classify and describe content across millions of documents."
  },
  {
    "objectID": "slides/week_01_intro.html#principles-of-text-analysis-gmb-textbook",
    "href": "slides/week_01_intro.html#principles-of-text-analysis-gmb-textbook",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Principles of Text Analysis (GMB Textbook)",
    "text": "Principles of Text Analysis (GMB Textbook)\n\nPrinciple 1: Social science theories and substantive knowledge are essential for research design\nPrinciple 2: Text analysis does not replace humans—it augments them.\nPrinciple 3: Building, refining, and testing social science theories requires iteration and cumulation.\nPrinciple 4: Text analysis methods distill generalizations from language. (all models are wrong!)\nPrinciple 5: The best method depends on the task. (Qualitative knowledge)\nPrinciple 6: Validations are essential and depend on the theory and the task"
  },
  {
    "objectID": "slides/week_01_intro.html#challenges-i-text-is-an-unstructure-data-source",
    "href": "slides/week_01_intro.html#challenges-i-text-is-an-unstructure-data-source",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Challenges I: Text is an unstructure data source",
    "text": "Challenges I: Text is an unstructure data source"
  },
  {
    "objectID": "slides/week_01_intro.html#challenge-ii-text-is-high-dimensionality",
    "href": "slides/week_01_intro.html#challenge-ii-text-is-high-dimensionality",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Challenge II: Text is High dimensionality",
    "text": "Challenge II: Text is High dimensionality\nFrom Gentzkow et al 2017:\n\nsample of documents, each \\(n_L\\) words long, drawn from vocabulary of \\(n_V\\) words.\nThe unique representation of each document has dimension \\(n_{V}^{n_L}\\) .\n\ne.g., a sample of 30-word (\\(n_L\\)) Twitter messages using only the one thousand most common words in the English language\nDimensionality = \\(1000^{30}\\)\nAs a matrix: \\(M^{1000}_{n_tweets}\\)\n\nMost of what you learned in statistics so far does not equip you to deal with this curse of dimensionality."
  },
  {
    "objectID": "slides/week_01_intro.html#text-as-data-workflow",
    "href": "slides/week_01_intro.html#text-as-data-workflow",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Text-as-Data Workflow",
    "text": "Text-as-Data Workflow\n\nAcquire textual data:\n\nExisting corpora; scraped data; digitized text"
  },
  {
    "objectID": "slides/week_01_intro.html#text-as-data-workflow-1",
    "href": "slides/week_01_intro.html#text-as-data-workflow-1",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Text-as-Data Workflow",
    "text": "Text-as-Data Workflow\n\nMap Documents to a numerical representation M\n\nBag-of-words (sparse vectors)\nEmbeddings (dense vectors)\nReduce noise, capture signal"
  },
  {
    "objectID": "slides/week_01_intro.html#text-as-data-workflow-2",
    "href": "slides/week_01_intro.html#text-as-data-workflow-2",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Text-as-Data Workflow",
    "text": "Text-as-Data Workflow\n\nMap M to predicted values \\(V^{*}\\) of unknown outcomes V\n\nDescriptive Analysis\nClassify documents into unknown categories\n\nTopic models\n\nClassify documents into known categories\n\nDictionary methods\nSupervised machine learning\nTransfer-Learning - use models trained in text for other purposes\n\nScale documents on latent dimension:"
  },
  {
    "objectID": "slides/week_01_intro.html#text-as-data-workflow-3",
    "href": "slides/week_01_intro.html#text-as-data-workflow-3",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Text-as-Data Workflow",
    "text": "Text-as-Data Workflow\n\nUse \\(V^{*}\\) in subsequent analysis with other data sources\n\nThis is where social science happens!"
  },
  {
    "objectID": "slides/week_01_intro.html#assume-you-already-did-it",
    "href": "slides/week_01_intro.html#assume-you-already-did-it",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Assume you already did it!",
    "text": "Assume you already did it!\n\nAcquire textual data: Existing corpora; scraped data; digitized text"
  },
  {
    "objectID": "slides/week_01_intro.html#overview-of-tad-methods",
    "href": "slides/week_01_intro.html#overview-of-tad-methods",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Overview of TAD Methods",
    "text": "Overview of TAD Methods\n\nDescriptive inference: how to convert text to matrices, vector space model, bag-of-words, dissimilarity measures, diversity, complexity, style.\nSupervised techniques: dictionaries, classication, scaling, machine learning approaches.\nUnsupervised techniques: clustering, topic models, embeddings.\nSpecial topics: Word embeddings and Large Language Models."
  },
  {
    "objectID": "slides/week_01_intro.html#some-cool-applications",
    "href": "slides/week_01_intro.html#some-cool-applications",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Some cool applications",
    "text": "Some cool applications\n\nMeasure text-reuse across thousands of bills from U.S. state legislatures\nEstimate levels of toxicity of comments from stremming chats platforms during political debates\nMeasure how out-group negative makes things go viral on social media\nMeasure the policy target of bills proposals in Mexico\nEstimate ideological positions using who a user follows on Twitter, what a user share on social media, political manifestos, or just asking ChatGPT to pair-wise compare politicians"
  },
  {
    "objectID": "slides/week_01_intro.html#what-this-class-in-not-about-it",
    "href": "slides/week_01_intro.html#what-this-class-in-not-about-it",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "What this class in not about it…",
    "text": "What this class in not about it…\n\nData acquisition: no scrapping in class. Assume you have learned already.\nRegular expressions and basic text manipulation.\nCS Stuff: machine translation, OCR, POS, entity recognition.\n\nMost NLP/CS will focus on developing new algorithms, information retrievel and purely better measurements.\nin a productive dialogue with NLP, we will focus on using text for social science research\n\ntheoretically driven discovery and measurement\nintegration with social science problems + tabular data."
  },
  {
    "objectID": "slides/week_01_intro.html#your-turn",
    "href": "slides/week_01_intro.html#your-turn",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Your turn!",
    "text": "Your turn!\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\nName & pronouns\nWhy are you taking this course?\nYour experience (if any) working with text\nThe most interesting thing you learned in the DSPP so far"
  },
  {
    "objectID": "slides/week_01_intro.html#read-the-syllabus",
    "href": "slides/week_01_intro.html#read-the-syllabus",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Read the syllabus!",
    "text": "Read the syllabus!\n\n\n\n−+\n10:00"
  },
  {
    "objectID": "slides/week_01_intro.html#class-requirements",
    "href": "slides/week_01_intro.html#class-requirements",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Class Requirements",
    "text": "Class Requirements\n\nAssume you all have a intro course in statistics and probability (which I know you do)\nMath: Basic knowledge of calculus, probability, densities, distributions, statistical tests, hypothesis testing, the linear model, maximum likelihood and generalized linear models is assumed.\nProgramming: Functional knowledge of R - main programming language of the course. Some Python at the end.\n\nR is excellent for text analysis, and for some social science applications, better than Python\nFree, and massive online community writing packages and extending modeling capabilities.\nWe will divide our learning between using tidytext and quanteda for text analysis.\nDownload RStudio IDE!"
  },
  {
    "objectID": "slides/week_01_intro.html#how-to-do-well-in-the-class",
    "href": "slides/week_01_intro.html#how-to-do-well-in-the-class",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "How to do well in the class?",
    "text": "How to do well in the class?\nI designed this course as PhD style seminar:\n\nSo far, you learned a lot of DS techniques (DS I, DS II, DS III)\nYou haven’t dig deep enough in a particular field. That’s what electives are for!\nHeavy on readings - Lot’s of applied and technical readings.\nDo the readings before class\nSubstantive readings are especially important, because they’ll help you understand what an interesting question looks like – in social science/public policy.\nPlan ahead – particularly for the replication exercise\nIf you have a corpus you want work with, please bring it to class!"
  },
  {
    "objectID": "slides/week_01_intro.html#what-our-classes-will-look-like.",
    "href": "slides/week_01_intro.html#what-our-classes-will-look-like.",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "What our classes will look like.",
    "text": "What our classes will look like.\nThis is a one meeting per week class. You should expect:\n\nBetween 1h-1.5h of lecture based on this week topics + readings\nYour participation in the lecture is expected I will ask your insights about the readings.\nBreak (10min)\nCoding.\n\nMix of you working through some code I prepared.\nAnd I live-coding for you."
  },
  {
    "objectID": "slides/week_01_intro.html#textbook",
    "href": "slides/week_01_intro.html#textbook",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Textbook",
    "text": "Textbook"
  },
  {
    "objectID": "slides/week_01_intro.html#logistics",
    "href": "slides/week_01_intro.html#logistics",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Logistics",
    "text": "Logistics\n\nCommunication: via slack. Join the channel!\nAll materials: hosted on the class website: https://tiagoventura.github.io/PPOL_6801_2024//\nSyllabus: also on the website.\nMy Office Hours: Every Tuesday from 4 to 6pm. Just stop by!\nCanvas: Only for communication! Materials will be hosted in the website!"
  },
  {
    "objectID": "slides/week_01_intro.html#evalutation",
    "href": "slides/week_01_intro.html#evalutation",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Evalutation",
    "text": "Evalutation\n\n\n\nAssignment\nPercentage of Grade\n\n\n\n\nParticipation/Attendance\n10%\n\n\nProblem Sets\n20%\n\n\nReplication Exercises\n30%\n\n\nFinal Project\n40%"
  },
  {
    "objectID": "slides/week_01_intro.html#participation",
    "href": "slides/week_01_intro.html#participation",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Participation",
    "text": "Participation\n\nActive involvement during class sessions, fostering a dynamic learning environment.\nContributions made to your group’s ultimate project.\nAssisting classmates with slack questions, sharing interesting materials on slack, asking question, and anything that provides healthy contributions to the course."
  },
  {
    "objectID": "slides/week_01_intro.html#problem-sets",
    "href": "slides/week_01_intro.html#problem-sets",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Problem Sets",
    "text": "Problem Sets\n\n\n\nAssignment\nDate Assigned\nDate Due\n\n\n\n\nNo. 1\nWeek 4\nBefore EOD of Week 5’s class\n\n\nNo. 2\nWeek 7\nBefore EOD of Week 8’s class\n\n\nNo. 3\nWeek 9\nBefore EOD of Week 10’s class\n\n\n\n\nYou will have a week to complete your assignments\nindividual assignment\ndistributed through github"
  },
  {
    "objectID": "slides/week_01_intro.html#replication-exercises",
    "href": "slides/week_01_intro.html#replication-exercises",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Replication Exercises",
    "text": "Replication Exercises\nOpportunity to learn how science is made!\nWork in randomly assigned pairs I will post on Slack.\n\nStep 1: finding a paper to replicate (from the syllabus)\n\nBy the end of the week 2 and week 7, you should select an article from the syllabus to be replicated by your team.\nInform the class on slack\n“first come, first served”\n\nStep 2: Acquiring the Data\n\nif you fail to get the data, pick another article.\n\nStep 3: Presentation (weeks 6 and 11)\nStep 4: Replication Repository on Github"
  },
  {
    "objectID": "slides/week_01_intro.html#final-project",
    "href": "slides/week_01_intro.html#final-project",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Final Project",
    "text": "Final Project\nThe project is composed of three parts:\n\na 2 page project proposal: (which should be discussed and approved by me)\nan in-class presentation,\nA 10-page project report.\n\n\n\n\n\n\n\n\n\n\nRequirement\nDue\nLength\nPercentage\n\n\n\n\nProject Proposal\nEOD Friday Week 9\n2 pages\n5%\n\n\nPresentation\nWeek 14\n10-15 minutes\n10%\n\n\nProject Report\nWednesday Week 15\n10 pages\n25%"
  },
  {
    "objectID": "slides/week_01_intro.html#chatgpt",
    "href": "slides/week_01_intro.html#chatgpt",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "ChatGPT",
    "text": "ChatGPT\n\nYou are allowed to use ChatGPT as you would use google in this class. This means:\n\nDo not copy the responses from chatgpt – a lot of them are wrong or will just not run on your computer\nUse chatgpt as a auxiliary source.\nIf your entire homework comes straight from chatgpt, I will consider it plagiarism.\nIf you use chatgpt, I ask you to mention on your code how chatgpt worked for you."
  },
  {
    "objectID": "slides/week_01_intro.html#acquiring-text",
    "href": "slides/week_01_intro.html#acquiring-text",
    "title": "PPOL 6801 - Text as Data - Computational Linguistics",
    "section": "Acquiring text:",
    "text": "Acquiring text:\n\nAs a review, here are some notebooks I developed for Data Science I introducing a full toolkit for acquiring data in the web:\n\nStatic Websites\nAPIs\nSelenium for Dynamics Websites\n\n\n\n\nText-as-Data"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus: PPOL 6801 - Text as Data: Computational Linguistics",
    "section": "",
    "text": "Download a PDF Version here"
  },
  {
    "objectID": "syllabus.html#course-description",
    "href": "syllabus.html#course-description",
    "title": "Syllabus: PPOL 6801 - Text as Data: Computational Linguistics",
    "section": "Course Description",
    "text": "Course Description\nIn recent years, the surge in the availability of textual data, ranging from the digitalization of archival documents, political speeches, social media posts, and online news articles, has led to a growing demand for statistical analysis using large volumes of text data. In this course, we will teach students how to analyze and somewhat collect this data from a social science viewpoint. The focus is on understanding the real-world use of text as data rather than just the theory behind it. Students will learn how to acquire text, turn text into data, and analyze it to answer important policy-relevant questions. Each week, we’ll go over different methods, like building and using dictionaries, understanding sentiment in text, scaling texts on ideological and policy dimensions, and using machine learning to classify text. Lectures will include hands-on activities, letting students work directly with actual texts, and I strongly encourage students to bring their own data to class. The course aims to equip students with a variety of text analysis techniques that will be valuable for their future work as policy experts and computational social scientists.\nWhile the course covers an interdisciplinary topic, and many of the techniques we discuss have their origins in computer science or statistics, we will spend relatively little time on traditional Natural Language Processing techniques, such as machine translation, optical character recognition, and parts of speech tagging, etc. Although we will touch on Large Language Models (ChatGPT) at the end of the course, we will focus mostly on the practical use of these models through their APIs instead of building up and providing an in-depth understanding of their architecture.\nI assume students taking this class have taken, at minimum, an introductory class in statistics and have basic knowledge of probability, distributions, hypothesis testing, and linear models. The core language and software environment of this course is R. If you are not familiar with R, you will struggle with the assigned exercises. We will also provide some code in Python, but no prior knowledge here is assumed since this will be additional material."
  },
  {
    "objectID": "syllabus.html#instructor",
    "href": "syllabus.html#instructor",
    "title": "Syllabus: PPOL 6801 - Text as Data: Computational Linguistics",
    "section": "Instructor",
    "text": "Instructor\nProfessor Tiago Ventura\n\nPronouns: He/Him\nEmail: tv186@georgetown.edu\nOffice hours: Every Tuesday, 4pm - 6pm\nLocation: Old north, 312"
  },
  {
    "objectID": "syllabus.html#our-classes",
    "href": "syllabus.html#our-classes",
    "title": "Syllabus: PPOL 6801 - Text as Data: Computational Linguistics",
    "section": "Our classes",
    "text": "Our classes\nClasses will take place at the scheduled class time/place and will involve a combination of lectures, coding walkthrough, breakout group sessions, and questions. We will start our classes with a lecture highlighting what I consider to be the broader substantive and programming concepts covered in the class. From that, we will switch to a mix of coding walk through and breakout group sessions.\nThis class follows a more classic PhD style seminar. This means that the class is heavy on the readings, and I expect you to do the readings before class. For most classes, you will read one or more chapters of the text book and between two or three applied articles. Most of the lectures will cover topics discussed on the readings.\nNote that this class is scheduled to meet weekly for 2.5 hours. I will do my best to make our meetings dynamic and enjoyable for all parts involved. We will take one or two breaks in each of our lecture."
  },
  {
    "objectID": "syllabus.html#required-materials",
    "href": "syllabus.html#required-materials",
    "title": "Syllabus: PPOL 6801 - Text as Data: Computational Linguistics",
    "section": "Required Materials",
    "text": "Required Materials\nReadings: We will rely primarily on the following textbooks for this course. While this textbook is not freely available online, all the other materials of the course will be or should be accessible through Georgetown library.\n\nGrimmer, Justin, Margaret E. Roberts, and Brandon M. Stewart. Text as data: A new framework for machine learning and the social sciences. Princeton University Press, 2022 - [GMB]\nDaniel Jurafsky and James H. Martin. Speech and Language Processing, 3nd Edition. - [SLP]\n\nThe weekly articles are listed in the syllabus"
  },
  {
    "objectID": "syllabus.html#course-infrastructure",
    "href": "syllabus.html#course-infrastructure",
    "title": "Syllabus: PPOL 6801 - Text as Data: Computational Linguistics",
    "section": "Course Infrastructure",
    "text": "Course Infrastructure\nClass Website: A class website will be used throughout the course and should be checked on a regular basis for lecture materials and required readings.\nClass Slack Channel: The class also has a dedicated slack channel. The channel serves as an open forum to discuss, collaborate, pose problems/questions, and offer solutions. Students are encouraged to pose any questions they have there as this will provide the professor and TA the means of answering the question so that all can see the response. If you’re unfamiliar with, please consult the following start-up tutorial (https://get.slack.help/hc/en-us/articles/218080037-Getting-started-for-new-members).\nCanvas: A Canvas site (http://canvas.georgetown.edu) will be used throughout the course and should be checked on a regular basis for announcements and assignments. All announcements for the assignments and classes will be posted on Canvas; they will not be distributed in class or by e-mail. Support for Canvas is available at (202) 687-4949"
  },
  {
    "objectID": "syllabus.html#weekly-schedule-readings",
    "href": "syllabus.html#weekly-schedule-readings",
    "title": "Syllabus: PPOL 6801 - Text as Data: Computational Linguistics",
    "section": "Weekly Schedule & Readings",
    "text": "Weekly Schedule & Readings\n\nWeek 1: Introduction - Overview of the course\nTopics: Review of syllabus and class organization. Introduction to computational text analysis.\n\n[GMB] - Chapter 2.\n\nOr one of the three below:\n\nPolitical Science Perspective: Wilkerson, J. and Casas, A. (2017). Large-Scale Computerized Text Analysis in Political Science: Opportunities and Challenges. Annual Review of Political Science, 20, 529:544.\nEconomics Perspective: Gentzkow, Matthew, Bryan Kelly, and Matt Taddy. “Text as data.” Journal of Economic Literature 57, no. 3 (2019): 535-574.\nSociology Perspective: https://www.sciencedirect.com/science/article/pii/S0049089X22000904\n\n\n\nWeek 2: From Text to Matrices: Representing Text as Data\nTopics: How to represent text as data? What is a Bag of Words? What are tokens? Why should we care about tokens?\n-[GMB] - Chapters 3-5\n\nApplied Papers:\n\nDenny, M. J., & Spirling, A. (2018). Text preprocessing for unsupervised learning: why it matters, when it misleads, and what to do about it. Political Analysis, 26(2): 168-189.\nBan, Pamela, Alexander Fouirnaies, Andrew B. Hall, and James M. Snyder. “How newspapers reveal political power.” Political Science Research and Methods 7, no. 4 (2019): 661-678.\nMichel, J.B., et al. 2011. “Quantitative Analysis of Culture Using Millions of Digitized Books.” Science 331 (6014): 176–82. https://doi.org/10.1126/science.1199644;\n\n\n\n\nWeek 3: Text Similarity, Text re-use and Complexity\nTopics: How do we evaluate complexity in text? Why should we care about complexity in text? How do we evaluate similarity in text? Why is this useful?\n\n[GMB] - Chapter 7\nApplied Papers:\n\nSpirling, Arthur. 2016. “Democratization and Linguistic Complexity”, Journal of Politics.\nBenoit, K., Munger, K. and Spirling, A. 2017. Measuring and Explaining Political Sophistication Through Textual Complexity\nLinder, Fridolin, Bruce Desmarais, Matthew Burgess, and Eugenia Giraudy. “Text as policy: Measuring policy similarity through bill text reuse.” Policy Studies Journal 48, no. 2 (2020): 546-574.\n\n\n\n\nWeek 4: Supervised Learning I: Dictionary Methods and Out of Box Classifier Analysis\nTopics: What are dictionaries? Why/when are they useful? What are their limitations? Can we use models trained by others?\n\n[GMB] - Chapters 15-16\nApplied Papers:\n\nLori Young and Stuart Soroka 2012 “Affective News: The Automated Coding of Sentiment in Political Texts.” Political Communication, 29:2, 205-231.\nRathje, Steve, Jay J. Van Bavel, and Sander Van Der Linden. “Out-group animosity drives engagement on social media.” Proceedings of the National Academy of Sciences 118, no. 26 (2021): e2024292118.\nVentura, Tiago, Kevin Munger, Katherine McCabe, and Keng-Chi Chang. “Connective effervescence and streaming chat during political debates.” Journal of Quantitative Description: Digital Media 1 (2021).\n\nProblem set 1 Assigned\n\n\n\nWeek 5: Supervised Learning II: Training your own classifiers\nTopics: We will study the framework to train our own supervised models, and when to use them.\n\n[GMB] - Chapters 17, 18, 19, and 20.\nApplied Papers:\n\nBarberá, Pablo, Amber E. Boydstun, Suzanna Linn, Ryan McMahon, and Jonathan Nagler. “Automated text classification of news articles: A practical guide.” Political Analysis 29, no. 1 (2021): 19-42.\nSiegel, Alexandra, et al. “Trumping Hate on Twitter? Online Hate Speech in the 2016 US Election Campaign and its Aftermath.”\nTheocharis, Y., Barberá, P., Fazekas, Z., & Popa, S. A. (2020). The dynamics of political incivility on Twitter. Sage Open, 10(2), 2158244020919447.\nMitts, T., Phillips, G., & Walter, B. (2021). Studying the Impact of ISIS Propaganda Campaigns. Journal of Politics\n\n\n\n\nWeek 6: Replication Class I: Students’ Presentation\n\n\nWeek 7: Unsupervised Learning: Topic Models\nTopics: what if we do not have an outcome to predict? can we cluster the text in groups? what are topics?\n\n[GMB] - Chapters 12-14.\nDavid M. Blei . 2012. “Probabilistic Topic Models.” http://www.cs.columbia.edu/~blei/papers/ Blei2012.pdf\nApplied Papers:\n\nMotolinia, Lucia. Electoral accountability and particularistic legislation: evidence from an electoral reform in Mexico. American Political Science Review 115, no. 1 (2021): 97-113.\nBarberá, P., Casas, A., Nagler, J., Egan, P. J., Bonneau, R., Jost, J. T., & Tucker, J. A. (2019). Who leads? Who follows? Measuring issue attention and agenda setting by legislators and the mass public using social media data. American Political Science Review, 113(4), 883-901.\nEshima, Shusei, Kosuke Imai, and Tomoya Sasaki. “Keyword‐Assisted Topic Models.” American Journal of Political Science (2020).\n\n\n\n\nWeek 8: Word Embeddings: What they are and how to estimate?\nTopics: What are word-embeddings? When and how can we use them? What? Topic models again? Is this still a bag of words?\n\n[GMB] - Chapter 8.\n[SLP] Chapter 6, “Vector Semantics and Embeddings.”\nJay Alanmar, The Illustrated Word2vec\nSpirling and Rodriguez, Word embeddings: What works, what doesn’t, and how to tell the difference for applied research.\n\n\n\nWeek 9: Word Embeddings: Social Science Applications\n\nApplied Papers:\n\nRodman, E., 2020. A Timely Intervention: Tracking the Changing Meanings of Political Concepts with Word Vectors. Political Analysis, 28(1), pp.87-111.\nGennaro, Gloria, and Elliott Ash. “Emotion and reason in political language.” The Economic Journal 132, no. 643 (2022): 1037-1059.\nRheault, Ludovic, and Christopher Cochrane. “Word embeddings for the analysis of ideological placement in parliamentary corpora.” Political Analysis 28, no. 1 (2020): 112-133.\nAustin C. Kozlowski, Austin C., Matt Taddy, and James A. Evans. 2019. “The Geometry of Culture: Analyzing the Meanings of Class through Word Embeddings.” American Sociological Review 84, no. 5: 905–49. https://doi.org/10.1177/0003122419877135.\nGarg, Nikhil, Londa Schiebinger, Dan Jurafsky and James Zou. 2018. “Word embeddings quantify 100 years of gender and ethnic stereotypes.” Proceedings of the National Academy of Sciences 115(16):E3635–E3644.\n\nProblem set 3 Assigned\n\n\n\nWeek 10: Using Text to Measure Ideology - Scaling\nTopics: What are scaling models and what can they tell us? Can we represent politicians/users ideology using text?\n\nApplied Papers:\n\nLaver, Michael, Kenneth Benoit, and John Garry. 2003. “Extracting Policy Positions from Political Texts Using Words as Data”. American Political Science Review. 97, 2, 311-331\nSlapin, Jonathan and Sven-Oliver Prokschk. 2008. “A Scaling Model for Estimating Time-Series Party Positions from Texts.” American Journal of Political Science. 52, 3 705-722\nRheault, Ludovic, and Christopher Cochrane. “Word embeddings for the analysis of ideological placement in parliamentary corpora.” Political Analysis 28, no. 1 (2020): 112-133.\nAruguete, Natalia, Ernesto Calvo, and Tiago Ventura. “News by popular demand: Ideological congruence, issue salience, and media reputation in news sharing.” The International Journal of Press/Politics 28, no. 3 (2023): 558-579.\nIzumi, Mauricio Y., and Danilo B. Medeiros. “Government and opposition in legislative speechmaking: using text-as-data to estimate Brazilian political parties’ policy positions.” Latin American Politics and Society 63, no. 1 (2021): 145-164.\n\n\n\n\nWeek 11: Replication Class II: Students’ Presentation\n\n\nWeek 12: Large Language Models: Theory and Fine-tuning a Transformers-based model (Inviter Speaker: Dr. Sebastian Vallejo)\nTopics: We will learn about the Transformers architecture, attention, and the encoder-coder infrastructure.\n\n[SLP] - Chapter 10.\nJay Alammar. 2018. “The Illustrated Transformer.” https://jalammar.github.io/illustratedtransformer/\nVaswani, A., et al. (2017). Attention is all you need. Advances in neural information processing systems, 30;\nTimoneda and Vera, BERT, RoBERTa or DeBERTa? Comparing Performance Across Transformer Models in Political Science Text, Forthcoming Journal of Politics.\n\n\n\nWeek 13: Large Language Models: Outsourcing Applications\n\nApplied Papers: We will see some social science application of LLMs chatbots. Can we use ChatGPT to perform zero-shot classification tasks? What are the concerns of these applications?\n\nWu, Patrick Y., Joshua A. Tucker, Jonathan Nagler, and Solomon Messing. “Large language models can be used to estimate the ideologies of politicians in a zero-shot learning setting.” arXiv preprint arXiv:2303.12057 (2023).\nRathje, Steve, Dan-Mircea Mirea, Ilia Sucholutsky, Raja Marjieh, Claire Robertson, and Jay J. Van Bavel. GPT is an effective tool for multilingual psychological text analysis. (2023).\nDavidson, Thomas: Start Generating: Harnessing Generative Artificial Intelligence for Sociological Research, PrePrint\nBisbee, James, Joshua Clinton, Cassy Dorff, Brenton Kenkel, and Jennifer Larson. 2023. “Synthetic Replacements for Human Survey Data? the Perils of Large Language Models.” SocArXiv. May 4.\n\n\n\n\nWeek 14: Final Projects: Students Presentation"
  },
  {
    "objectID": "syllabus.html#course-requirements",
    "href": "syllabus.html#course-requirements",
    "title": "Syllabus: PPOL 6801 - Text as Data: Computational Linguistics",
    "section": "Course Requirements",
    "text": "Course Requirements\n\n\n\nAssignment\nPercentage of Grade\n\n\n\n\nParticipation/Attendance\n10%\n\n\nProblem Sets\n20%\n\n\nReplication Exercises\n30%\n\n\nFinal Project\n40%\n\n\n\nParticipation and Attendance (10%):\nData science is an cooperative endeavor, and it’s crucial to view your fellow classmates as valuable assets rather than rivals. Your performance in the following aspects will be considered when assessing this part of your grade:\n\nActive involvement during class sessions, fostering a dynamic learning environment.\nContributions made to your group’s ultimate project.\nAssisting classmates by addressing problem set queries through GitHub issues. Supporting your peers will enhance your evaluation in terms of teamwork and engagement\nAssisting classmates with slack questions, sharing interesting materials on slack, asking question, and anything that provides healthy contributtions to the course.\n\nProblem Sets (20%):\nYou will have a week to complete your assignments\nStudents will be assigned three problem sets over the course of the semesters. While you are encouraged to discuss the problem sets with your peers and/or consult online resources, the finished product must be your own work. The goal of the assignment is to reinforce the student’s comprehension of the materials covered in each section.\nThe problems sets will assess your ability to apply the concepts to data that is substantially messier, and problems that are substantially more difficult, than the ones in the coding discussion in class.\nI will distribute the assignment through a mix of canvas and github. The assignments can be in the form of a Jupyter Notebook (.ipynb) or Quarto (.qmd). Students must submit completed assignments as a rendered .html file and the corresponding source code (.ipynb or .qmd).\nThe assignments will be graded in accuracy and quality of the programming style. For instance, our grading team will be looking at:\n\n\nall code must run;\n\n\nsolutions should be readable\n\n\nCode should be thoroughly commented (the Professor/TA should be able to understand the codes purpose by reading the comment),\nCoding solutions should be broken up into individual code chunks in Jupyter/R Markdown notebooks, not clumped together into one large code chunk (See examples in class or reach out to the TA/Professor if this is unclear),\nEach student defined function must contain a doc string explaining what the function does, each input argument, and what the function returns;\n\n\nCommentary, responses, and/or solutions should all be written in Markdown and explain sufficiently the outpus.\n\n\nAll solutions must be completed in Python.\n\n\nThe follow schedule lays out when each assignment will be assigned.\n\n\n\nAssignment\nDate Assigned\nDate Due\n\n\n\n\nNo. 1\nWeek 4\nBefore EOD of Week 5’s class\n\n\nNo. 3\nWeek 9\nBefore EOD of Week 10’s class\n\n\n\nReplication Exercises (30%)\nReplication exercises are adapted from Gary King’s work here and here. Replication consists in the process of repeating a research study using the original data or brining new data to the conversation. Replicability is crucial for the advancement of knowledge and the credibility of scientific inquiry.\nFor our purposes, replication exercises will work as a educational tool. A common say in science is that you just learn a new skill/methods when you use in your own work. Since we do not have time to write three papers during a semester, we will take advantage of published work with open available datasets and code for us to work on replication exercises.\nYou will work in pairs for this assignment. Your partner will be randomly assigned. You will repeat this exercise twice throughout the course. This is a stylized step-by-step of this exercise:\n\nStep 1: finding a paper to replicate\n\nBy the end of the week 2 and week 7, you should select an article from the syllabus to be replicated by you.\nThe first replication exercise should consider articles from weeks 2 to week 5, and the second from the remaining of the class.\n\nStep 2: Acquiring the Data\n\nMost research articles are published with open data rules. This means their data and code are often available on github, or on harvard dataverse. Your first task is to find the data and code from these articles.\nIf your articles does not have the data and code, you should:\n\nPolitely contact the authors of the article and ask for the replication materials\nIf you don’t get response, select another article.\n\n\nStep 3: Presentation\n\nFor weeks 6 and 11, you will do a presentation of your replication efforts.\nThe presentation should have the following sections:\n\nIntroduction: introduction summarizing the article.\nMethods: data used in the article\nResults: the results you were able to replicate\nDifferences: any differences between your results and the authors’\nAutopsy of the replication: what worked and what did not work\nExtension: what would you do different if you were to write this article today? Where would you innovate?\n\n\nStep 4: Replication Repository\n\nBy Friday EOD of each replication week, you should share with me and all your colleagues your replication report. The replication report should be:\n\na github repo with a well-detailed readme. See an model here: https://github.com/TiagoVentura/winning_plosone\nyour presentation as a pdf\nthe code used in the replication as a notebook (Markdown or Jupyter)\na report with maximum of 5 pages (it is fine if you do less than that) summarizing the replication process, with emphasis on three sections of your presentation: Differences, Autopsy and Extension.\n\n\n\nIn addition to following the requirements above, the replication exercises will also be graded in accuracy and quality of the programming style. For instance, our grading team will be looking at:\n\n\nall code must run;\n\n\nsolutions should be readable\n\n\nCode should be thoroughly commented (the Professor/TA should be able to understand the codes purpose by reading the comment),\nCoding solutions should be broken up into individual code chunks in Jupyter/R Markdown notebooks, not clumped together into one large code chunk (See examples in class or reach out to the TA/Professor if this is unclear),\nEach student defined function must contain a doc string explaining what the function does, each input argument, and what the function returns;\n\n\nCommentary, responses, and/or solutions should all be written in Markdown and explain sufficiently the outpus.\n\n\n\nFinal Project (40%): Data science is an applied field and the DSPP is particularly geared towards providing students the tools to make policy and substantive contributtions using data and recent computational developments. In this sense, it is fundamental that you understand how to conduct a complete analysis from collecting data, to cleaning and analyzing it, to presenting your findings. For this reason, a considerable part of your grade will come from a an independent data science project, applying concepts learned throughout the course.\nThe project is composed of three parts:\n\na 2 page project proposal: (which should be discussed and approved by me)\nan in-class presentation,\nA 10-page project report.\n\nDue dates and breakdowns for the project are as follows:\n\n\n\n\n\n\n\n\n\nRequirement\nDue\nLength\nPercentage\n\n\n\n\nProject Proposal\nEOD Friday Week 9\n2 pages\n5%\n\n\nPresentation\nWeek 14\n10-15 minutes\n10%\n\n\nProject Report\nWednesday Week 15\n10 pages\n25%\n\n\n\nImportant notes about the final project\n\nFor the project proposal, you need to schedule a 30min with me at least a week before the due date. For this meeting, I expect you to send me a draft of your ideas.\nFor the presentation, You will have 10-15 minutes in our last class of the semester to present you project.\nTake the final project seriously. After you finish your Masters, in any path you take, you will need to show concrete examples of your portfolio. This is a good opportunity to start building it.\nYour groups will be randomly assigned.\n\nSubmission of the Final Project\nThe end product should be a github repository that contains:\n\nThe raw source data you used for the project. If the data is too large for GitHub, talk with me, and we will find a solution\nYour proposal\nA README for the repository that, for each file, describes in detail:\n\nInputs to the file: e.g., raw data; a file containing credentials needed to access an API\nWhat the file does: describe major transformations.\nOutput: if the file produces any outputs (e.g., a cleaned dataset; a figure or graph).\nA set of code files that transform that data into a form usable to answer the question you have posed in your descriptive research proposal.\nYour final 10 pages report (I will share a template later in the semester)\n\n\nOf course, no commits after the due date will be considered in the assessment.\n\nGrading\nCourse grades will be determined according to the following scale:\n\n\n\nLetter\nRange\n\n\n\n\nA\n95% – 100%\n\n\nA-\n91% – 94%\n\n\nB+\n87% – 90%\n\n\nB\n84% – 86%\n\n\nB-\n80% – 83%\n\n\nC\n70% – 79%\n\n\nF\n&lt; 70%\n\n\n\nGrades may be curved if there are no students receiving A’s on the non-curved grading scale.\nLate problem sets will be penalized a letter grade per day.\n\n\nCommunication\n\nClass-relevant and/or coding-related questions, Slack is the preferred method of communication. Please use the general or the relevant channel for these questions.\nFor private questions concerning the class, email is the preferred method of communication. All email messages must originate from your Georgetown University email account(s). Please email the professor directly rather than through the Canvas messaging system.\nI will try my best to respond to all emails/slack questions within 24 hours of being sent during a weekday. I will not respond to emails/slack sent late Friday (after 5:00 pm) or during the weekend until Monday (9:00 am). Please plan accordingly if you have questions regarding current or upcoming assignments.\nOnly reach out to the professor or teaching assistant regarding a technical question, error, or issue after you made a good faith effort to debugging/isolate your problem prior to reaching out. Learning how to search for help online is a important skill for data scientists."
  },
  {
    "objectID": "syllabus.html#electronic-devices",
    "href": "syllabus.html#electronic-devices",
    "title": "Syllabus: PPOL 6801 - Text as Data: Computational Linguistics",
    "section": "Electronic Devices",
    "text": "Electronic Devices\nWhen meeting in-person: the use of laptops, tablets, or other mobile devices is permitted only for class-related work. Audio and video recording is not allowed unless prior approval is given by the professor. Please mute all electronic devices during class."
  },
  {
    "objectID": "syllabus.html#georgetown-policies",
    "href": "syllabus.html#georgetown-policies",
    "title": "Syllabus: PPOL 6801 - Text as Data: Computational Linguistics",
    "section": "Georgetown Policies",
    "text": "Georgetown Policies\n\nDisability\nIf you believe you have a disability, then you should contact the Academic Resource Center (arc@georgetown.edu) for further information. The Center is located in the Leavey Center, Suite 335 (202-687-8354). The Academic Resource Center is the campus office responsible for reviewing documentation provided by students with disabilities and for determining reasonable accommodations in accordance with the Americans with Disabilities Act (ASA) and University policies. For more information, go to http://academicsupport.georgetown.edu/disability/\n\n\nImportant Academic Policies and Academic Integrity\nMcCourt School students are expected to uphold the academic policies set forth by Georgetown University and the Graduate School of Arts and Sciences. Students should therefore familiarize themselves with all the rules, regulations, and procedures relevant to their pursuit of a Graduate School degree. The policies are located at: http://grad.georgetown.edu/academics/policies/\nApplied to this course, while I encourage collaboration on assignments and use of resources like StackOverflow, the problem sets will ask you to list who you worked on the problem set with and cite StackOverflow if it is the direct source of a code snippet.\n\nChatGPT\nIn the last year, the world was inundated with popularization of Large Language Models, particularly the easy use of ChatGPT. I see ChatGPT as Google on steroids, so I assume ChatGPT will be part of your daily work in this course, and it is part of my work as a researcher.\nThat being said, ChatGPT does not replace your training as a data scientist. If you are using ChatGPT instead of learning, I consider you are cheating in the course. And most importantly, you are wasting your time and resources. So that’s our policy for using LLMs models in class:\n\nDo not copy the responses from chatgpt – a lot of them are wrong or will just not run on your computer.\nUse chatgpt as a auxiliary source.\nIf your entire homework comes straight from chatgpt, I will consider it plagiarism.\nIf you use chatgpt, I ask you to mention on your code how chatgpt worked for you.\n\n\n\n\nStatement on Sexual Misconduct\nGeorgetown University and its faculty are committed to supporting survivors and those impacted by sexual misconduct, which includes sexual assault, sexual harassment, relationship violence, and stalking. Georgetown requires faculty members, unless otherwise designated as confidential, to report all disclosures of sexual misconduct to the University Title IX Coordinator or a Deputy Title IX Coordinator. If you disclose an incident of sexual misconduct to a professor in or outside of the classroom (with the exception of disclosures in papers), that faculty member must report the incident to the Title IX Coordinator, or Deputy Title IX Coordinator. The coordinator will, in turn, reach out to the student to provide support, resources, and the option to meet. [Please note that the student is not required to meet with the Title IX coordinator.]. More information about reporting options and resources can be found on the Sexual Misconduct\nWebsite: https://sexualassault.georgetown.edu/resourcecenter\nIf you would prefer to speak to someone confidentially, Georgetown has a number of fully confidential professional resources that can provide support and assistance. These resources include: Health Education Services for Sexual Assault Response and Prevention: confidential email: sarp[at]georgetown.edu\nCounseling and Psychiatric Services (CAPS): 202.687.6985 or after hours, call (833) 960-3006 to reach Fonemed, a telehealth service; individuals may ask for the on-call CAPS clinician\nMore information about reporting options and resources can be found on the Sexual Misconduct Website.\n\n\nProvost’s Policy on Religious Observances\nGeorgetown University promotes respect for all religions. Any student who is unable to attend classes or to participate in any examination, presentation, or assignment on a given day because of the observance of a major religious holiday or related travel shall be excused and provided with the opportunity to make up, without unreasonable burden, any work that has been missed for this reason and shall not in any other way be penalized for the absence or rescheduled work. Students will remain responsible for all assigned work. Students should notify professors in writing at the beginning of the semester of religious observances that conflict with their classes. The Office of the Provost, in consultation with Campus Ministry and the Registrar, will publish, before classes begin for a given term, a list of major religious holidays likely to affect Georgetown students. The Provost and the Main Campus Executive Faculty encourage faculty to accommodate students whose bona fide religious observances in other ways impede normal participation in a course. Students who cannot be accommodated should discuss the matter with an advising dean."
  }
]